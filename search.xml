<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>多语言模型中事实知识的跨语言一致性</title>
      <link href="/2023/11/02/duo-yu-yan-mo-xing-zhong-shi-shi-zhi-shi-de-kua-yu-yan-yi-zhi-xing/"/>
      <url>/2023/11/02/duo-yu-yan-mo-xing-zhong-shi-shi-zhi-shi-de-kua-yu-yan-yi-zhi-xing/</url>
      
        <content type="html"><![CDATA[<h1 id="多语言模型中事实知识的跨语言一致性"><a href="#多语言模型中事实知识的跨语言一致性" class="headerlink" title="多语言模型中事实知识的跨语言一致性"></a>多语言模型中事实知识的跨语言一致性</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p><span style="background-color: #2ea8e580">多语言大规模预训练语言模型（PLM）已被证明可以存储大量的事实知识，但观察到语言之间存在很大差异</span>。为了确保具有不同语言背景的用户从同一模型获得一致的反馈，我们研究了各种多语言PLM中事实知识的跨语言一致性（CLC）。为此，我们<span style="background-color: #2ea8e580">提出了一个基于排名的一致性（RankC）指标，以独立于准确性来评估跨语言的知识一致性</span>。使用这个指标，我们在模型级别和语言对级别对CLC的决定因素进行了深入分析。在其他结果中，我们发现<span style="background-color: #ff666680">增加模型大小会导致大多数语言中更高的事实探测准确性，但不会提高跨语言的一致性</span>。最后，我们进行了关于CLC的案例研究，当通过模型编辑在PLM中插入新的事实关联时。英<span style="background-color: #ff666680">语插入的一小部分事实样本的结果揭示了一个清晰的模式，即新知识仅转移到英语具有高 RankC 分数的语言。</span></p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>     大规模预训练语言模型 （PLM） 在事实知识发挥重要作用的任务中展示了强大的能力（Roberts 等人，2020 年;秦等人，2022 年）。虽然以前大多数关于探索 PLM 中事实知识的工作都集中在英语上（Davison 等人，2019 年;布拉维等人，2020 年;申等人，2020;布朗等人，2020 年;阿尔甘米等人，2021 年;Peng 等人，2022 年），一些值得注意的研究已将评估扩展到许多其他语言（Jiang 等人，2020 年;卡斯纳等人，2021 年;尹等人，2022 年）。这些研究结果表明事实知识在多大程度上跨语言泛化，揭示了现代 NLP 技术中语言不平等的另一个方面（Hupkes 等人，2022 年）。</p><p><img src="/../imgs/$%7Bfiilename%7D/JTY72Y4D-1698899470325-23.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/JTY72Y4D-1698899470325-23.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;JTY72Y4D&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%225VBT7M5M%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B293.654%2C401.698%2C529.038%2C631.313%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%221%22%7D%7D&quot; width=&quot;392&quot; height=&quot;382&quot; src=&quot;attachments/JTY72Y4D.png&quot; ztype=&quot;zimage&quot;&gt;"></p><p>     然而，<span style="background-color: #5fb23680">评估跨语言的事实知识并非易事。确保结果的可比性要求以所有语言查询一组“普遍”事实，但该集合的选择可能偏向于在维基数据等流行知识库中代表性更高的特定世界区域.2相反，在世界其他地区更相关的事实（例如， 关于某一特定区域的地点或重要人物的信息）不太可能出现在基准中，这使得难以解释这种评估的结果</span>。</p><p>     在这项工作中，我们采取了不同的立场：我们没有衡量PLM在每种语言中编码的事实知识量，而是关注其跨语言的一致性。如图 1 所示，多语言 BLOOM-3b 模型当以英语、西班牙语和越南语查询时，输出始终正确完成第一个提示，但不是匈牙利语和希腊语。该模型还以英语、西班牙语和越南语（但不是匈牙利语和希腊语）对第二个查询输出一致但错误的答案，这表明前三种语言在模型中共享相关的知识表示。</p><p>     跨语言一致性 （CLC） 的研究很重要，至少有两个原因：首先，对事实的真正了解意味着无论给定的表面形式如何，都要对其含义进行编码（Ohmer 等人，2023 年）。因此，如果模型知道北京市是中国的首都，那么当用不同的语言询问相同的问题时，它应该返回相同的答案。从实际的角度来看，CLC 对于确保用户在不同语言与同一模型交互时具有相似的体验至关重要。其次，研究CLC对于了解在多语言PLM中以一种语言获得的知识是否以及如何隐含地转移到另一种语言非常重要。除了科学相关性外，这对将外部知识纳入多语言PLM具有实际意义。事实上，虽然多产的工作线侧重于模型编辑，作为以各种数据和计算效率的方式在 PLM 中插入新事实关联的一种方式（De Cao 等人，2021 年;侯等人，2022;Meng 等人，2022 年），据我们所知，还没有人研究过这如何影响直接应用编辑的语言以外的语言中的事实知识。</p><p>      我们对多语言PLM中的事实知识CLC进行了首次深入研究，并做出了以下贡献：（i）我们提出了一种新的基于排名的一致性（RankC）指标，该指标独立于准确性评估知识的一致性。（ii） 我们过滤现有的不平衡数据集（Jiang 等人，2020 年;Kassner 等人，2021 年）形成多并行 CLC 基准，平衡多语言模型分析 （BMLAMA），该基准将相同的一组提示翻译成所有语言。（iii）我们将新指标应用于BMLAMA，以评估各种仅编码器，仅解码器和编码器解码器PLM中的CLC，包括XLM-RoBERTa-large，mT5-large和BLOOM系列。我们分析了许多与CLC相关的语言属性，并为事实知识如何在语言之间渗透提供了新的见解。最后（iv）我们使用基于神经元可解释性的最先进的模型编辑技术（Meng 等人，2022 年）提供案例研究，提供初步证据，证明 CLC 可以预测插入语言 X 的事实是否会转移到语言 Y 中。</p><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2.Related Work"></a>2.Related Work</h2><p><strong>探索 PLM 中的事实知识</strong> 自 LAMA 首次提出以来（Petroni 等人，2019 年），基于提示的探测已成为评估 PLM 中事实知识的主要技术（Davison 等人，2019 年;布拉维等人，2020 年;申等人，2020;布朗等人，2020 年;阿尔甘米等人，2021 年;彭等人，2022 年）。给定元组（主体、关系、对象）中表示的知识，通过将主题填充到特定于关系的模板中来形成查询 q，该模板被馈送到 PLM 中。如果预测与对象一致，则认为模型具有此知识。例如，给定一组候选城市名称，当查询“中华人民共和国的首都是_”时，如果PLM在所有候选城市中正确答案“北京”的概率最高，则认为PLM捕获了这条知识。</p><p><strong>事实知识的多语言探索</strong> 除了大量关注英语的著作外，一些著名的研究通过将英语提示-对象对翻译成多种语言来多语言探索事实知识。X-FACTR（Jiang 等人，2020 年）和 MLAMA（Kassner 等人，2021 年）表明，由于其培训语料库的大小，不同语言的知识量之间存在很大差异。除了英语和少数其他高资源欧洲语言外，总体上报告的探测准确性非常低（即&lt;10%）。另一项相关工作， GeoMLAMA（Yin 等人，2022 年）专门探测了在不同地区可能有所不同的常识性知识，导致相当令人惊讶的发现，即探索某个国家（例如中国）知识的最佳语言通常不是给定国家的母语（例如中文）。所有这些研究的主要重点是评估每种语言编码的事实知识的数量，而不是了解这些知识如何在语言之间渗透。</p><p><strong>自洽性</strong> 自洽性是指 PLM 对同一查询的保留含义的释义输出相同答案的能力。英语PLM的自洽性在不同任务中都受到了关注（Li等人，2019;米切尔等人，2022 年;王等人，2023 年）。Fierro和Søgaard（2022）通过将自洽性的研究扩展到多语言PLM，方法是在每种语言中单独测量自洽性。他们的结果显示，所有语言的自洽性都很差。</p><p><strong>跨语言一致性</strong> 据我们所知，我们是第一个对多语言PLM中事实知识的跨语言一致性进行系统分析的公司，即PLM对不同语言提出的相同问题返回相同答案的程度。作为探索研究的一部分，Jiang等人（2020）计算了mBERT中两种语言之间重叠的正确预测的比例（参见第3.1节）。他们报告的总体比率较低，在最相似的对（英语 - 荷兰语）中只有34%的峰值，但没有进一步调查决定一致性的因素。此外，他们将这种分析限制在一个（仅编码器）模型，同时我们还检查了编码器-解码器和一系列仅解码器模型（参见第5.1节）。另一个区别是，<span style="color: #ff2020"><span style="background-color: #ff666680">我们对一致性采取了更全面的观点，即不正确但跨语言引用同一实体的预测也应被视为一致。</span></span>有趣的是，Ohmer 等人（2023 年）的并行工作建议使用模型预测的跨语言一致性作为评估其对特定单词形式之外的含义的理解的一种手段。他们在两个语言理解任务（释义识别和自然语言推理）中展示了他们的方法。尽管范围不同，但他们使用英语、德语和中文翻译对 ChatGPT 的评估表明，模型响应的一致性有限，这与我们的事实调查结果一致（参见第 5 节），并进一步表明这个问题在非常大规模的上一代 PLM 中仍然存在</p><h2 id="3-Measuring-Cross-Lingual-Consistentcy"><a href="#3-Measuring-Cross-Lingual-Consistentcy" class="headerlink" title="3.Measuring Cross-Lingual Consistentcy"></a>3.Measuring Cross-Lingual Consistentcy</h2><p><strong>任务定义</strong> 每种语言l ∈ L有一组定义为 Ql 的查询（即提示）。对于每个查询 qi ∈ Ql，有Ni对应候选项，例如，查询“史蒂夫乔布斯为 __ 工作”有 10 个候选者：苹果、任天堂、谷歌、WWE、亚历山大、德国、雅虎、柏林、BBC、Microsoft。每个查询都会馈送到 PLM，返回的概率用于计算每个候选单词的排名分数。分数计算取决于模型的类型（仅编码器、编码器解码器或仅解码器）以及候选单词分割为子单词的方式（请参阅附录 B 中的详细信息）。按排名分数排序后，Qi 的候选集表示为 {ci1， . . . ， cNi i }，其中 ci1 的预测概率最高，cNi i 的预测概率最低。请注意，现有的用于知识探测的多语言数据集（X-FACTR（Jiang 等人，2020 年）和 MLAMA（Kassner 等人，2021 年））在不同语言中具有不同数量的查询，这对于衡量一致性是有问题的。</p><h3 id="3-1Prioions-Work-Correct-Predictions-Overlap"><a href="#3-1Prioions-Work-Correct-Predictions-Overlap" class="headerlink" title="3.1Prioions Work:Correct Predictions Overlap"></a>3.1Prioions Work:Correct Predictions Overlap</h3><p>     基于每个 qi 和 q′ i 的预测 ci1 和 c′1 i（即排序候选列表的第一个元素），Jiang 等人 （2020） 计算正确预测的平均重叠率如下：</p><p><img src="/../imgs/$%7Bfiilename%7D/UTGQ3CDQ-1698899466973-21.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/UTGQ3CDQ-1698899466973-21.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;UTGQ3CDQ&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%22BSQLBJQX%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B303%2C459.39%2C527.5%2C518.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;374&quot; height=&quot;98&quot; src=&quot;attachments/UTGQ3CDQ.png&quot; ztype=&quot;zimage&quot;&gt;"><br>其中 1（·) 是指示函数，oi 和 o′i 分别是 qi 和 q′ i 的正确答案。</p><p>     由于他们的基准测试包含不同语言的不同数量的查询，因此它们通过丢弃 l 或 l′ 中不可用的样本来过滤每个语言对 （l， l′） 的查询集：</p><p><img src="/../imgs/$%7Bfiilename%7D/CKJAUHAN-1698899465379-19.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/CKJAUHAN-1698899465379-19.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;CKJAUHAN&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%22GIHZDH3X%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B307.5%2C333.39%2C526.5%2C367.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;365&quot; height=&quot;57&quot; src=&quot;attachments/CKJAUHAN.png&quot; ztype=&quot;zimage&quot;&gt;">\ <span style="background-color: #2ea8e580">由于筛选是分别对每个语言对完成的，因此这会导致不同的查询集，这限制了它们的结果在具有非常不同的筛选集的语言对之间的可比性。</span></p><h3 id="3-2This-Work-RankC-Metric"><a href="#3-2This-Work-RankC-Metric" class="headerlink" title="3.2This Work:RankC Metric"></a>3.2This Work:RankC Metric</h3><p><span style="background-color: #2ea8e580">为了确保不同语言对之间的可比性，我们要求基准测试中的所有查询及其相应的候选查询都翻译成所有语言。</span>因此，对于任何语言对 （l， l′），查询集的长度始终相等 |Ql|&#x3D; |Ql′|，第 i 个查询 Ni &#x3D; N ′ i 的候选项数也是如此。基于这些假设，我们提出了一种新的基于排名的一致性（RankC）指标，以有效地评估PLM中知识的跨语言一致性，而与准确性无关。<span style="background-color: #2ea8e580">我们不只是关注正确的预测，而是将所有候选的排名纳入考虑。</span>RankC的灵感来自信息检索的K（MAP@K）指标的平均平均精度（Schutze等人，2008）。与原版MAP@K不同，在 RankC K 中因查询而异。qi 的值 K 等于 Ni，即其候选者的数量。给定语言 l 和 l′，两种语言之间的一致性分数定义为所有翻译查询对 （qi， q′ i） ∈ （Ql， Ql′） 的一致性平均值：</p><p><img src="/../imgs/$%7Bfiilename%7D/4BUNNT2A-1698899462448-17.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/4BUNNT2A-1698899462448-17.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;4BUNNT2A&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%22UWV6KRQ7%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B72%2C596.89%2C292.5%2C645.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;368&quot; height=&quot;81&quot; src=&quot;attachments/4BUNNT2A.png&quot; ztype=&quot;zimage&quot;&gt;"><br>每个查询对的一致性是通过加权平均 P @j 函数计算的，该函数输出具有前 j 个最高概率的候选函数之间的重叠比率3：</p><p><img src="/../imgs/$%7Bfiilename%7D/9E3DW5HT-1698899458937-15.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/9E3DW5HT-1698899458937-15.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;9E3DW5HT&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%22IP7IB4WX%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B71.5%2C453.39%2C291%2C536.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;366&quot; height=&quot;139&quot; src=&quot;attachments/9E3DW5HT.png&quot; ztype=&quot;zimage&quot;&gt;"><br>每个 P @j的权重 wj 定义如下。</p><p><strong>基于排名的权重</strong> 直观地说，排名较高的候选人应该对一致性分数产生更大的影响。为了实现这一目标，RankC 对所有 P @js采用加权平均值，其中 j 较小的 P @j被赋予较高的权重 wj，以强调具有高概率的候选人的影响。但是，预测概率不能直接使用，因为它们对于 qi 和 q′ i 的候选者是不同的。为了解决这个问题，我们引入了基于softmax的归一化权重，而不是值j：</p><p><img src="/../imgs/$%7Bfiilename%7D/SGCTJSZD-1698899456427-13.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/SGCTJSZD-1698899456427-13.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;SGCTJSZD&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%22B2LWRTBH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B71.5%2C223.39%2C292%2C274.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;368&quot; height=&quot;86&quot; src=&quot;attachments/SGCTJSZD.png&quot; ztype=&quot;zimage&quot;&gt;"><br>其中 Ni 是查询 qi 和 q′ i.4 的候选数量 结合等式 3、4 和 5，RankC 指标变为：</p><p><img src="/../imgs/$%7Bfiilename%7D/ML4DE4BX-1698899454568-11.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/ML4DE4BX-1698899454568-11.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;ML4DE4BX&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%22UUK6AGQ7%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B69.5%2C92.89%2C291.5%2C180.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;370&quot; height=&quot;147&quot; src=&quot;attachments/ML4DE4BX.png&quot; ztype=&quot;zimage&quot;&gt;"><br>附录D给出了RankC计算示例，以及高&#x2F;低RankC的解释</p><p><img src="/../imgs/$%7Bfiilename%7D/UNBA9DMX-1698899452209-9.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/UNBA9DMX-1698899452209-9.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;UNBA9DMX&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%22JHHBPXWD%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B299%2C662.89%2C527.5%2C774.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;381&quot; height=&quot;187&quot; src=&quot;attachments/UNBA9DMX.png&quot; ztype=&quot;zimage&quot;&gt;"><br>     我们在同一数据集上对RankC与以前使用的指标（COverlap，参见公式1)进行了实证比较。附录F中的结果表明，<span style="background-color: #2ea8e580">几乎所有具有高COVERLAP分数的语言对也获得了较高的RankC分数。此外，RankC揭示了一些新的高一致性对，由于探测精度低，它们的COverlap评分较低。</span></p><h2 id="4-Experimental-Setup"><a href="#4-Experimental-Setup" class="headerlink" title="4.Experimental Setup"></a>4.Experimental Setup</h2><p><strong>数据集</strong> 如第 3.2 节所述，RankC 要求将查询及其候选语言翻译成所有评估语言。因此，我们从 X-FACTR（Jiang 等人，2020 年）和 MLAMA（Kassner 等人，2021 年）中提取所有满足此标准的查询。我们将生成的多并行数据集称为平衡多语言模型分析（BMLAMA），并以两个版本发布：BMLAMA-17，包括17种语言的6.7k查询（接近X-FACTR，包括23种语言），BMLAMA-53包括53种语言的3k查询（与MLAMA相同）。详细统计数据如表1所示。</p><p><strong>模型</strong> 多语言知识探索的先前工作（Jiang等人，2020;Kassner 等人，2021 年）专注于仅编码器的 PLM，例如 mBERT（Devlin 等人，2019 年）或 XLM-RoBERTa（Liu 等人，2019 年）。然而，由于纯解码器 PLM 已成为当前 NLP 时代的主流，我们的实验还包括仅解码器的 BLOOM 系列（560m、1.1b、1.7b、3b 参数）（Scao 等人，2022 年）和编码器-解码器 mT5large （1.2b）（Xue 等人，2021 年），此外还包括仅编码器的 XLM-RoBERTa-large（354m）。</p><h2 id="5-Main-Consistency-Result"><a href="#5-Main-Consistency-Result" class="headerlink" title="5.Main Consistency Result"></a>5.Main Consistency Result</h2><p>在查看一致性之前，我们在图 2 中展示了 BMLAMA-17.5 上三个 PLM 的实际探测精度结果，我们首先注意到，<span style="background-color: #2ea8e580">仅编码器 XLM-RoBERTa-large 和编码器解码器 mT5-large 模型在平均探测精度方面优于整个仅解码器的 BLOOM 系列。三种型号的跨语言趋势相似，但是，BLOOM以远高于所有其他语言的英语准确性脱颖而出。</span><span style="background-color: #ff666680">关于模型大小（BLOOM 系列，绿条），我们发现增加参数数量会导致事实探测精度的轻微但一致的提高，</span>这与以前的工作一致（Petroni 等人，2019 年）。</p><p>    我们的XLM-RoBERTa-large结果与Jiang等人（2020）在XFACTR上报告的结果一致，证明了我们的多并行数据集BMLAMA的可靠性。</p><h3 id="5-1Consistency-in-Different-PLMs"><a href="#5-1Consistency-in-Different-PLMs" class="headerlink" title="5.1Consistency in Different PLMs"></a>5.1Consistency in Different PLMs</h3><p>图 3 显示了三种 PLM 的 RankC 结果。第一个观察结果是，所有模型的平均一致性6都相低，BLOOM3b（25%）最低。这一阴性结果与Jiang等人（2020）在mBERT上观察到的正确预测的低重叠率一致。</p><p>     <span style="background-color: #ff666680">我们现在放大了不同语言对之间的比较，这是通过新的RankC指标和平衡数据集BMLAMA实现的。在这里，我们发现欧洲语言英语，法语，荷兰语，西班牙语和加泰罗尼亚语在mT5-large和XLM-RoBERTa-large方面共享了相当多的知识。类似的模式适用于BLOOM-3b，但荷兰语除外，这是意料之中的，因为该语言未包含在此模型的训练语料库中。此外，越南语和土耳其语在所有PLM中都与上述欧洲语言实现了显着的一致性。这些语言的一个共同特点是它们都使用相同的脚本（拉丁语）。另一个值得注意的高一致性对是俄语和乌克兰语，使用相同脚本（西里尔文）并且也密切相关的两种语言。这些观察表明，各种语言属性会影响多语言知识的CLC。我们将在第 6.1 节中检查许多此类属性。</span></p><h3 id="5-2Effect-of-Model-Size"><a href="#5-2Effect-of-Model-Size" class="headerlink" title="5.2Effect of Model Size"></a>5.2Effect of Model Size</h3><p>如上所述（图 2 中的绿条）和之前的工作（Petroni 等人，2019 年）所观察到的，<span style="background-color: #ff666680">当其他因素固定时，检索正确知识的能力会随着模型大小的增长而增长。</span>我们问CLC是否也是如此。<span style="background-color: #5fb23680">然而，图4中的BLOOM结果显示，从我们系列中最小的模型移动到最大的模型时，平均RankC（+2%）只有很小的变化，即参数增加了5倍。</span>虽然这种模式不能安全地推广到其他模型，但它确实表明，在非常大规模的PLM中，跨语言一致性可能仍然是一个问题8。</p><h2 id="6-Typological-Similarity"><a href="#6-Typological-Similarity" class="headerlink" title="6.Typological Similarity"></a>6.Typological Similarity</h2><p>类型学特征已被证明可用于模拟语言之间的细粒度相似性，并指导各种多语言 NLP 任务的迁移学习技术（Ponti 等人，2019 年;尤斯图恩等人，2022 年）。这些特征是否也能解释在多语言PLM中观察到的事实知识一致性的一些差异？<span style="background-color: #2ea8e580">例如，我们可能期望具有相似语法和词序或具有相关词汇的语言共享更高的语言程度。在多语言模型中。我们可能还期望在同一世界地区使用的语言更有可能在训练数据中遇到相同实体和事件的提及。</span></p><p>     为了回答这个问题，我们从lang2vec（Littell等人，2017）获得了四种类型的<span style="background-color: #2ea8e580">类型相似性（句法，遗传，地理和语音）</span>，这是一个开源库，提供基于各种类型学数据库的预先计算的相似性.9接下来，我们计算RankC分数与BMLAMA中所有语言对的类型相似性之间的皮尔逊相关系数（Cohen等人，2009）。</p><p>     表2显示了BMLAMA-17和较小但多语言的BMLAMA-53.10的结果 对于BMLAMA-17，<span style="background-color: #ff666680">我们发现RankC与遗传相似性具有中等相关性，与地理相似性具有弱相关性，但与句法相似性没有显着相关性。正如预期的那样，没有观察到与语音相似性的相关性。更全面的数据集BMLAMA-53上的相关性结果相似，除了句法相似性获得弱正相关。</span>有点令人惊讶的是，在这个更大的数据集中，<span style="background-color: #5fb23680">遗传和地理上的相似性使它们的相关性略有下降，这可能是由于低资源语言的类型向量中存在噪声。</span></p><p>     遗传相关语言的一个重要特征是它们往往有很多单词共同或具有共同祖先。<span style="background-color: #ff666680">因此，RankC与遗传相似性的中等相关性，加上与句法和地理相似性的弱相关性，表明词汇重叠可能是CLC比具有相似的语法和词序或在附近地区使用更重要的因素。</span></p><h3 id="6-2Subword-Vocabulary-Overlap"><a href="#6-2Subword-Vocabulary-Overlap" class="headerlink" title="6.2Subword Vocabulary Overlap"></a>6.2Subword Vocabulary Overlap</h3><p>基于上述观察结果，我们研究了词汇重叠的粗略测量是否也可以很好地预测CLC。具体来说，我们提取了我们评估语言中严格平行语料库的词汇表，并测量它们的成对重叠：</p><p><img src="/../imgs/$%7Bfiilename%7D/9DVJYQ7U.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/9DVJYQ7U.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;9DVJYQ7U&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%22Q2E54XNM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B70.385%2C344.005%2C291.923%2C381.505%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%227%22%7D%7D&quot; width=&quot;369&quot; height=&quot;62&quot; src=&quot;attachments/9DVJYQ7U.png&quot; ztype=&quot;zimage&quot;&gt;"><br>我们考虑两个语料库：BMLAMA 本身和 Flores-200（Costa-jussà 等人，2022 年)。前者预计非常相关，但由此产生的相关性可能不太可推广，因为它是衡量一致性本身的同一语料库。相比之下，后者是一组混合域的 2k 个句子，从英语翻译成 200 种语言，用于机器翻译评估。因为我们对不同语言使用完全相同的单词表示的程度感兴趣，所以我们在测量词汇重叠之前用模型的分词器对语料库进行分割，这使得这个指标模型依赖于。</p><p>     如表2（右）所示，BMLAMA上的皮尔逊相关分数证明，<span style="background-color: #ff666680">子词词汇重叠对PLM中知识的跨语言一致性有显著的强烈影响，掩盖了遗传的影响</span></p><p><img src="/../imgs/$%7Bfiilename%7D/7UYEHQJN.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/7UYEHQJN.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;7UYEHQJN&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%22I4F9IANM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B294.808%2C500.928%2C528.462%2C596.698%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%227%22%7D%7D&quot; width=&quot;389&quot; height=&quot;159&quot; src=&quot;attachments/7UYEHQJN.png&quot; ztype=&quot;zimage&quot;&gt;"><br>相似。<span style="background-color: #ff666680">这表明事实知识可能主要以相当肤浅的方式（通过共享使用一些子词嵌入）渗透到语言之间，相反，即使语言相关，在没有这种锚点的情况下，它也可能受到阻碍。</span>例如，<span style="background-color: #2ea8e580">BLOOM-3b中一致性最高的对是乌克兰语-俄语，它们位于语言树中（遗传相似性：0.8），并且总体上共享大量子词词汇（词汇重叠：0.76）。然而，在查询大卫·卡梅伦的工作地点时，BLOOM-3b预测的是俄语查询（“伦敦”）中的正确答案，但乌克兰语（“莫斯科”）中的错误答案。</span>这表明<span style="background-color: #2ea8e580">正确的知识没有从俄语转移到乌克兰语，因为这两个查询之间的子词重叠有限（0.17）。</span>当在Flores上测量词汇重叠时（表2的最后一列），相关性较低，但仍然显着为正，表明我们的发现不仅限于我们的基准。跨语言知识一致性与词汇重叠之间的相关性如图5所示。<span style="background-color: #2ea8e580">CLC对浅词汇重叠的强烈依赖部分解释了为什么增加模型大小没有积极的影响</span>（参见第5.2节)。<span style="background-color: #5fb23680">我们推测，较大的子单词词汇实际上可能导致较低的一致性，因为在任何两种语言之间共享部分单词的机会会降低。我们将对这一假设的进一步调查留给未来的工作。</span></p><h2 id="7-Case-Study-Cross-Lingual-Consistency-and-Knowledge-Incorporation"><a href="#7-Case-Study-Cross-Lingual-Consistency-and-Knowledge-Incorporation" class="headerlink" title="7.Case Study: Cross-Lingual Consistency and Knowledge Incorporation"></a>7.Case Study: Cross-Lingual Consistency and Knowledge Incorporation</h2><p>之前的工作（Jiang et al., 2020；Kassner et al., 2021；Artetxe et al., 2022）和我们的探索结果表明，低资源语言的知识量是有限的。简单地在更大的非英语语料库上训练新的 PLM 非常耗时，而且大多数大学和其他研究机构都无法承担其成本（Ding 等人，2022）。<span style="background-color: #2ea8e580">一个有前景的解决方案是通过微调方法整合外部知识（Hu et al., 2022）或以非常有针对性的方式直接编辑 PLM 的权重</span>（De Cao et al., 2021；Meng et al., 2022）。<span style="background-color: #2ea8e580">为了使该过程在多语言场景中可行并避免意外影响，重要的是要了解以一种语言插入知识是否以及如何影响 PLM 中的其他语言，包括最易受影响和最不易受影响的语言</span>。在本节中，我们将针对这个问题及其与 CLC 的相互作用进行第一个案例研究。</p><p><strong>Rank-One 模型编辑（ROME）</strong>由Meng 等人提出。 (2022)，这种基于神经元可解释性的最先进的模型编辑技术在特异性和泛化方面都优于其他几种编辑技术。简而言之，<span style="background-color: #2ea8e580">该技术直接修改 PLM 早期前馈层中的权重，其中事实关联已通过因果干预找到。</span></p><p>**反事实知识 **遵循孟等人。 （2022），我们考虑将反事实知识插入 PLM 的任务，例如事实上错误的“史蒂夫·乔布斯曾为微软工作”。由于在预训练期间从未观察到此类事实关联，因此这种方法避免了插入模型已认为可能的事实的风险。</p><p><strong>案例研究</strong> 我们研究了 BLOOM-3b，因为 ROME 目前仅适用于仅解码器模型。选择英语作为插入事实的源语言。作为目标语言，我们选择两种与英语具有高度一致性（RankC）的语言（西班牙语和越南语）和两种RankC 较低（匈牙利语和希腊语）。这些语言在脚本和与英语的相关性方面也各不相同。通过确保 PLM 在编辑之前选择最有可能的最初正确答案来挑选六个查询。我们还确保，对于每个编辑的知识，主题和客体实体在所有语言中都是相同的标记。这消除了这样的担忧：例如，西班牙语和越南语仅仅因为所评估的查询中主语和宾语标记的词汇共现而获得与英语一致的预测。对于评估，我们遵循孟等人的设置。 （2022）并将候选集缩小为两个单词——一个正确，一个错误。后者是ROME的编辑目标。根据每个查询，PLM 计算正确和错误答案的 logit 值，分别为 logitC 和 logitW。这些 logits 在不同语言之间差异很大。为了关注原始事实和编辑事实之间的关系，我们按照之前的工作（Sarti et al., 2023）将 logits 标准化为</p><p><img src="/../imgs/$%7Bfiilename%7D/JTSVCJAA.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/JTSVCJAA.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;JTSVCJAA&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRSQJFTAR%22%2C%22annotationKey%22%3A%22LQTSE2UG%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B300.577%2C424.775%2C526.731%2C775.544%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FRHFZGHTF%22%5D%2C%22locator%22%3A%228%22%7D%7D&quot; width=&quot;377&quot; height=&quot;585&quot; src=&quot;attachments/JTSVCJAA.png&quot; ztype=&quot;zimage&quot;&gt;"><br>     表 3 显示了三个查询的结果。一个非常清晰的模式出现了：<span style="background-color: #ff666680">当一个事实被插入到英语中时，它会一致地传播到高 CLC 语言（即西班牙语和越南语）。相反，低 CLC 语言（匈牙利语和希腊语)受到的影响要小得多，即使在模型编辑后，仍然会输出更高的正确答案概率。</span>附录 J 中给出的其余三个查询显示了相同的模式。</p><p>    尽管我们的研究规模较小，但结果表明，<span style="background-color: #5fb23680">CLC 不仅是 PLM 中现有知识的副产品，而且还代表了在将新知识融入其他语言时对语言扰动的敏感性。</span>我们认为这是增强多语言场景中模型编辑优势的一个有前途的方向。</p><h2 id="8-Conclusion"><a href="#8-Conclusion" class="headerlink" title="8.Conclusion"></a>8.Conclusion</h2><p>我们分析了多语言大型 PLM 中事实知识的跨语言一致性 (CLC)。我们提出了一个新的指标 RankC，用于独立于准确性来量化一致性，并将其应用于跨语言平衡的事实知识基准。我们的综合分析表明，<span style="background-color: #ff666680">(i) 不同 PLM 的平均 CLC 较低，并且不受模型大小的明显影响；</span> <span style="background-color: #ff666680">(ii) PLM 内不同语言对的 CLC 与遗传相似性显着相关，但与词汇重叠的相关性明显更强；</span> <span style="background-color: #ff666680">(iii) 通过模型编辑插入到语言 X 中的新事实更有可能传播到具有 X 的 CLC 分数较高的语言。</span></p><h3 id="Limitations"><a href="#Limitations" class="headerlink" title="Limitations"></a>Limitations</h3><p>由于 GPU 资源的限制，我们无法测试大于 BLOOM-7.1b 的模型。鼓励在未来的工作中将我们的分析扩展到更大规模的模型，看看是否得出相同的结论。<span style="background-color: #5fb23680">然而，图4的结果表明，随着模型规模的增加，平均CLC增长极其缓慢。 BMLAMA 中包含的事实虽然被认为具有普遍性，但可能与西方世界更相关，这可能会在评估中引入偏见。我</span>们从 BMLAMA 所建立的基准中继承了这个问题。解决这个问题并非易事，特别是在比较工作中，需要探究跨语言的确切事实集，并且应该在未来的工作中予以关注。</p>]]></content>
      
      
      <categories>
          
          <category> NLP顶会 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EMNLP2023 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>我们可以编辑多模态大型语言模型吗？</title>
      <link href="/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/"/>
      <url>/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/</url>
      
        <content type="html"><![CDATA[<h1 id="我们可以编辑多模态大型语言模型吗？"><a href="#我们可以编辑多模态大型语言模型吗？" class="headerlink" title="我们可以编辑多模态大型语言模型吗？"></a>我们可以编辑多模态大型语言模型吗？</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在本文中，我们重点关注编辑多模态大型语言模型（MLLM）。与编辑单模态LLMs相比，多模态模型编辑更具挑战性，需要在编辑过程中进行更高水平的审查和仔细考虑。为了促进这一领域的研究，我们<span style="background-color: #ff666680">构建了一个名为 MMEdit 的新基准，用于编辑多模式LLMs并建立一套创新的评估指标</span>。我们进行了涉及各种模型编辑基线的综合实验，并分析了编辑不同组件对多模式LLMs的影响。根据经验，我们注意到<span style="background-color: #ff666680">以前的基线可以在一定程度上实现多模态 LLM 的编辑，但效果仍然差强人意，这表明这项任务的潜在难度。</span>我们希望我们的工作能够为 NLP 社区提供见解1。</p><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>随着大型语言模型 (LLM) 的广泛部署（Zhao 等人，2023），在不产生大量再培训成本的情况下保持其知识准确和最新的必要性变得越来越重要（Sinitsin 等人，2020）。<span style="background-color: #2ea8e580">先前的研究引入了知识编辑方法，旨在逐步向语言模型注入一组新的事实</span>（Mitchell 等人，2022a；Han 等人，2023；Hartvigsen 人，2022；Zhong 等人，2023；Gandikota等人，2023；姚等人，2023）。</p><p>      与单模态模型编辑不同，多模态LLMs的编辑任务因其固有的多样性和复杂性而面临相当大的挑战。具体来说，<span style="background-color: #5fb23680">多模式模型的错误输出可能源于各种模式的协同效应。输出不正确可能不仅仅源于LLMs，类似于误读或误识别等人为错误</span>（例如，色盲影响图像中的颜色识别）。如图1所示，在编辑之前，模型将物体错误地识别为“梯子”而不是正确的“障碍物”，从而导致错误的预测。编辑后，模型准确识别了“障碍”。请注意，多模态LLMs（Yin et al., 2023）的效用正在增加，<span style="background-color: #5fb23680">但缺乏相应的数据集资源和用于编辑多模态大语言模型的基准。</span></p><p>     为了促进这一领域的研究，我们第一步构建了一个多模态模型编辑基准：<span style="background-color: #ff666680">称为 MMEdit，它包含两个子任务：编辑 VQA 和编辑图像标题。</span>具体来说，我们<span style="background-color: #ff666680">遵循单模态模型编辑方法（Mitchell et al., 2022a；Cao et al., 2021；Mitchell et al., 2022b）来构建数据集，这扩展了之前的评估原则，即 Reliability2、Locality3 和通用性4，多模式设置。对于可靠性评估，我们从严格的数据收集开始，收集表现不佳的多模态模型数据来创建专用的可靠性编辑数据集（§3.2.1）。对于局部性评估，我们将其分为文本局部性和多模态局部性，以评估多模态 LLM 的稳定性（第 3.2.2 节）。对于通用性评估，与局部性类似，我们将其分为文本通用性和多模态通用性，并利用 ChatGLM (Du et al., 2022) 和稳定扩散 (Rombach et al., 2022) 生成重新措辞的文本以及重新措辞的图像进行评估（第 3.2.3 节）。我们评估了 MMEdit 上的几种知识编辑方法。</span>根据经验，我们注意到<span style="background-color: #5fb23680">当前的编辑方法对于编辑多模态语言模型中的文本模型有效，但对于编辑视觉模块则不那么有效</span>。例如，在编辑BLIP-2模型的语言模块时，MEND的可靠性可以达到92.6%，但在编辑视觉模块时只能达到14.1%，表明该任务的潜在难度和机遇。总的来说，我们的主要贡献如下：</p><ul><li>我们迈出了第一步，研究编辑多模态LLMs，将模型编辑扩展到多模态设置。</li></ul><!----><ul><li>我们提出了MMEdit，一个新的基准，用于评估多模态模型编辑方法的可靠性、局部性和通用性。</li></ul><!----><ul><li>我们使用各种基线进行实验，证明虽然当前的方法可以在一定程度上帮助多模式编辑，但结果仍然达不到完全满意的程度。我们将公开代码和数据集以用于未来的研究目的。</li></ul><h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h2><h3 id="2-1-Multimodal-Language-Models"><a href="#2-1-Multimodal-Language-Models" class="headerlink" title="2.1 Multimodal Language Models"></a>2.1 Multimodal Language Models</h3><p>     多模态学习 (MML)（Xu 等人，2022a；Yin 等人，2023）提供了一种构建 AI 模型的整体方法，该模型可以从各种数据模态中提取和关联信息。由于其社会意义，MML 在研究界站稳了脚跟，在过去十年中巩固了自己作为一个重要研究领域的地位。视<span style="background-color: #2ea8e580">觉语言预训练是MML的重要分支之一，旨在学习在各种视觉和语言任务上具有改进性能的多模态基础模型</span>。 Vision Transformer (ViT)（Dosovitskiy et al., 2021）<span style="background-color: #2ea8e580">是一项开创性的工作，贡献了端到端将 Transformers 的编码器应用于图像的解决方案。</span> CLIP（Radford et al., 2021）<span style="background-color: #2ea8e580">提出了一种方法，使用多模态预训练将分类转换为检索任务，使预训练模型能够解决零样本识别问题</span>。最近，LLaMA (Touvron et al., 2023)、BLOOM (Scao et al., 2022) 和 ChatGPT (OpenAI, 2022) 等 LLM 的进步得到了扩大训练数据和增加参数的支持，最近取得了重大成功。这些模型展示了令人印象深刻的语言理解、生成和知识推理能力，增强了它们理解自然语言和生成高质量、基于上下文的文本的能力。大型语言模型的发展刺激了自回归语言模型作为视觉语言任务中的解码器的广泛使用。<span style="background-color: #2ea8e580">利用跨模态迁移，这种方法可以实现语言和多模态领域之间的知识共享</span>（Gao et al., 2023; Liu et al., 2023; Li et al., 2023a; Ye et al., 2023; Zhu et al., 2023；Li 等人，2023b；Zhang 等人，2023）。</p><h3 id="2-2-Model-Editing"><a href="#2-2-Model-Editing" class="headerlink" title="2.2 Model Editing"></a>2.2 Model Editing</h3><p>     LLMs（Zhao et al., 2023）主要从训练语料库中获取知识。然而，数据集的质量并不总是得到保证，可能会将有害或不正确的信息集成到模型中（Hernandez 等人，2023）。<span style="background-color: #2ea8e580">一种解决方案是使用更新的知识重新训练模型，尽管这可能成本高昂且难以实施</span>。或者，<span style="background-color: #2ea8e580">可以考虑使用一些更新的事实进行微调，但它存在过度拟合和灾难性遗忘的风险</span>（Zhai et al., 2023）。为了解决这些问题，（Sinitsin et al., 2020）提出了模型编辑，旨在高效、准确地改变模型中存储的事实知识。这种方法应用于各个领域（Mao et al., 2023; Onoe et al., 2023; Xu et al., 2022b; Wang et al., 2023a; Li et al., 2023c），并且研究数量不断增加调查编辑的影响（Ilharco 等人，2023；Gupta 等人，2023；Hase 等人，2023；Cohen 等人，2023；Wu 等人，2023；Wang 等人，2023b；Gandikota 等人等人，2023；Li 等人，2023d）。目前，模型编辑方法主要有三种类型：<span style="background-color: #2ea8e580">1）元学习方法，2）定位然后编辑方法，3）上下文知识编辑方法。</span></p><p><strong>元学习方法</strong>。 MEND（Mitchell 等人，2022a）和知识编辑器 (KE)（Cao 等人，2021）提出了<span style="background-color: #2ea8e580">涉及外部编辑器的方法，能够学习用于知识更新的最佳参数集 θ，同时施加约束以保持模型稳定性。</span> CaliNET (Dong et al., 2022) 和 T-Patcher (Huang et al., 2023) 从 (Dai et al., 2022) 中汲取灵感，<span style="background-color: #2ea8e580">将额外的可训练参数引入到预训练语言模型的前馈模块中。</span> SERAC（Mitchell 等人，2022b）<span style="background-color: #2ea8e580">利用显式记忆来存储编辑，并学习对它们进行推理，以根据需要调整基本模型的预测。</span></p><p><strong>定位然后编辑方法</strong>。 ROME（Meng et al., 2022a）<span style="background-color: #2ea8e580">提出了采用因果中介分析来识别编辑区域的方法。</span> ROME 发现记忆的事实关联可以精确定位到 GPT 模型中的特定位置。然而，ROME 的一个显着限制是它一次只能编辑一个事实。为了解决这个问题，Meng 等人。 (2022b)提出了一种称为MEMIT的新方法，<span style="background-color: #2ea8e580">它是之前工作ROME的继承者，它对单层的MLP权重进行rankone修改，以将内存直接写入模型中。</span></p><p><strong>上下文知识编辑方法</strong>。 InContext Learning (ICL)（Brown et al., 2020）<span style="background-color: #2ea8e580">表示一种免训练范式，其中知识是从输入上下文中直接串联的演示中获取的</span>。最近出现了一种新颖的编辑范式，它<span style="background-color: #2ea8e580">利用LLMs理解上下文的能力（Zheng et al., 2023），从而实现基于上下文的模型编辑，指导模型的生成过程，并提供高效、轻量级的模型方法编辑</span>。迄今为止的模型编辑方法主要迎合单模态场景，在多模态编辑方面留下了空白。据我们所知，我们是第一个研究LLMs多模式模型编辑的人，并为促进该领域的研究提供了新的基准。</p><h2 id="3-Editing-Multimodal-LLMs"><a href="#3-Editing-Multimodal-LLMs" class="headerlink" title="3 Editing Multimodal LLMs"></a>3 Editing Multimodal LLMs</h2><p>我们在图 2 中说明了多模态编辑的建议任务。我们将介绍任务定义（§3.1）、（§3.2）中的数据集构建细节、多模态模型（§3.3）以及我们在实验。</p><p><img src="/../imgs/$%7Bfiilename%7D/KQ9KJ8DQ.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/KQ9KJ8DQ.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;KQ9KJ8DQ&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22MS6FUQ8I%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B68.654%2C497.467%2C526.731%2C777.852%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;763&quot; height=&quot;467&quot; src=&quot;attachments/KQ9KJ8DQ.png&quot; ztype=&quot;zimage&quot;&gt;"></p><h3 id="3-1-Task-Definition"><a href="#3-1-Task-Definition" class="headerlink" title="3.1 Task Definition"></a>3.1 Task Definition</h3><p>假设我们有一个由 *θ <em>参数化的多模态 LLM</em> f *（由两部分组成，由 θ<sub>vision</sub> 和 θ<sub>text</sub> 参数化的 f<sub>vision</sub> 和 f<sub>text </sub>），将输入 i<sub>e</sub> 和 x<sub>e</sub> 映射到 y<sub>o</sub> 的预测，其中 i<sub>e </sub>指的是编辑图像输入，xe 指的是编辑文本提示输入和 y<sub>o</sub> 表示为原始输出。我们将 M 表示为特定指标的符号表示，下标表示特定度量，上标表示变化编辑数据。我们准备第 3.2.1 节中所述的编辑数据集，其表示为 Dedit。受到姚等人的启发。 （2023），我们引入了一系列多模式模型编辑指标。</p><p><img src="/../imgs/$%7Bfiilename%7D/ZFVB46CA.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/ZFVB46CA.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;ZFVB46CA&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22MJQK4385%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B64.853%2C526.008%2C292.5%2C791.596%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;379&quot; height=&quot;442&quot; src=&quot;attachments/ZFVB46CA.png&quot; ztype=&quot;zimage&quot;&gt;"></p><p><strong>可靠性</strong>。需要编辑可靠性才能将预测从 y<sub>o</sub> 更改为 y<sub>e</sub>。直观上，我们需要的是更新后的 θe，其中 f（即 x<sub>e</sub>;θ<sub>e</sub>）&#x3D; y<sub>e</sub>。为了衡量可靠性，我们使用编辑准确性，如下所述：</p><p><img src="/../imgs/$%7Bfiilename%7D/HSTRNPML.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/HSTRNPML.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;HSTRNPML&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22NS7JGJWI%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B68.824%2C342.919%2C292.059%2C378.214%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;372&quot; height=&quot;59&quot; src=&quot;attachments/HSTRNPML.png&quot; ztype=&quot;zimage&quot;&gt;"><br>其中θ<sub>e</sub>指的是编辑后的参数。</p><p><strong>局部性</strong>。为了保持模型的稳定性，必须最大限度地减少编辑对模型更广泛的知识库造成的意外副作用。为了实现这一目标，我们引入了两个指标：M <sup>Text</sup> <sub>loc</sub> (T-Locality) 和 M<sup>Image</sup> <sub>loc</sub> (M-Locality)，这两个指标都是为了在编辑过程中保持模型的稳定性。鉴于多模态语言模型中的知识是从LLMs继承的，保护这些知识至关重要。考虑到这一目标，我们搁置了模型的视觉辨别模块，而是采用基本的问答数据集 D<sub>loc-t</sub>，如第 3.2.2 节中所述。我们定义问题为x，答案为y，如下：</p><p><img src="/../imgs/$%7Bfiilename%7D/J24EN8UP.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/J24EN8UP.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;J24EN8UP&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22M2ZHI4QM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B67.941%2C93.214%2C292.941%2C133.361%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;375&quot; height=&quot;67&quot; src=&quot;attachments/J24EN8UP.png&quot; ztype=&quot;zimage&quot;&gt;"><br>视觉编码器在多模态语言模型中发挥着关键作用，将将图像转换为矢量表示，以便与自然语言文本共同编码。</p><p><img src="/../imgs/$%7Bfiilename%7D/IPZFZHUW.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/IPZFZHUW.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;IPZFZHUW&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22ZIQHSJG7%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B301.765%2C653.066%2C528.529%2C777.919%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;378&quot; height=&quot;208&quot; src=&quot;attachments/IPZFZHUW.png&quot; ztype=&quot;zimage&quot;&gt;"></p><p>因此，我们必须考虑对该模块进行任何修改的潜在后果。我们构建表示为 D<sub>loc-v</sub> 的数据集用于测试 MImage loc ，并计算如下：</p><p><img src="/../imgs/$%7Bfiilename%7D/4NC86SS7.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/4NC86SS7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;4NC86SS7&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22K2BYGI7E%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B302.647%2C522.478%2C532.941%2C557.772%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;384&quot; height=&quot;59&quot; src=&quot;attachments/4NC86SS7.png&quot; ztype=&quot;zimage&quot;&gt;"><br>其中（iv，xv，yv）是范围外数据，θe表示编辑数据更新的参数（即xe，ye)。</p><p><strong>泛化性</strong>。在整个编辑过程中，仅仅修改个别错误的输入是不够的。修订后的模型还应保留泛化能力，并始终为等效输入（例如改写的句子）生成一致的输出，如图 3 所示。虽然以前的单模态模型编辑任务仅需要考虑改写的文本，但多模态场景需要泛化以及图像。为了解决这个问题，我们引入了两个泛化考虑因素：MText gen (TGenerality) 和 MImage gen (M-Generality)，其表示如下：</p><p><img src="/../imgs/$%7Bfiilename%7D/BS3DHYZH.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/BS3DHYZH.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;BS3DHYZH&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22SSPQQ5PC%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B303.088%2C236.596%2C527.206%2C296.596%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;374&quot; height=&quot;100&quot; src=&quot;attachments/BS3DHYZH.png&quot; ztype=&quot;zimage&quot;&gt;"><br>其中 ir 表示重新表述的图像，xr 指重新表述的文本提示，N (x) 表示 x 范围内的对象。</p><h3 id="3-2-Datasets"><a href="#3-2-Datasets" class="headerlink" title="3.2 Datasets"></a>3.2 Datasets</h3><p>我们构建的数据集MMEdit主要包含两个子任务：编辑VQA（E-VQA）和编辑图像标题（E-IC）。</p><h4 id="3-2-1-Reliability-Dataset-Construction"><a href="#3-2-1-Reliability-Dataset-Construction" class="headerlink" title="3.2.1 Reliability Dataset Construction"></a>3.2.1 Reliability Dataset Construction</h4><p>为了对我们的实验进行基准测试，我们选择了两个常见的多模态任务：视觉问答（VQA）（Antol et al., 2015）和图像</p><p><img src="/../imgs/$%7Bfiilename%7D/TT279E4N.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/TT279E4N.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;TT279E4N&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%229KCXYY3D%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B60%2C488.39%2C297%2C778.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%225%22%7D%7D&quot; width=&quot;395&quot; height=&quot;483&quot; src=&quot;attachments/TT279E4N.png&quot; ztype=&quot;zimage&quot;&gt;"><br>字幕（Herdade 等人，2019）。 <span style="background-color: #2ea8e580">VQA 旨在设计一种算法，不仅可以理解图像中的视觉内容，还可以理解用于查询该图像的自然语言，并随后生成这些查询的精确答案</span>。<span style="background-color: #2ea8e580">图像字幕是设计能够理解图像视觉内容的算法，随后用自然语言生成连贯且精确的图像描述</span>。在本研究中，我们选择 BLIP-2 OPT。我们的基础编辑数据源自两个评估数据集的次优条目，即 VQAv2（Goyal 等人，2017）和 COCO Caption（Chen 等人，2015)。</p><p>     除了基本的编辑数据之外，利用其他数据也至关重要。这些数据不仅有助于编辑过程，还验证更改的有效性，评估模型编辑的稳定性和通用性。</p><h4 id="3-2-2-Locality-Dataset-Construction"><a href="#3-2-2-Locality-Dataset-Construction" class="headerlink" title="3.2.2 Locality Dataset Construction"></a>3.2.2 Locality Dataset Construction</h4><p>我们必须仔细考虑多模态模型中编辑对语言功能的影响，<span style="background-color: #5fb23680">类似于我们如何评估手术后个体大脑的各个认知区域。</span></p><p><strong>文本局部性数据集</strong>。为了评估语言模型的稳定性，我们利用之前在 MEND 中使用的 NQ 数据集（Kwiatkowski 等人，2019）作为模型内 LLM 组件稳定性的基准。我们专门使用模型的输出预编辑和后期编辑来构建 KL 散点图，从而促进对模型编辑的约束。此外，我们还计算了保持 top-1 状态的实例比例，进一步量化了模型的稳定性。</p><p><strong>多模态局部性数据集</strong>。同样，验证编辑对视觉模块的影响也至关重要。因此，我们在多模态领域使用简单的数据集 OK-VQA（Marino 等人，2019），作为多模态视觉模块局部性的度量。我们再次在编辑过程之前和之后使用 logits 更新 KL 离散度约束。</p><h4 id="3-2-3-Generality-Dataset-Construction"><a href="#3-2-3-Generality-Dataset-Construction" class="headerlink" title="3.2.3 Generality Dataset Construction"></a>3.2.3 Generality Dataset Construction</h4><p>我们在多模态模型中提出了两种形式的通用性。共性数据集构建的整体流程如图4所示。</p><p><strong>文本泛化数据集</strong>。值得注意的是，LLMs表现出强大的会话能力和强大的解决问题的能力，这使我们能够制定任务指令，从而指导模型生成类似的文本输入。对于 E-VQA 任务，我们利用 ChatGLM（Du et al., 2022；Zeng et al., 2022）生成类似的查询。然而，对于E-IC任务，由于提示的简洁和相对直接，模型生成的输出质量并不令人满意。因此，我们采用手动编写的包含20条提示的模板来随机替换原来的提示。</p><p><strong>视觉泛化数据集</strong>。近年来，<span style="background-color: #2ea8e580">扩散模型（Ho et al., 2020）在图像生成领域取得了巨大成功。超越最初最先进的模型</span>：生成对抗网络（GAN）模型（Goodfellow 等人，2014）。扩散模型在许多图像生成任务中表现出色，并在各个应用领域中表现出了值得称赞的性能。<span style="background-color: #2ea8e580">稳定扩散（Rombach 等人，2022）是一种潜在的文本到图像扩散模型，能够根据给定的文本输入生成逼真的图像</span>。我们利用稳定扩散 2.1 来生成重新解释的图像。该数据集利用 COCO 数据集的标题描述来评估模型的图像泛化能力。</p><h3 id="3-3-Multimodal-Language-Models"><a href="#3-3-Multimodal-Language-Models" class="headerlink" title="3.3 Multimodal Language Models"></a>3.3 Multimodal Language Models</h3><p><strong>BLIP-2 OPT</strong>。 BLIP-2（Li et al., 2023b）<span style="background-color: #2ea8e580">是一种通用且高效的预训练策略，可从现成的冻结预训练图像编码器和冻结大型语言模型引导视觉语言预训练。该模型利用轻量级查询转换器来弥合视觉模态和文本模态之间的差距，并在各种视觉语言任务上实现最先进的性能</span>。我们选择 BLIP-2 OPT 作为基本编辑模型，它在视觉模块中利用 ViT-L，并选择无监督训练的 OPT 模型用于基于解码器的 LLM。迷你GPT-4。</p><p><strong>MiniGPT-4</strong>（Zhu et al., 2023）是一种类似于 BLIP-2 的有效视觉语言模型，利用冷冻视觉编码器与冷冻 Vicuna（Chiang et al., 2023）相结合。据报道，基于 LLaMA 构建的 Vicuna 根据 GPT-4 的评估标准达到了 ChatGPT 90% 的性能。 MiniGPT-4 添加了一个投影层，以使编码的视觉特征与 Vicuna 语言模型保持一致。 MiniGPT-4 采用与 BLIP-2 相同的预训练视觉组件，由 EVA-CLIP（Sun 等人，2023）的 Vit-G&#x2F;14 和 Q-Former 组成。</p><h3 id="3-4-Baselines"><a href="#3-4-Baselines" class="headerlink" title="3.4 Baselines"></a>3.4 Baselines</h3><p><strong>Finetune</strong>。微调已成为一种广泛采用的策略，用于使预训练的语言模型适应特定任务或领域（Cortes 等人，2015）。在我们的探索中，我们深入研究了两种不同的微调方法：一种专注于语言模型的最后一层。以BLIP-2 OPT模型为例，我们对OPT模型的第31个解码器层进行微调。另一个目标是多模态语言模型中的视觉块，具体来说，我们微调 Q-former 模型以过度拟合编辑数据集。</p><p><strong>MEND</strong>。具有梯度分解的模型编辑器网络（Mitchell 等人，2022a）使用单个输入输出对对语言模型进行高效的本地编辑。本质上，MEND 学习转换微调 LLM 的梯度，它利用梯度的低秩分解。</p><p><strong>KE</strong>。 KE（Cao et al., 2021）是一种可以编辑语言模型中错误知识而无需重新训练整个模型的方法。 KE 利用具有约束优化的超网络（双向 LSTM），用于预测推理过程中的权重更新。</p><p><strong>SERAC</strong>。 SERAC（Mitchell 等人，2022b）引入了一种基于内存的模型编辑方法，该方法利用显式内存系统来缓存编辑。该内存随后用于在推理过程中调整基本模型的输出。该系统利用一个小型辅助范围分类器和反事实模型。范围分类器的作用是确定输入是否在内存缓存的范围内。如果在此范围内找到输入，则会将其与最相关的缓存项结合起来，并输入到反事实模型中进行预测。</p><p>**In-Context Knowledge Editing.**。上下文知识编辑（IKE）（Zheng et al., 2023）构造 k 个演示 C &#x3D; {c1, . 。 。 , ck }，遵循 Liu 等人中概述的方法。 （2022）。该方法采用基于余弦相似度的无监督检索器，在将事实 f &#x3D; (x*, y*) 注入语言模型之前从训练集中获取演示。 x* 是探索模型中事实知识的提示（例如，美国总统是），y* 将是编辑目标乔·拜登。上下文演示的排名也取决于余弦相似度：cos(c1, f ) &lt; cos(c2, f ) &lt; · · · &lt; cos(ck, f )。其中 c1, . 。 。 , ck 在上下文中从左到右顺序排列。演示 C 可以被视为外部增强的知识库，主要设计用于指导 LM 内的生成。其最终目标是当提示符x落在目标提示符x*的编辑范围内时，最大化P(y|x,f,C)。</p><h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h2><h3 id="4-1-Results"><a href="#4-1-Results" class="headerlink" title="4.1 Results"></a>4.1 Results</h3><p>在这一部分中，我们对 MMEdit 上的多种编辑方法进行了比较分析。这些比较的结果如表2所示。之后，我们深入研究了实验结果的三个指标，包括可靠性、局部性和通用性三个方面。此外，我们通过文本和视觉方式分析局部性和通用性，并在图 6 中提供了几个编辑案例。</p><p><strong>可靠性</strong>。从结果来看，<span style="background-color: #ff666680">所有模型编辑方法在可靠性方面均优于基本方法</span>。特别是，<span style="background-color: #ff666680">利用外部存储器进行编辑的 IKE 和 SERAC 方法在多模态语言中表现出了值得称赞的性能楷模</span>。我们<span style="background-color: #ff666680">观察到微调方法的性能比模型编辑方法差</span>。请注意，<span style="background-color: #5fb23680">仅微调 LLM 或模态融合模块的参数并不能充分捕获多模态数据的特征</span>。我们分析原因为如下：<span style="background-color: #5fb23680">用于微调的数据与原始模型有较大差异，例如Q-former和OPT模型，需要有效协作。简单地微调这些模块之一可能无法准确捕获特定于任务的特征。另一方面，微调所有模块会产生大量的资源开销</span>。此外，<span style="background-color: #ff666680">根据我们的实验结果，我们观察到微调可能会导致原始模型发生重大变化，通常会导致其他知识的丢失，在多模式数据集中尤其明显。</span></p><p><img src="/../imgs/$%7Bfiilename%7D/WSEXRRE8.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/WSEXRRE8.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;WSEXRRE8&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%229DEQKYBI%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B68.077%2C533.813%2C530.769%2C783.044%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%227%22%7D%7D&quot; width=&quot;771&quot; height=&quot;415&quot; src=&quot;attachments/WSEXRRE8.png&quot; ztype=&quot;zimage&quot;&gt;"><br><strong>局部性</strong>。<span style="background-color: #ff666680">几种传统的编辑方法仍然适用于多模式编辑</span>，这对于有效修改模型内的知识并纠正其输出很有价值。然而，IKE和SERAC尽管在可靠性方面表现出色，但由于缺乏对M-Locality的约束而在M-Locality上表现不佳，<span style="background-color: #5fb23680">这表明尽管这些基于外部存储器的编辑技术无疑成功地修复了输出，但它们在稳定内部模型中的知识还有改进的空间。</span>对于T-Locality，大多数模型编辑方法都获得了良好的性能，而IKE再次表现不佳。根本原因是其他三种方法对T-Locality施加了约束，而IKE作为InContext Learning方法缺乏鲁棒的约束机制，导致性能不佳。</p><p><strong>泛化</strong>。我们在 E-VQA 中与 MiniGPT-4 进行了各种方法的文本和图像泛化能力的比较探索。请注意，<span style="background-color: #ff666680">KE 往往表现出较低程度的图像泛化，这主要是由于其在训练阶段对 M 局部性的固有考虑。</span>因此，<span style="background-color: #ff666680">与基于记忆的方法相比，元学习方法的图像泛化效率往往较低。另一方面，基于内存的方法所表现出的卓越图像泛化能力是以牺牲 M-Locality 为代价实现的，导致 M-Locality 水平显着降低。</span>通过对各种编辑方法的评估，<span style="background-color: #5fb23680">我们经常发现图像泛化性能往往不如文本泛化性能强大。</span></p><p><img src="/../imgs/$%7Bfiilename%7D/QNQGNXI7.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/QNQGNXI7.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;QNQGNXI7&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%227YT8KPYF%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B54.231%2C457.659%2C540.577%2C782.467%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%228%22%7D%7D&quot; width=&quot;811&quot; height=&quot;542&quot; src=&quot;attachments/QNQGNXI7.png&quot; ztype=&quot;zimage&quot;&gt;"></p><p><img src="/../imgs/$%7Bfiilename%7D/CPEZX96N.png" class="lazyload placeholder" data-srcset="/../imgs/$%7Bfiilename%7D/CPEZX96N.png" srcset="https://pic1.zhimg.com/v2-cd38920285d125be80b3eb504052c550_b.webp" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;CPEZX96N&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%2293WRHX5J%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B55.962%2C212.467%2C303.462%2C451.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%228%22%7D%7D&quot; width=&quot;413&quot; height=&quot;400&quot; src=&quot;attachments/CPEZX96N.png&quot; ztype=&quot;zimage&quot;&gt;"></p><h3 id="4-2-Editing-Different-Component"><a href="#4-2-Editing-Different-Component" class="headerlink" title="4.2 Editing Different Component"></a>4.2 Editing Different Component</h3><p>我们进一步分析了编辑多模态模型不同区域的变化。与编辑单模态模型相比，由于多模态模型的复杂性和多样性，我们可以尝试编辑更多模块并分析它们对视觉和文本知识的影响。结果如图 7 所示。对于 BLIP-2 OPT 模型，我们研究了在 VQA 数据集上编辑 Q-former 和 OPT 的区别。关于MiniGPT4模型，我们主要关注llama_proj和Vicuna模型在编辑最后几层的区别。选择的分析编辑方法有 MEND、KE 和 FT，这使我们能够指定编辑区域。</p><p>     结果表明，<span style="background-color: #5fb23680">编辑视觉模块比编辑语言模块更具挑战性（另请参阅图 6 中的失败编辑）。我们认为这种困难可能归因于模型的架构</span>。<span style="background-color: #ff666680">编辑LLM的最后一层可以直接修改输出，而修改视觉模块只影响LLM的输入，对模型的影响相对较小</span>。具体来说，<span style="background-color: #5fb23680">各种模态驻留在不同的空间中，这意味着事实知识可以存储在模型内的单独参数中</span>。考虑到LLMs拥有大量参数，这一点对于多模态模型变得更加重要。因此，编辑语言模型可以显着提高性能。值得注意的是，模型中的视觉模块在图像理解中起着至关重要的作用，<span style="background-color: #5fb23680">因此表明未来的工作需要同时考虑来自不同模式的信息。</span></p><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>在本文中，我们介绍了多模式模型编辑，以及新的基准 MMEdit。根据经验，我们分析了各种模型编辑基线的有效性，并探索它们对不同组件（例如视觉和文本）的影响。</p><h2 id="6-Limitations"><a href="#6-Limitations" class="headerlink" title="6 Limitations"></a>6 Limitations</h2><p><strong>楷模</strong>。我们只编辑几个基本的多模式LLMs，留下许多其他的。此外，由于资源限制，我们编辑的多模态LLM的参数数量低于10B，我们无法编辑具有更多参数的LLM，例如65B LLaMA Adapter V2（Gao et al., 2023） 。</p><p><strong>高效的视觉编辑</strong>。在本文中，我们的分析主要集中于比较不同模式模块中现有编辑方法的不同效果。然而，结果并不令人满意。展望未来，我们的主要目标是探索如何跨其他模式高效、准确地编辑信息。这包括研究技术，<span style="background-color: #5fb23680">例如通过查明多模态模型内的知识并识别需要修改的内容来在不同模态之间进行共同编辑。</span></p>]]></content>
      
      
      <categories>
          
          <category> NLP顶会 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> EMNLP2023 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2023/11/02/hello-world/"/>
      <url>/2023/11/02/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">&quot;My New Post&quot;</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>

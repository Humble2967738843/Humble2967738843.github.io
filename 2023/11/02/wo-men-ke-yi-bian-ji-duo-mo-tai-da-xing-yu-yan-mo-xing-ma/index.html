
<!DOCTYPE html><html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.27.0" theme-name="Stellar" theme-version="1.27.0">
  
  <meta name="generator" content="Hexo 7.0.0">
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  
  <title>我们可以编辑多模态大型语言模型吗？ - humbleyl</title>

  
    <meta name="description" content="我们可以编辑多模态大型语言模型吗？Abstract在本文中，我们重点关注编辑多模态大型语言模型（MLLM）。与编辑单模态LLMs相比，多模态模型编辑更具挑战性，需要在编辑过程中进行更高水平的审查和仔细考虑。为了促进这一领域的研究，我们构建了一个名为 MMEdit 的新基准，用于编辑多模式LLMs并建立一套创新的评估指标。我们进行了涉及各种模型编辑基线的综合实验，并分析了编辑不同组件对多模式LLMs">
<meta property="og:type" content="article">
<meta property="og:title" content="我们可以编辑多模态大型语言模型吗？">
<meta property="og:url" content="http://humble2967738843.github.io/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/index.html">
<meta property="og:site_name" content="humbleyl">
<meta property="og:description" content="我们可以编辑多模态大型语言模型吗？Abstract在本文中，我们重点关注编辑多模态大型语言模型（MLLM）。与编辑单模态LLMs相比，多模态模型编辑更具挑战性，需要在编辑过程中进行更高水平的审查和仔细考虑。为了促进这一领域的研究，我们构建了一个名为 MMEdit 的新基准，用于编辑多模式LLMs并建立一套创新的评估指标。我们进行了涉及各种模型编辑基线的综合实验，并分析了编辑不同组件对多模式LLMs">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/KQ9KJ8DQ.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/ZFVB46CA.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/HSTRNPML.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/J24EN8UP.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/IPZFZHUW.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/4NC86SS7.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/BS3DHYZH.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/TT279E4N.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/WSEXRRE8.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/QNQGNXI7.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/CPEZX96N.png">
<meta property="article:published_time" content="2023-11-02T04:21:08.000Z">
<meta property="article:modified_time" content="2023-11-02T05:02:28.683Z">
<meta property="article:author" content="yuan long">
<meta property="article:tag" content="EMNLP2023">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/KQ9KJ8DQ.png">
<meta name="twitter:creator" content="@humbleyl">
  
  
  
  <meta name="keywords" content="EMNLP2023">

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="humbleyl" type="application/atom+xml">
  

  <link rel="stylesheet" href="/css/main.css?v=1.27.0">

  
    <link rel="shortcut icon" href="solar:documents-bold-duotone">
  

  

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>
<body>

<div class="l_body s:aa content tech" id="start" layout="post" ><aside class="l_left"><div class="leftbar-container">


<header class="header"><div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="https://s2.loli.net/2024/04/10/32Y4obwgxJCeOMr.png" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></a><a class="title" href="/"><div class="main" ff="title">humbleyl</div><div class="sub normal cap">院龙的博客</div><div class="sub hover cap" style="opacity:0"> humbleyl</div></a></div></header>

<div class="nav-area">
<div class="search-wrapper" id="search-wrapper"><form class="search-form"><a class="search-button" onclick="document.getElementById(&quot;search-input&quot;).focus();"><svg t="1705074644177" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1560" width="200" height="200"><path d="M1008.839137 935.96571L792.364903 719.491476a56.783488 56.783488 0 0 0-80.152866 0 358.53545 358.53545 0 1 1 100.857314-335.166073 362.840335 362.840335 0 0 1-3.689902 170.145468 51.248635 51.248635 0 1 0 99.217358 26.444296 462.057693 462.057693 0 1 0-158.255785 242.303546l185.930047 185.725053a51.248635 51.248635 0 0 0 72.568068 0 51.248635 51.248635 0 0 0 0-72.978056z" p-id="1561"></path><path d="M616.479587 615.969233a50.428657 50.428657 0 0 0-61.498362-5.534852 174.655348 174.655348 0 0 1-177.525271 3.484907 49.403684 49.403684 0 0 0-58.833433 6.76482l-3.074918 2.869923a49.403684 49.403684 0 0 0 8.609771 78.10292 277.767601 277.767601 0 0 0 286.992355-5.739847 49.403684 49.403684 0 0 0 8.404776-76.667958z" p-id="1562"></path></svg></a><input type="text" class="search-input" id="search-input" placeholder="站内搜索"></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div>


<nav class="menu dis-select"><a class="nav-item active" title="博客" href="/" style="color:#1BCDFC"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M5.879 2.879C5 3.757 5 5.172 5 8v8c0 2.828 0 4.243.879 5.121C6.757 22 8.172 22 11 22h2c2.828 0 4.243 0 5.121-.879C19 20.243 19 18.828 19 16V8c0-2.828 0-4.243-.879-5.121C17.243 2 15.828 2 13 2h-2c-2.828 0-4.243 0-5.121.879M8.25 17a.75.75 0 0 1 .75-.75h3a.75.75 0 0 1 0 1.5H9a.75.75 0 0 1-.75-.75M9 12.25a.75.75 0 0 0 0 1.5h6a.75.75 0 0 0 0-1.5zM8.25 9A.75.75 0 0 1 9 8.25h6a.75.75 0 0 1 0 1.5H9A.75.75 0 0 1 8.25 9" clip-rule="evenodd"/><path fill="currentColor" d="M5.235 4.058C5 4.941 5 6.177 5 8v8c0 1.823 0 3.058.235 3.942L5 19.924c-.975-.096-1.631-.313-2.121-.803C2 18.243 2 16.828 2 14v-4c0-2.829 0-4.243.879-5.121c.49-.49 1.146-.707 2.121-.803zm13.53 15.884C19 19.058 19 17.822 19 16V8c0-1.823 0-3.059-.235-3.942l.235.018c.975.096 1.631.313 2.121.803C22 5.757 22 7.17 22 9.999v4c0 2.83 0 4.243-.879 5.122c-.49.49-1.146.707-2.121.803z" opacity=".5"/></svg></a><a class="nav-item" title="文档" href="/wiki/" style="color:#3DC550"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M14.25 4.48v3.057c0 .111 0 .27.02.406a.936.936 0 0 0 .445.683a.96.96 0 0 0 .783.072c.13-.04.272-.108.378-.159L17 8.005l1.124.534c.106.05.248.119.378.16a.958.958 0 0 0 .783-.073a.936.936 0 0 0 .444-.683c.021-.136.021-.295.021-.406V3.031c.113-.005.224-.01.332-.013C21.154 2.98 22 3.86 22 4.933v11.21c0 1.112-.906 2.01-2.015 2.08c-.97.06-2.108.179-2.985.41c-1.082.286-1.99 1.068-3.373 1.436c-.626.167-1.324.257-1.627.323V5.174c.32-.079 1.382-.203 1.674-.371c.184-.107.377-.216.576-.323m5.478 8.338a.75.75 0 0 1-.546.91l-4 1a.75.75 0 0 1-.364-1.456l4-1a.75.75 0 0 1 .91.546" clip-rule="evenodd"/><path fill="currentColor" d="M18.25 3.151c-.62.073-1.23.18-1.75.336a8.2 8.2 0 0 0-.75.27v3.182l.75-.356l.008-.005a1.13 1.13 0 0 1 .492-.13c.047 0 .094.004.138.01c.175.029.315.1.354.12l.009.005l.749.356V3.647z"/><path fill="currentColor" d="M12 5.214c-.334-.064-1.057-.161-1.718-.339C8.938 4.515 8.05 3.765 7 3.487c-.887-.234-2.041-.352-3.018-.412C2.886 3.007 2 3.9 2 4.998v11.146c0 1.11.906 2.01 2.015 2.079c.97.06 2.108.179 2.985.41c.486.129 1.216.431 1.873.726c1.005.451 2.052.797 3.127 1.034z" opacity=".5"/><path fill="currentColor" d="M4.273 12.818a.75.75 0 0 1 .91-.545l4 1a.75.75 0 1 1-.365 1.455l-4-1a.75.75 0 0 1-.545-.91m.909-4.545a.75.75 0 1 0-.364 1.455l4 1a.75.75 0 0 0 .364-1.455z"/></svg></a><a class="nav-item" title="探索" href="/explore/" style="color:#FA6400"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M20 12a8 8 0 1 1-16 0a8 8 0 0 1 16 0" opacity=".5"/><path fill="currentColor" d="M17.712 5.453c1.047-.193 2.006-.259 2.797-.152c.77.103 1.536.393 1.956 1.064c.446.714.312 1.542-.012 2.258c-.33.728-.918 1.499-1.672 2.268c-1.516 1.547-3.836 3.226-6.597 4.697c-2.763 1.472-5.495 2.484-7.694 2.92c-1.095.217-2.098.299-2.923.201c-.8-.095-1.6-.383-2.032-1.075c-.47-.752-.296-1.63.07-2.379c.375-.768 1.032-1.586 1.872-2.403L4 12.416c0 .219.083.71.168 1.146c.045.23.09.444.123.596c-.652.666-1.098 1.263-1.339 1.756c-.277.567-.208.825-.145.925c.072.116.305.305.937.38c.609.073 1.44.018 2.455-.183c2.02-.4 4.613-1.351 7.28-2.772c2.667-1.42 4.85-3.015 6.23-4.423c.694-.707 1.15-1.334 1.377-1.836c.233-.515.167-.75.107-.844c-.07-.112-.289-.294-.883-.374c-.542-.072-1.272-.041-2.163.112L16.87 5.656c.338-.101.658-.17.842-.203"/></svg></a><a class="nav-item" title="社交" href="/friends/" style="color:#F44336"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="m13.629 20.472l-.542.916c-.483.816-1.69.816-2.174 0l-.542-.916c-.42-.71-.63-1.066-.968-1.262c-.338-.197-.763-.204-1.613-.219c-1.256-.021-2.043-.098-2.703-.372a5 5 0 0 1-2.706-2.706C2 14.995 2 13.83 2 11.5v-1c0-3.273 0-4.91.737-6.112a5 5 0 0 1 1.65-1.651C5.59 2 7.228 2 10.5 2h3c3.273 0 4.91 0 6.113.737a5 5 0 0 1 1.65 1.65C22 5.59 22 7.228 22 10.5v1c0 2.33 0 3.495-.38 4.413a5 5 0 0 1-2.707 2.706c-.66.274-1.447.35-2.703.372c-.85.015-1.275.022-1.613.219c-.338.196-.548.551-.968 1.262" opacity=".5"/><path fill="currentColor" d="M10.99 14.308c-1.327-.978-3.49-2.84-3.49-4.593c0-2.677 2.475-3.677 4.5-1.609c2.025-2.068 4.5-1.068 4.5 1.609c0 1.752-2.163 3.615-3.49 4.593c-.454.335-.681.502-1.01.502c-.329 0-.556-.167-1.01-.502"/></svg></a></nav>
</div>
<div class="widgets">


<widget class="widget-wrapper post-list"><div class="widget-header dis-select"><span class="name">最近更新</span></div><div class="widget-body fs14"><a class="item title" href="/2024/04/10/tui-jian-xi-tong/"><span class="title">推荐系统</span></a><a class="item title" href="/2023/11/02/suan-fa-bi-ji-v2.0/"><span class="title">算法笔记V2.0</span></a><a class="item title" href="/2023/12/02/16-1-ci-qian-ru-word2vec/"><span class="title">16.1词嵌入（Word2Vec）</span></a><a class="item title" href="/2023/12/06/ci-xiang-liang-yu-yu-yan-mo-xing/"><span class="title">词向量与语言模型</span></a><a class="item title" href="/2023/11/16/kaggle-s-30-days-of-ml/"><span class="title">Kaggle's_30_Days_Of_ML</span></a><a class="item title" href="/2023/11/02/qing-jing-xue-xi-chuang-jian-ren-wu-xiang-liang/"><span class="title">情境学习创建任务向量</span></a><a class="item title" href="/2023/11/02/she-ji-mo-shi/"><span class="title">设计模式</span></a><a class="item title" href="/2023/11/09/wen-xian-zheng-li/"><span class="title">文献整理</span></a><a class="item title" href="/2023/11/06/wang-diao-ni-xiang-wang-diao-de-dong-xi-llms-de-gao-xiao-wang-que/"><span class="title">忘掉你想忘掉的东西：LLMs的高效忘却</span></a><a class="item title" href="/2023/11/02/zhi-shi-shen-jing-yuan-zhong-xin-zhi-lu-yu-yan-wu-guan-zhi-shi-shen-jing-yuan-he-jian-bing-zhi-shi-shen-jing-yuan-de-fa-xian/"><span class="title">知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现</span></a></div></widget>
</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/assets/placeholder/social/08a41b181ce68.svg"/></a><a class="social" href="https://" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/assets/placeholder/social/3845874.svg"/></a><a class="social" href="https://" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/assets/placeholder/social/3616429.svg"/></a><a class="social" href="https://" target="_blank" rel="external nofollow noopener noreferrer"><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="/assets/placeholder/social/942ebbf1a4b91.svg"/></a></div></footer>
</div></aside><div class="l_main" id="main">





<div class="article banner top">
  <div class="content">
    <div class="top bread-nav footnote"><div class="left"><div class="flex-row" id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a>
<span class="sep"></span><a class="cap breadcrumb" href="/">文章</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/NLP%E9%A1%B6%E4%BC%9A/">NLP顶会</a></div>
<div class="flex-row" id="post-meta"><span class="text created">发布于：<time datetime="2023-11-02T04:21:08.000Z">2023-11-02</time></span><span class="sep updated"></span><span class="text updated">更新于：<time datetime="2023-11-02T05:02:28.683Z">2023-11-02</time></span></div></div></div>
    
    <div class="bottom only-title">
      
      <div class="text-area">
        <h1 class="text title"><span>我们可以编辑多模态大型语言模型吗？</span></h1>
        
      </div>
    </div>
    
  </div>
  </div><article class="md-text content"><h1 id="我们可以编辑多模态大型语言模型吗？"><a href="#我们可以编辑多模态大型语言模型吗？" class="headerlink" title="我们可以编辑多模态大型语言模型吗？"></a>我们可以编辑多模态大型语言模型吗？</h1><h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>在本文中，我们重点关注编辑多模态大型语言模型（MLLM）。与编辑单模态LLMs相比，多模态模型编辑更具挑战性，需要在编辑过程中进行更高水平的审查和仔细考虑。为了促进这一领域的研究，我们<span style="background-color: #ff666680">构建了一个名为 MMEdit 的新基准，用于编辑多模式LLMs并建立一套创新的评估指标</span>。我们进行了涉及各种模型编辑基线的综合实验，并分析了编辑不同组件对多模式LLMs的影响。根据经验，我们注意到<span style="background-color: #ff666680">以前的基线可以在一定程度上实现多模态 LLM 的编辑，但效果仍然差强人意，这表明这项任务的潜在难度。</span>我们希望我们的工作能够为 NLP 社区提供见解1。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>随着大型语言模型 (LLM) 的广泛部署（Zhao 等人，2023），在不产生大量再培训成本的情况下保持其知识准确和最新的必要性变得越来越重要（Sinitsin 等人，2020）。<span style="background-color: #2ea8e580">先前的研究引入了知识编辑方法，旨在逐步向语言模型注入一组新的事实</span>（Mitchell 等人，2022a；Han 等人，2023；Hartvigsen 人，2022；Zhong 等人，2023；Gandikota等人，2023；姚等人，2023）。</p>
<pre><code>  与单模态模型编辑不同，多模态LLMs的编辑任务因其固有的多样性和复杂性而面临相当大的挑战。具体来说，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;多模式模型的错误输出可能源于各种模式的协同效应。输出不正确可能不仅仅源于LLMs，类似于误读或误识别等人为错误&lt;/span&gt;（例如，色盲影响图像中的颜色识别）。如图1所示，在编辑之前，模型将物体错误地识别为“梯子”而不是正确的“障碍物”，从而导致错误的预测。编辑后，模型准确识别了“障碍”。请注意，多模态LLMs（Yin et al., 2023）的效用正在增加，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;但缺乏相应的数据集资源和用于编辑多模态大语言模型的基准。&lt;/span&gt;

 为了促进这一领域的研究，我们第一步构建了一个多模态模型编辑基准：&lt;span style=&quot;background-color: #ff666680&quot;&gt;称为 MMEdit，它包含两个子任务：编辑 VQA 和编辑图像标题。&lt;/span&gt;具体来说，我们&lt;span style=&quot;background-color: #ff666680&quot;&gt;遵循单模态模型编辑方法（Mitchell et al., 2022a；Cao et al., 2021；Mitchell et al., 2022b）来构建数据集，这扩展了之前的评估原则，即 Reliability2、Locality3 和通用性4，多模式设置。对于可靠性评估，我们从严格的数据收集开始，收集表现不佳的多模态模型数据来创建专用的可靠性编辑数据集（§3.2.1）。对于局部性评估，我们将其分为文本局部性和多模态局部性，以评估多模态 LLM 的稳定性（第 3.2.2 节）。对于通用性评估，与局部性类似，我们将其分为文本通用性和多模态通用性，并利用 ChatGLM (Du et al., 2022) 和稳定扩散 (Rombach et al., 2022) 生成重新措辞的文本以及重新措辞的图像进行评估（第 3.2.3 节）。我们评估了 MMEdit 上的几种知识编辑方法。&lt;/span&gt;根据经验，我们注意到&lt;span style=&quot;background-color: #5fb23680&quot;&gt;当前的编辑方法对于编辑多模态语言模型中的文本模型有效，但对于编辑视觉模块则不那么有效&lt;/span&gt;。例如，在编辑BLIP-2模型的语言模块时，MEND的可靠性可以达到92.6%，但在编辑视觉模块时只能达到14.1%，表明该任务的潜在难度和机遇。总的来说，我们的主要贡献如下：
</code></pre><ul>
<li>我们迈出了第一步，研究编辑多模态LLMs，将模型编辑扩展到多模态设置。</li>
</ul>
<!---->
<ul>
<li>我们提出了MMEdit，一个新的基准，用于评估多模态模型编辑方法的可靠性、局部性和通用性。</li>
</ul>
<!---->
<ul>
<li>我们使用各种基线进行实验，证明虽然当前的方法可以在一定程度上帮助多模式编辑，但结果仍然达不到完全满意的程度。我们将公开代码和数据集以用于未来的研究目的。</li>
</ul>
<h2 id="2-Related-Work"><a href="#2-Related-Work" class="headerlink" title="2 Related Work"></a>2 Related Work</h2><h3 id="2-1-Multimodal-Language-Models"><a href="#2-1-Multimodal-Language-Models" class="headerlink" title="2.1 Multimodal Language Models"></a>2.1 Multimodal Language Models</h3><pre><code> 多模态学习 (MML)（Xu 等人，2022a；Yin 等人，2023）提供了一种构建 AI 模型的整体方法，该模型可以从各种数据模态中提取和关联信息。由于其社会意义，MML 在研究界站稳了脚跟，在过去十年中巩固了自己作为一个重要研究领域的地位。视&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;觉语言预训练是MML的重要分支之一，旨在学习在各种视觉和语言任务上具有改进性能的多模态基础模型&lt;/span&gt;。 Vision Transformer (ViT)（Dosovitskiy et al., 2021）&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;是一项开创性的工作，贡献了端到端将 Transformers 的编码器应用于图像的解决方案。&lt;/span&gt; CLIP（Radford et al., 2021）&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;提出了一种方法，使用多模态预训练将分类转换为检索任务，使预训练模型能够解决零样本识别问题&lt;/span&gt;。最近，LLaMA (Touvron et al., 2023)、BLOOM (Scao et al., 2022) 和 ChatGPT (OpenAI, 2022) 等 LLM 的进步得到了扩大训练数据和增加参数的支持，最近取得了重大成功。这些模型展示了令人印象深刻的语言理解、生成和知识推理能力，增强了它们理解自然语言和生成高质量、基于上下文的文本的能力。大型语言模型的发展刺激了自回归语言模型作为视觉语言任务中的解码器的广泛使用。&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;利用跨模态迁移，这种方法可以实现语言和多模态领域之间的知识共享&lt;/span&gt;（Gao et al., 2023; Liu et al., 2023; Li et al., 2023a; Ye et al., 2023; Zhu et al., 2023；Li 等人，2023b；Zhang 等人，2023）。
</code></pre><h3 id="2-2-Model-Editing"><a href="#2-2-Model-Editing" class="headerlink" title="2.2 Model Editing"></a>2.2 Model Editing</h3><pre><code> LLMs（Zhao et al., 2023）主要从训练语料库中获取知识。然而，数据集的质量并不总是得到保证，可能会将有害或不正确的信息集成到模型中（Hernandez 等人，2023）。&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;一种解决方案是使用更新的知识重新训练模型，尽管这可能成本高昂且难以实施&lt;/span&gt;。或者，&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;可以考虑使用一些更新的事实进行微调，但它存在过度拟合和灾难性遗忘的风险&lt;/span&gt;（Zhai et al., 2023）。为了解决这些问题，（Sinitsin et al., 2020）提出了模型编辑，旨在高效、准确地改变模型中存储的事实知识。这种方法应用于各个领域（Mao et al., 2023; Onoe et al., 2023; Xu et al., 2022b; Wang et al., 2023a; Li et al., 2023c），并且研究数量不断增加调查编辑的影响（Ilharco 等人，2023；Gupta 等人，2023；Hase 等人，2023；Cohen 等人，2023；Wu 等人，2023；Wang 等人，2023b；Gandikota 等人等人，2023；Li 等人，2023d）。目前，模型编辑方法主要有三种类型：&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;1）元学习方法，2）定位然后编辑方法，3）上下文知识编辑方法。&lt;/span&gt;
</code></pre><p><strong>元学习方法</strong>。 MEND（Mitchell 等人，2022a）和知识编辑器 (KE)（Cao 等人，2021）提出了<span style="background-color: #2ea8e580">涉及外部编辑器的方法，能够学习用于知识更新的最佳参数集 θ，同时施加约束以保持模型稳定性。</span> CaliNET (Dong et al., 2022) 和 T-Patcher (Huang et al., 2023) 从 (Dai et al., 2022) 中汲取灵感，<span style="background-color: #2ea8e580">将额外的可训练参数引入到预训练语言模型的前馈模块中。</span> SERAC（Mitchell 等人，2022b）<span style="background-color: #2ea8e580">利用显式记忆来存储编辑，并学习对它们进行推理，以根据需要调整基本模型的预测。</span></p>
<p><strong>定位然后编辑方法</strong>。 ROME（Meng et al., 2022a）<span style="background-color: #2ea8e580">提出了采用因果中介分析来识别编辑区域的方法。</span> ROME 发现记忆的事实关联可以精确定位到 GPT 模型中的特定位置。然而，ROME 的一个显着限制是它一次只能编辑一个事实。为了解决这个问题，Meng 等人。 (2022b)提出了一种称为MEMIT的新方法，<span style="background-color: #2ea8e580">它是之前工作ROME的继承者，它对单层的MLP权重进行rankone修改，以将内存直接写入模型中。</span></p>
<p><strong>上下文知识编辑方法</strong>。 InContext Learning (ICL)（Brown et al., 2020）<span style="background-color: #2ea8e580">表示一种免训练范式，其中知识是从输入上下文中直接串联的演示中获取的</span>。最近出现了一种新颖的编辑范式，它<span style="background-color: #2ea8e580">利用LLMs理解上下文的能力（Zheng et al., 2023），从而实现基于上下文的模型编辑，指导模型的生成过程，并提供高效、轻量级的模型方法编辑</span>。迄今为止的模型编辑方法主要迎合单模态场景，在多模态编辑方面留下了空白。据我们所知，我们是第一个研究LLMs多模式模型编辑的人，并为促进该领域的研究提供了新的基准。</p>
<h2 id="3-Editing-Multimodal-LLMs"><a href="#3-Editing-Multimodal-LLMs" class="headerlink" title="3 Editing Multimodal LLMs"></a>3 Editing Multimodal LLMs</h2><p>我们在图 2 中说明了多模态编辑的建议任务。我们将介绍任务定义（§3.1）、（§3.2）中的数据集构建细节、多模态模型（§3.3）以及我们在实验。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/KQ9KJ8DQ.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;KQ9KJ8DQ&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22MS6FUQ8I%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B68.654%2C497.467%2C526.731%2C777.852%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;763&quot; height=&quot;467&quot; src=&quot;attachments/KQ9KJ8DQ.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<h3 id="3-1-Task-Definition"><a href="#3-1-Task-Definition" class="headerlink" title="3.1 Task Definition"></a>3.1 Task Definition</h3><p>假设我们有一个由 <em>θ </em>参数化的多模态 LLM<em> f </em>（由两部分组成，由 θ<sub>vision</sub> 和 θ<sub>text</sub> 参数化的 f<sub>vision</sub> 和 f<sub>text </sub>），将输入 i<sub>e</sub> 和 x<sub>e</sub> 映射到 y<sub>o</sub> 的预测，其中 i<sub>e </sub>指的是编辑图像输入，xe 指的是编辑文本提示输入和 y<sub>o</sub> 表示为原始输出。我们将 M 表示为特定指标的符号表示，下标表示特定度量，上标表示变化编辑数据。我们准备第 3.2.1 节中所述的编辑数据集，其表示为 Dedit。受到姚等人的启发。 （2023），我们引入了一系列多模式模型编辑指标。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/ZFVB46CA.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;ZFVB46CA&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22MJQK4385%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B64.853%2C526.008%2C292.5%2C791.596%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;379&quot; height=&quot;442&quot; src=&quot;attachments/ZFVB46CA.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p><strong>可靠性</strong>。需要编辑可靠性才能将预测从 y<sub>o</sub> 更改为 y<sub>e</sub>。直观上，我们需要的是更新后的 θe，其中 f（即 x<sub>e</sub>;θ<sub>e</sub>）= y<sub>e</sub>。为了衡量可靠性，我们使用编辑准确性，如下所述：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/HSTRNPML.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;HSTRNPML&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22NS7JGJWI%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B68.824%2C342.919%2C292.059%2C378.214%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;372&quot; height=&quot;59&quot; src=&quot;attachments/HSTRNPML.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中θ<sub>e</sub>指的是编辑后的参数。</p>
<p><strong>局部性</strong>。为了保持模型的稳定性，必须最大限度地减少编辑对模型更广泛的知识库造成的意外副作用。为了实现这一目标，我们引入了两个指标：M <sup>Text</sup> <sub>loc</sub> (T-Locality) 和 M<sup>Image</sup> <sub>loc</sub> (M-Locality)，这两个指标都是为了在编辑过程中保持模型的稳定性。鉴于多模态语言模型中的知识是从LLMs继承的，保护这些知识至关重要。考虑到这一目标，我们搁置了模型的视觉辨别模块，而是采用基本的问答数据集 D<sub>loc-t</sub>，如第 3.2.2 节中所述。我们定义问题为x，答案为y，如下：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/J24EN8UP.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;J24EN8UP&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22M2ZHI4QM%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B67.941%2C93.214%2C292.941%2C133.361%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;375&quot; height=&quot;67&quot; src=&quot;attachments/J24EN8UP.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>视觉编码器在多模态语言模型中发挥着关键作用，将将图像转换为矢量表示，以便与自然语言文本共同编码。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/IPZFZHUW.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;IPZFZHUW&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22ZIQHSJG7%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B301.765%2C653.066%2C528.529%2C777.919%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;378&quot; height=&quot;208&quot; src=&quot;attachments/IPZFZHUW.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>因此，我们必须考虑对该模块进行任何修改的潜在后果。我们构建表示为 D<sub>loc-v</sub> 的数据集用于测试 MImage loc ，并计算如下：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/4NC86SS7.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;4NC86SS7&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22K2BYGI7E%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B302.647%2C522.478%2C532.941%2C557.772%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;384&quot; height=&quot;59&quot; src=&quot;attachments/4NC86SS7.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中（iv，xv，yv）是范围外数据，θe表示编辑数据更新的参数（即xe，ye)。</p>
<p><strong>泛化性</strong>。在整个编辑过程中，仅仅修改个别错误的输入是不够的。修订后的模型还应保留泛化能力，并始终为等效输入（例如改写的句子）生成一致的输出，如图 3 所示。虽然以前的单模态模型编辑任务仅需要考虑改写的文本，但多模态场景需要泛化以及图像。为了解决这个问题，我们引入了两个泛化考虑因素：MText gen (TGenerality) 和 MImage gen (M-Generality)，其表示如下：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/BS3DHYZH.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;BS3DHYZH&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%22SSPQQ5PC%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B303.088%2C236.596%2C527.206%2C296.596%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;374&quot; height=&quot;100&quot; src=&quot;attachments/BS3DHYZH.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 ir 表示重新表述的图像，xr 指重新表述的文本提示，N (x) 表示 x 范围内的对象。</p>
<h3 id="3-2-Datasets"><a href="#3-2-Datasets" class="headerlink" title="3.2 Datasets"></a>3.2 Datasets</h3><p>我们构建的数据集MMEdit主要包含两个子任务：编辑VQA（E-VQA）和编辑图像标题（E-IC）。</p>
<h4 id="3-2-1-Reliability-Dataset-Construction"><a href="#3-2-1-Reliability-Dataset-Construction" class="headerlink" title="3.2.1 Reliability Dataset Construction"></a>3.2.1 Reliability Dataset Construction</h4><p>为了对我们的实验进行基准测试，我们选择了两个常见的多模态任务：视觉问答（VQA）（Antol et al., 2015）和图像</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/TT279E4N.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;TT279E4N&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%229KCXYY3D%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B60%2C488.39%2C297%2C778.39%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%225%22%7D%7D&quot; width=&quot;395&quot; height=&quot;483&quot; src=&quot;attachments/TT279E4N.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>字幕（Herdade 等人，2019）。 <span style="background-color: #2ea8e580">VQA 旨在设计一种算法，不仅可以理解图像中的视觉内容，还可以理解用于查询该图像的自然语言，并随后生成这些查询的精确答案</span>。<span style="background-color: #2ea8e580">图像字幕是设计能够理解图像视觉内容的算法，随后用自然语言生成连贯且精确的图像描述</span>。在本研究中，我们选择 BLIP-2 OPT。我们的基础编辑数据源自两个评估数据集的次优条目，即 VQAv2（Goyal 等人，2017）和 COCO Caption（Chen 等人，2015)。</p>
<pre><code> 除了基本的编辑数据之外，利用其他数据也至关重要。这些数据不仅有助于编辑过程，还验证更改的有效性，评估模型编辑的稳定性和通用性。
</code></pre><h4 id="3-2-2-Locality-Dataset-Construction"><a href="#3-2-2-Locality-Dataset-Construction" class="headerlink" title="3.2.2 Locality Dataset Construction"></a>3.2.2 Locality Dataset Construction</h4><p>我们必须仔细考虑多模态模型中编辑对语言功能的影响，<span style="background-color: #5fb23680">类似于我们如何评估手术后个体大脑的各个认知区域。</span></p>
<p><strong>文本局部性数据集</strong>。为了评估语言模型的稳定性，我们利用之前在 MEND 中使用的 NQ 数据集（Kwiatkowski 等人，2019）作为模型内 LLM 组件稳定性的基准。我们专门使用模型的输出预编辑和后期编辑来构建 KL 散点图，从而促进对模型编辑的约束。此外，我们还计算了保持 top-1 状态的实例比例，进一步量化了模型的稳定性。</p>
<p><strong>多模态局部性数据集</strong>。同样，验证编辑对视觉模块的影响也至关重要。因此，我们在多模态领域使用简单的数据集 OK-VQA（Marino 等人，2019），作为多模态视觉模块局部性的度量。我们再次在编辑过程之前和之后使用 logits 更新 KL 离散度约束。</p>
<h4 id="3-2-3-Generality-Dataset-Construction"><a href="#3-2-3-Generality-Dataset-Construction" class="headerlink" title="3.2.3 Generality Dataset Construction"></a>3.2.3 Generality Dataset Construction</h4><p>我们在多模态模型中提出了两种形式的通用性。共性数据集构建的整体流程如图4所示。</p>
<p><strong>文本泛化数据集</strong>。值得注意的是，LLMs表现出强大的会话能力和强大的解决问题的能力，这使我们能够制定任务指令，从而指导模型生成类似的文本输入。对于 E-VQA 任务，我们利用 ChatGLM（Du et al., 2022；Zeng et al., 2022）生成类似的查询。然而，对于E-IC任务，由于提示的简洁和相对直接，模型生成的输出质量并不令人满意。因此，我们采用手动编写的包含20条提示的模板来随机替换原来的提示。</p>
<p><strong>视觉泛化数据集</strong>。近年来，<span style="background-color: #2ea8e580">扩散模型（Ho et al., 2020）在图像生成领域取得了巨大成功。超越最初最先进的模型</span>：生成对抗网络（GAN）模型（Goodfellow 等人，2014）。扩散模型在许多图像生成任务中表现出色，并在各个应用领域中表现出了值得称赞的性能。<span style="background-color: #2ea8e580">稳定扩散（Rombach 等人，2022）是一种潜在的文本到图像扩散模型，能够根据给定的文本输入生成逼真的图像</span>。我们利用稳定扩散 2.1 来生成重新解释的图像。该数据集利用 COCO 数据集的标题描述来评估模型的图像泛化能力。</p>
<h3 id="3-3-Multimodal-Language-Models"><a href="#3-3-Multimodal-Language-Models" class="headerlink" title="3.3 Multimodal Language Models"></a>3.3 Multimodal Language Models</h3><p><strong>BLIP-2 OPT</strong>。 BLIP-2（Li et al., 2023b）<span style="background-color: #2ea8e580">是一种通用且高效的预训练策略，可从现成的冻结预训练图像编码器和冻结大型语言模型引导视觉语言预训练。该模型利用轻量级查询转换器来弥合视觉模态和文本模态之间的差距，并在各种视觉语言任务上实现最先进的性能</span>。我们选择 BLIP-2 OPT 作为基本编辑模型，它在视觉模块中利用 ViT-L，并选择无监督训练的 OPT 模型用于基于解码器的 LLM。迷你GPT-4。</p>
<p><strong>MiniGPT-4</strong>（Zhu et al., 2023）是一种类似于 BLIP-2 的有效视觉语言模型，利用冷冻视觉编码器与冷冻 Vicuna（Chiang et al., 2023）相结合。据报道，基于 LLaMA 构建的 Vicuna 根据 GPT-4 的评估标准达到了 ChatGPT 90% 的性能。 MiniGPT-4 添加了一个投影层，以使编码的视觉特征与 Vicuna 语言模型保持一致。 MiniGPT-4 采用与 BLIP-2 相同的预训练视觉组件，由 EVA-CLIP（Sun 等人，2023）的 Vit-G/14 和 Q-Former 组成。</p>
<h3 id="3-4-Baselines"><a href="#3-4-Baselines" class="headerlink" title="3.4 Baselines"></a>3.4 Baselines</h3><p><strong>Finetune</strong>。微调已成为一种广泛采用的策略，用于使预训练的语言模型适应特定任务或领域（Cortes 等人，2015）。在我们的探索中，我们深入研究了两种不同的微调方法：一种专注于语言模型的最后一层。以BLIP-2 OPT模型为例，我们对OPT模型的第31个解码器层进行微调。另一个目标是多模态语言模型中的视觉块，具体来说，我们微调 Q-former 模型以过度拟合编辑数据集。</p>
<p><strong>MEND</strong>。具有梯度分解的模型编辑器网络（Mitchell 等人，2022a）使用单个输入输出对对语言模型进行高效的本地编辑。本质上，MEND 学习转换微调 LLM 的梯度，它利用梯度的低秩分解。</p>
<p><strong>KE</strong>。 KE（Cao et al., 2021）是一种可以编辑语言模型中错误知识而无需重新训练整个模型的方法。 KE 利用具有约束优化的超网络（双向 LSTM），用于预测推理过程中的权重更新。</p>
<p><strong>SERAC</strong>。 SERAC（Mitchell 等人，2022b）引入了一种基于内存的模型编辑方法，该方法利用显式内存系统来缓存编辑。该内存随后用于在推理过程中调整基本模型的输出。该系统利用一个小型辅助范围分类器和反事实模型。范围分类器的作用是确定输入是否在内存缓存的范围内。如果在此范围内找到输入，则会将其与最相关的缓存项结合起来，并输入到反事实模型中进行预测。</p>
<p><strong>In-Context Knowledge Editing.</strong>。上下文知识编辑（IKE）（Zheng et al., 2023）构造 k 个演示 C = {c1, . 。 。 , ck }，遵循 Liu 等人中概述的方法。 （2022）。该方法采用基于余弦相似度的无监督检索器，在将事实 f = (x*, y*) 注入语言模型之前从训练集中获取演示。 x* 是探索模型中事实知识的提示（例如，美国总统是），y* 将是编辑目标乔·拜登。上下文演示的排名也取决于余弦相似度：cos(c1, f ) &lt; cos(c2, f ) &lt; · · · &lt; cos(ck, f )。其中 c1, . 。 。 , ck 在上下文中从左到右顺序排列。演示 C 可以被视为外部增强的知识库，主要设计用于指导 LM 内的生成。其最终目标是当提示符x落在目标提示符x*的编辑范围内时，最大化P(y|x,f,C)。</p>
<h2 id="4-Experiments"><a href="#4-Experiments" class="headerlink" title="4 Experiments"></a>4 Experiments</h2><h3 id="4-1-Results"><a href="#4-1-Results" class="headerlink" title="4.1 Results"></a>4.1 Results</h3><p>在这一部分中，我们对 MMEdit 上的多种编辑方法进行了比较分析。这些比较的结果如表2所示。之后，我们深入研究了实验结果的三个指标，包括可靠性、局部性和通用性三个方面。此外，我们通过文本和视觉方式分析局部性和通用性，并在图 6 中提供了几个编辑案例。</p>
<p><strong>可靠性</strong>。从结果来看，<span style="background-color: #ff666680">所有模型编辑方法在可靠性方面均优于基本方法</span>。特别是，<span style="background-color: #ff666680">利用外部存储器进行编辑的 IKE 和 SERAC 方法在多模态语言中表现出了值得称赞的性能楷模</span>。我们<span style="background-color: #ff666680">观察到微调方法的性能比模型编辑方法差</span>。请注意，<span style="background-color: #5fb23680">仅微调 LLM 或模态融合模块的参数并不能充分捕获多模态数据的特征</span>。我们分析原因为如下：<span style="background-color: #5fb23680">用于微调的数据与原始模型有较大差异，例如Q-former和OPT模型，需要有效协作。简单地微调这些模块之一可能无法准确捕获特定于任务的特征。另一方面，微调所有模块会产生大量的资源开销</span>。此外，<span style="background-color: #ff666680">根据我们的实验结果，我们观察到微调可能会导致原始模型发生重大变化，通常会导致其他知识的丢失，在多模式数据集中尤其明显。</span></p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/WSEXRRE8.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;WSEXRRE8&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%229DEQKYBI%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%227%22%2C%22position%22%3A%7B%22pageIndex%22%3A6%2C%22rects%22%3A%5B%5B68.077%2C533.813%2C530.769%2C783.044%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%227%22%7D%7D&quot; width=&quot;771&quot; height=&quot;415&quot; src=&quot;attachments/WSEXRRE8.png&quot; ztype=&quot;zimage&quot;&gt;"><br><strong>局部性</strong>。<span style="background-color: #ff666680">几种传统的编辑方法仍然适用于多模式编辑</span>，这对于有效修改模型内的知识并纠正其输出很有价值。然而，IKE和SERAC尽管在可靠性方面表现出色，但由于缺乏对M-Locality的约束而在M-Locality上表现不佳，<span style="background-color: #5fb23680">这表明尽管这些基于外部存储器的编辑技术无疑成功地修复了输出，但它们在稳定内部模型中的知识还有改进的空间。</span>对于T-Locality，大多数模型编辑方法都获得了良好的性能，而IKE再次表现不佳。根本原因是其他三种方法对T-Locality施加了约束，而IKE作为InContext Learning方法缺乏鲁棒的约束机制，导致性能不佳。</p>
<p><strong>泛化</strong>。我们在 E-VQA 中与 MiniGPT-4 进行了各种方法的文本和图像泛化能力的比较探索。请注意，<span style="background-color: #ff666680">KE 往往表现出较低程度的图像泛化，这主要是由于其在训练阶段对 M 局部性的固有考虑。</span>因此，<span style="background-color: #ff666680">与基于记忆的方法相比，元学习方法的图像泛化效率往往较低。另一方面，基于内存的方法所表现出的卓越图像泛化能力是以牺牲 M-Locality 为代价实现的，导致 M-Locality 水平显着降低。</span>通过对各种编辑方法的评估，<span style="background-color: #5fb23680">我们经常发现图像泛化性能往往不如文本泛化性能强大。</span></p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/QNQGNXI7.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;QNQGNXI7&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%227YT8KPYF%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B54.231%2C457.659%2C540.577%2C782.467%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%228%22%7D%7D&quot; width=&quot;811&quot; height=&quot;542&quot; src=&quot;attachments/QNQGNXI7.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/CPEZX96N.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;CPEZX96N&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F5LR2L362%22%2C%22annotationKey%22%3A%2293WRHX5J%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%228%22%2C%22position%22%3A%7B%22pageIndex%22%3A7%2C%22rects%22%3A%5B%5B55.962%2C212.467%2C303.462%2C451.89%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEQHAEIP8%22%5D%2C%22locator%22%3A%228%22%7D%7D&quot; width=&quot;413&quot; height=&quot;400&quot; src=&quot;attachments/CPEZX96N.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<h3 id="4-2-Editing-Different-Component"><a href="#4-2-Editing-Different-Component" class="headerlink" title="4.2 Editing Different Component"></a>4.2 Editing Different Component</h3><p>我们进一步分析了编辑多模态模型不同区域的变化。与编辑单模态模型相比，由于多模态模型的复杂性和多样性，我们可以尝试编辑更多模块并分析它们对视觉和文本知识的影响。结果如图 7 所示。对于 BLIP-2 OPT 模型，我们研究了在 VQA 数据集上编辑 Q-former 和 OPT 的区别。关于MiniGPT4模型，我们主要关注llama_proj和Vicuna模型在编辑最后几层的区别。选择的分析编辑方法有 MEND、KE 和 FT，这使我们能够指定编辑区域。</p>
<pre><code> 结果表明，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;编辑视觉模块比编辑语言模块更具挑战性（另请参阅图 6 中的失败编辑）。我们认为这种困难可能归因于模型的架构&lt;/span&gt;。&lt;span style=&quot;background-color: #ff666680&quot;&gt;编辑LLM的最后一层可以直接修改输出，而修改视觉模块只影响LLM的输入，对模型的影响相对较小&lt;/span&gt;。具体来说，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;各种模态驻留在不同的空间中，这意味着事实知识可以存储在模型内的单独参数中&lt;/span&gt;。考虑到LLMs拥有大量参数，这一点对于多模态模型变得更加重要。因此，编辑语言模型可以显着提高性能。值得注意的是，模型中的视觉模块在图像理解中起着至关重要的作用，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;因此表明未来的工作需要同时考虑来自不同模式的信息。&lt;/span&gt;
</code></pre><h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><p>在本文中，我们介绍了多模式模型编辑，以及新的基准 MMEdit。根据经验，我们分析了各种模型编辑基线的有效性，并探索它们对不同组件（例如视觉和文本）的影响。</p>
<h2 id="6-Limitations"><a href="#6-Limitations" class="headerlink" title="6 Limitations"></a>6 Limitations</h2><p><strong>楷模</strong>。我们只编辑几个基本的多模式LLMs，留下许多其他的。此外，由于资源限制，我们编辑的多模态LLM的参数数量低于10B，我们无法编辑具有更多参数的LLM，例如65B LLaMA Adapter V2（Gao et al., 2023） 。</p>
<p><strong>高效的视觉编辑</strong>。在本文中，我们的分析主要集中于比较不同模式模块中现有编辑方法的不同效果。然而，结果并不令人满意。展望未来，我们的主要目标是探索如何跨其他模式高效、准确地编辑信息。这包括研究技术，<span style="background-color: #5fb23680">例如通过查明多模态模型内的知识并识别需要修改的内容来在不同模态之间进行共同编辑。</span></p>

<div class="article-footer fs14">
    <section id="license">
      <div class="header"><span>许可协议</span></div>
      <div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div>
    </section>
    
    <section id="share">
      <div class="header"><span>分享文章</span></div>
      <div class="body">
        <div class="link"><input class="copy-area" readonly="true" id="copy-link" value="http://humble2967738843.github.io/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/" /></div>
        <div class="social-wrap dis-select"><a class="social share-item wechat" onclick="util.toggle(&quot;qrcode-wechat&quot)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/b32ef3da1162a.svg" /></a><a class="social share-item weibo" target="_blank" rel="external nofollow noopener noreferrer" href="https://service.weibo.com/share/share.php?url=http://humble2967738843.github.io/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/&title=我们可以编辑多模态大型语言模型吗？ - humbleyl&summary=我们可以编辑多模态大型语言模型吗？Abstract在本文中，我们重点关注编辑多模态大型语言模型（MLLM）。与编辑单模态LLMs相比，多模态模型编辑更具挑战性，需要在编辑过程中进行更高水平的审查和仔细考虑。为了促进这一领域的研究，我们..."><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/80c07e4dbb303.svg" /></a><a class="social share-item email" href="mailto:?subject=我们可以编辑多模态大型语言模型吗？ - humbleyl&amp;body=http://humble2967738843.github.io/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/a1b00e20f425d.svg" /></a><a class="social share-item link" onclick="util.copy(&quot;copy-link&quot;, &quot;复制成功&quot;)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/8411ed322ced6.svg" /></a></div>
        
        <div class="qrcode" id="qrcode-wechat" style="opacity:0;height:0">
          <img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://api.qrserver.com/v1/create-qr-code/?size=256x256&data=http://humble2967738843.github.io/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/"/>
        </div>
        
      </div>
    </section>
    </div>
</article>
<div class="related-wrap" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2023/11/02/duo-yu-yan-mo-xing-zhong-shi-shi-zhi-shi-de-kua-yu-yan-yi-zhi-xing/">多语言模型中事实知识的跨语言一致性</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2023/11/02/wo-men-ke-yi-tong-guo-qing-jing-xue-xi-lai-bian-ji-shi-shi-zhi-shi-ma/">我们可以通过情景学习来编辑事实知识吗？</a></div></section></div>

<div class="related-wrap" id="related-posts">
    <section class='header'>
      <div class='title cap theme'>您可能感兴趣的文章</div>
    </section>
    <section class='body'>
    <div class="related-posts"><a class="item" href="\2023\11\02\duo-yu-yan-mo-xing-zhong-shi-shi-zhi-shi-de-kua-yu-yan-yi-zhi-xing\" title="多语言模型中事实知识的跨语言一致性"><span class="title">多语言模型中事实知识的跨语言一致性</span></a><a class="item" href="\2023\11\02\qing-jing-xue-xi-chuang-jian-ren-wu-xiang-liang\" title="情境学习创建任务向量"><span class="title">情境学习创建任务向量</span></a><a class="item" href="\2023\11\06\wang-diao-ni-xiang-wang-diao-de-dong-xi-llms-de-gao-xiao-wang-que\" title="忘掉你想忘掉的东西：LLMs的高效忘却"><span class="title">忘掉你想忘掉的东西：LLMs的高效忘却</span></a><a class="item" href="\2023\11\02\wo-men-ke-yi-tong-guo-qing-jing-xue-xi-lai-bian-ji-shi-shi-zhi-shi-ma\" title="我们可以通过情景学习来编辑事实知识吗？"><span class="title">我们可以通过情景学习来编辑事实知识吗？</span></a><a class="item" href="\2023\11\06\ji-yu-ti-shi-de-ling-yang-ben-guan-xi-chou-qu-fang-fa-tan-suo\" title="基于提示的零样本关系抽取方法探索"><span class="title">基于提示的零样本关系抽取方法探索</span></a></div></section></div>


  <div class="related-wrap md-text" id="comments">
    <section class='header cmt-title cap theme'>
      <p>快来参与讨论吧~</p>

    </section>
    <section class='body cmt-body giscus'>
      

<svg class="loading" style="vertical-align:middle;fill:currentColor;overflow:hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg>

<div id="giscus" src="https://giscus.app/client.js" data-repo="Humble2967738843/giscus" data-repo-id="R_kgDOLsS5kA" data-category="Announcements" data-category-id="DIC_kwDOLsS5kM4Cel5C" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="preferred_color_scheme" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous"></div>

    </section>
  </div>



<footer class="page-footer footnote"><hr><div class="text"><p>本站由 <a href="/">yuan long</a> 使用 <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.27.0">Stellar 1.27.0</a> 主题创建。<br>本博客所有文章除特别声明外，均采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> 许可协议，转载请注明出处。</p>
</div></footer>
<div class="main-mask" onclick="sidebar.dismiss()"></div></div><aside class="l_right">
<div class="widgets">



<widget class="widget-wrapper toc" id="data-toc" collapse="false"><div class="widget-header dis-select"><span class="name">本文目录</span><a class="cap-action" onclick="sidebar.toggleTOC()" ><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg></a></div><div class="widget-body"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%88%91%E4%BB%AC%E5%8F%AF%E4%BB%A5%E7%BC%96%E8%BE%91%E5%A4%9A%E6%A8%A1%E6%80%81%E5%A4%A7%E5%9E%8B%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E5%90%97%EF%BC%9F"><span class="toc-text">我们可以编辑多模态大型语言模型吗？</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstract"><span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1 Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Related-Work"><span class="toc-text">2 Related Work</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Multimodal-Language-Models"><span class="toc-text">2.1 Multimodal Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Model-Editing"><span class="toc-text">2.2 Model Editing</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Editing-Multimodal-LLMs"><span class="toc-text">3 Editing Multimodal LLMs</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Task-Definition"><span class="toc-text">3.1 Task Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-Datasets"><span class="toc-text">3.2 Datasets</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-1-Reliability-Dataset-Construction"><span class="toc-text">3.2.1 Reliability Dataset Construction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-2-Locality-Dataset-Construction"><span class="toc-text">3.2.2 Locality Dataset Construction</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-3-Generality-Dataset-Construction"><span class="toc-text">3.2.3 Generality Dataset Construction</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-Multimodal-Language-Models"><span class="toc-text">3.3 Multimodal Language Models</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-Baselines"><span class="toc-text">3.4 Baselines</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Experiments"><span class="toc-text">4 Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-Results"><span class="toc-text">4.1 Results</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Editing-Different-Component"><span class="toc-text">4.2 Editing Different Component</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Conclusion"><span class="toc-text">5 Conclusion</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#6-Limitations"><span class="toc-text">6 Limitations</span></a></li></ol></li></ol></div><div class="widget-footer">

<a class="top" onclick="util.scrollTop()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 12c0-4.714 0-7.071 1.464-8.536C4.93 2 7.286 2 12 2c4.714 0 7.071 0 8.535 1.464C22 4.93 22 7.286 22 12c0 4.714 0 7.071-1.465 8.535C19.072 22 16.714 22 12 22s-7.071 0-8.536-1.465C2 19.072 2 16.714 2 12Z"/><path stroke-linecap="round" stroke-linejoin="round" d="m9 15.5l3-3l3 3m-6-4l3-3l3 3"/></g></svg><span>回到顶部</span></a></div></widget>
</div></aside><div class='float-panel blur'>
  <button type='button' style='display:none' class='laptop-only rightbar-toggle mobile' onclick='sidebar.rightbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg>
  </button>
  <button type='button' style='display:none' class='mobile-only leftbar-toggle mobile' onclick='sidebar.leftbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 11c0-3.771 0-5.657 1.172-6.828C4.343 3 6.229 3 10 3h4c3.771 0 5.657 0 6.828 1.172C22 5.343 22 7.229 22 11v2c0 3.771 0 5.657-1.172 6.828C19.657 21 17.771 21 14 21h-4c-3.771 0-5.657 0-6.828-1.172C2 18.657 2 16.771 2 13z"/><path id="sep" stroke-linecap="round" d="M5.5 10h6m-5 4h4m4.5 7V3"/></g></svg>
  </button>
</div>
</div><div class="scripts">
<script type="text/javascript">
  const ctx = {
    date_suffix: {
      just: `刚刚`,
      min: `分钟前`,
      hour: `小时前`,
      day: `天前`,
    },
    root : `/`,
  };

  // required plugins (only load if needs)
  if (`local_search`) {
    ctx.search = {};
    ctx.search.service = `local_search`;
    if (ctx.search.service == 'local_search') {
      let service_obj = Object.assign({}, `{"field":"all","path":"/search.json","content":true,"codeblock":true,"sort":"-date"}`);
      ctx.search[ctx.search.service] = service_obj;
    }
  }
  const def = {
    avatar: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/3442075.svg`,
    cover: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/cover/76b86c0226ffd.svg`,
  };
  const deps = {
    jquery: `https://cdn.bootcdn.net/ajax/libs/jquery/3.7.1/jquery.min.js`,
    marked: `https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js`
  }
  

</script>

<script type="text/javascript">
  const utils = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    css: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    js: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')){
        src = ctx.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    jq: (fn) => {
      if (typeof jQuery === 'undefined') {
        utils.js(deps.jquery).then(fn)
      } else {
        fn()
      }
    },
    
    onLoading: (el) => {
      if (el) {
        $(el).append('<div class="loading-wrap"><svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" stroke-opacity=".3" d="M12 3C16.9706 3 21 7.02944 21 12C21 16.9706 16.9706 21 12 21C7.02944 21 3 16.9706 3 12C3 7.02944 7.02944 3 12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="1.3s" values="60;0"/></path><path stroke-dasharray="15" stroke-dashoffset="15" d="M12 3C16.9706 3 21 7.02944 21 12"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.3s" values="15;0"/><animateTransform attributeName="transform" dur="1.5s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></g></svg></div>');
      }
    },
    onLoadSuccess: (el) => {
      if (el) {
        $(el).find('.loading-wrap').remove();
      }
    },
    onLoadFailure: (el) => {
      if (el) {
        $(el).find('.loading-wrap svg').remove();
        $(el).find('.loading-wrap').append('<svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" d="M12 3L21 20H3L12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.5s" values="60;0"/></path><path stroke-dasharray="6" stroke-dashoffset="6" d="M12 10V14"><animate fill="freeze" attributeName="stroke-dashoffset" begin="0.6s" dur="0.2s" values="6;0"/></path></g><circle cx="12" cy="17" r="1" fill="currentColor" fill-opacity="0"><animate fill="freeze" attributeName="fill-opacity" begin="0.8s" dur="0.4s" values="0;1"/></circle></svg>');
        $(el).find('.loading-wrap').addClass('error');
      }
    },
    request: (el, url, callback, onFailure) => {
      let retryTimes = 3;
      utils.onLoading(el);
      function req() {
        return new Promise((resolve, reject) => {
          let status = 0; // 0 等待 1 完成 2 超时
          let timer = setTimeout(() => {
            if (status === 0) {
              status = 2;
              timer = null;
              reject('请求超时');
              if (retryTimes == 0) {
                onFailure();
              }
            }
          }, 5000);
          fetch(url).then(function(response) {
            if (status !== 2) {
              clearTimeout(timer);
              resolve(response);
              timer = null;
              status = 1;
            }
            if (response.ok) {
              return response.json();
            }
            throw new Error('Network response was not ok.');
          }).then(function(data) {
            retryTimes = 0;
            utils.onLoadSuccess(el);
            callback(data);
          }).catch(function(error) {
            if (retryTimes > 0) {
              retryTimes -= 1;
              setTimeout(() => {
                req();
              }, 5000);
            } else {
              utils.onLoadFailure(el);
              onFailure();
            }
          });
        });
      }
      req();
    },
  };
</script>

<script>
  const sidebar = {
    leftbar: () => {
      if (l_body) {
        l_body.toggleAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    rightbar: () => {
      if (l_body) {
        l_body.toggleAttribute('rightbar');
        l_body.removeAttribute('leftbar');
      }
    },
    dismiss: () => {
      if (l_body) {
        l_body.removeAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    toggleTOC: () => {
      document.querySelector('#data-toc').classList.toggle('collapse');
    }
  }
</script>

<!-- required -->
<script src="/js/main.js?v=1.27.0" async></script>

<!-- optional -->

  <script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const els = document.querySelectorAll("#comments #giscus");
    if (els.length === 0) return;
    els.forEach((el, i) => {
      try {
        el.innerHTML = '';
      } catch (error) {
        console.error(error);
      }
      var script = document.createElement('script');
      script.async = true;
      for (let key of Object.keys(el.attributes)) {
        let attr = el.attributes[key];
        if (['class', 'id'].includes(attr.name) === false) {
          script.setAttribute(attr.name, attr.value);
        }
      }
      el.appendChild(script);
    });
  });
</script>




<script defer>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.services = Object.assign({}, JSON.parse(`{"mdrender":{"js":"/js/services/mdrender.js"},"siteinfo":{"js":"/js/services/siteinfo.js","api":null},"ghinfo":{"js":"/js/services/ghinfo.js"},"sites":{"js":"/js/services/sites.js"},"friends":{"js":"/js/services/friends.js"},"timeline":{"js":"/js/services/timeline.js"},"fcircle":{"js":"/js/services/fcircle.js"},"weibo":{"js":"/js/services/weibo.js"},"memos":{"js":"/js/services/memos.js"}}`));
    for (let id of Object.keys(ctx.services)) {
      const js = ctx.services[id].js;
      if (id == 'siteinfo') {
        ctx.cardlinks = document.querySelectorAll('a.link-card[cardlink]');
        if (ctx.cardlinks?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            setCardLink(ctx.cardlinks);
          });
        }
      } else {
        const els = document.getElementsByClassName(`ds-${id}`);
        if (els?.length > 0) {
          utils.jq(() => {
            if (id == 'timeline' || 'memos' || 'marked') {
              utils.js(deps.marked).then(function () {
                utils.js(js, { defer: true });
              });
            } else {
              utils.js(js, { defer: true });
            }
          });
        }
      }
    }
  });
</script>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.search = {
      path: `/search.json`,
    }
    utils.js('/js/search/local-search.js', { defer: true });
  });
</script><script>
  window.FPConfig = {
    delay: 0,
    ignoreKeywords: [],
    maxRPS: 5,
    hoverDelay: 25
  };
</script>
<script defer src="https://cdn.bootcdn.net/ajax/libs/flying-pages/2.1.2/flying-pages.min.js"></script><script defer src="https://cdn.bootcdn.net/ajax/libs/vanilla-lazyload/17.8.4/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazy",
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    window.lazyLoadInstance?.update();
  });
</script><script>
  ctx.fancybox = {
    selector: `.timenode p>img`,
    css: `https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.22/fancybox/fancybox.min.css`,
    js: `https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.22/fancybox/fancybox.umd.min.js`
  };
  var selector = '[data-fancybox]:not(.error)';
  if (ctx.fancybox.selector) {
    selector += `, ${ctx.fancybox.selector}`
  }
  var needFancybox = document.querySelectorAll(selector).length !== 0;
  if (!needFancybox) {
    const els = document.getElementsByClassName('ds-memos');
    if (els != undefined && els.length > 0) {
      needFancybox = true;
    }
  }
  if (needFancybox) {
    utils.css(ctx.fancybox.css);
    utils.js(ctx.fancybox.js, { defer: true }).then(function () {
      Fancybox.bind(selector, {
        hideScrollbar: false,
        Thumbs: {
          autoStart: false,
        },
        caption: (fancybox, slide) => {
          return slide.triggerEl.alt || null
        }
      });
    })
  }
</script><script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const swiper_api = document.getElementById('swiper-api');
    if (swiper_api != undefined) {
      utils.css(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.css`);
      utils.js(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.js`, { defer: true }).then(function () {
        const effect = swiper_api.getAttribute('effect') || '';
        var swiper = new Swiper('.swiper#swiper-api', {
          slidesPerView: 'auto',
          spaceBetween: 8,
          centeredSlides: true,
          effect: effect,
          loop: true,
          pagination: {
            el: '.swiper-pagination',
            clickable: true,
          },
          navigation: {
            nextEl: '.swiper-button-next',
            prevEl: '.swiper-button-prev',
          },
        });
      })
    }
  });
</script><script>
  document.addEventListener('DOMContentLoaded', function () {
    window.codeElements = document.querySelectorAll('.code');
    if (window.codeElements.length > 0) {
      ctx.copycode = {
        default_text: `Copy`,
        success_text: `Copied`,
        toast: `复制成功`,
      };
      utils.js('/js/plugins/copycode.js');
    }
  });
</script>


<!-- inject -->

</div></body></html>

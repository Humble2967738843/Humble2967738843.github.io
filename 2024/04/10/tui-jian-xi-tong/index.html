
<!DOCTYPE html><html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.27.0" theme-name="Stellar" theme-version="1.27.0">
  
  <meta name="generator" content="Hexo 7.0.0">
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  
  <title>推荐系统 - humbleyl</title>

  
    <meta name="description" content="第一章 推荐系统概述1.1推荐系统的意义推荐系统就是一个将信息生产者和信息消费者连接起来的桥梁。平台往往会作为推荐系统的载体，实现信息生产者和消费者之间信息的匹配。上述提到的平台方、信息生产者和消费者可以分别用平台方（如：腾讯视频、淘宝、网易云音乐等）、物品（如：视频、商品、音乐等）和用户和来指代。下面分别从这三方需求出发，介绍推荐系统的存在的意义。 平台方平台方一般是为信息生产者提供物品展示的位">
<meta property="og:type" content="article">
<meta property="og:title" content="推荐系统">
<meta property="og:url" content="http://humble2967738843.github.io/2024/04/10/tui-jian-xi-tong/index.html">
<meta property="og:site_name" content="humbleyl">
<meta property="og:description" content="第一章 推荐系统概述1.1推荐系统的意义推荐系统就是一个将信息生产者和信息消费者连接起来的桥梁。平台往往会作为推荐系统的载体，实现信息生产者和消费者之间信息的匹配。上述提到的平台方、信息生产者和消费者可以分别用平台方（如：腾讯视频、淘宝、网易云音乐等）、物品（如：视频、商品、音乐等）和用户和来指代。下面分别从这三方需求出发，介绍推荐系统的存在的意义。 平台方平台方一般是为信息生产者提供物品展示的位">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://s2.loli.net/2024/04/11/MlPqAFQacgiJpwL.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/11/1BOuDUGCretljZF.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/11/PNs4UCxmSDdvIuH.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/12/cubdwvZhHnsAx87.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/13/otixzas2RVNmWl8.png">
<meta property="og:image" content="c:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240417131451288.png">
<meta property="og:image" content="c:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240417131728890.png">
<meta property="og:image" content="http://ryluo.oss-cn-chengdu.aliyuncs.com/%E5%9B%BE%E7%89%87image-20220410000221817.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/17/xZzb5hRlyasTJM9.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/11/bZEseFk2vx4RM7q.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/11/n1FNf8TB9seOc7S.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/11/c8hMAeuiIvyJgxp.png">
<meta property="og:image" content="c:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240427215458380.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/27/8viM1fWGdKuEXN3.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/27/Ma1HrBGQpk3xsdy.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/28/2KL4yXo7pnV8YMJ.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/28/s7BDKTPaZ3VEGuR.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/29/nL1HB7tQdFo2yw6.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/29/mKQL1DdTsNwu2Ec.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/29/IzmOvTwlacWe4XQ.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/29/JiMa9vt7uXpqLyD.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/29/HNqI53UDMw2V6zc.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/29/TXCVQtEjUomghYB.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/29/EqmsJanTiGpb4yt.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/20/qpC37U1iFvQArsD.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/20/RIcVbTMsCmjkwWO.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/20/ErF3zNLtidIesAv.png">
<meta property="og:image" content="https://s2.loli.net/2024/04/20/fDnehZHYQj91dLM.png">
<meta property="og:image" content="c:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240420223257343.png">
<meta property="og:image" content="https://s2.loli.net/2024/05/02/1kzsxSruqDcEl2B.png">
<meta property="og:image" content="https://s2.loli.net/2024/05/02/ui65yf3hXp1zjE4.png">
<meta property="og:image" content="https://s2.loli.net/2024/05/02/AH9h3XGcQL6nBFr.png">
<meta property="article:published_time" content="2024-04-10T04:09:32.000Z">
<meta property="article:modified_time" content="2024-05-26T09:25:07.225Z">
<meta property="article:author" content="yuan long">
<meta property="article:tag" content="DeepFM">
<meta property="article:tag" content="FFM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://s2.loli.net/2024/04/11/MlPqAFQacgiJpwL.png">
<meta name="twitter:creator" content="@humbleyl">
  
  
  
  <meta name="keywords" content="DeepFM,FFM">

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="humbleyl" type="application/atom+xml">
  

  <link rel="stylesheet" href="/css/main.css?v=1.27.0">

  
    <link rel="shortcut icon" href="solar:documents-bold-duotone">
  

  

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>
<body>

<div class="l_body s:aa content tech" id="start" layout="post" ><aside class="l_left"><div class="leftbar-container">


<header class="header"><div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="https://s2.loli.net/2024/04/10/32Y4obwgxJCeOMr.png" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></a><a class="title" href="/"><div class="main" ff="title">humbleyl</div><div class="sub normal cap">院龙的博客</div><div class="sub hover cap" style="opacity:0"> humbleyl</div></a></div></header>

<div class="nav-area">
<div class="search-wrapper" id="search-wrapper"><form class="search-form"><a class="search-button" onclick="document.getElementById(&quot;search-input&quot;).focus();"><svg t="1705074644177" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1560" width="200" height="200"><path d="M1008.839137 935.96571L792.364903 719.491476a56.783488 56.783488 0 0 0-80.152866 0 358.53545 358.53545 0 1 1 100.857314-335.166073 362.840335 362.840335 0 0 1-3.689902 170.145468 51.248635 51.248635 0 1 0 99.217358 26.444296 462.057693 462.057693 0 1 0-158.255785 242.303546l185.930047 185.725053a51.248635 51.248635 0 0 0 72.568068 0 51.248635 51.248635 0 0 0 0-72.978056z" p-id="1561"></path><path d="M616.479587 615.969233a50.428657 50.428657 0 0 0-61.498362-5.534852 174.655348 174.655348 0 0 1-177.525271 3.484907 49.403684 49.403684 0 0 0-58.833433 6.76482l-3.074918 2.869923a49.403684 49.403684 0 0 0 8.609771 78.10292 277.767601 277.767601 0 0 0 286.992355-5.739847 49.403684 49.403684 0 0 0 8.404776-76.667958z" p-id="1562"></path></svg></a><input type="text" class="search-input" id="search-input" placeholder="站内搜索"></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div>


<nav class="menu dis-select"><a class="nav-item active" title="博客" href="/" style="color:#1BCDFC"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M5.879 2.879C5 3.757 5 5.172 5 8v8c0 2.828 0 4.243.879 5.121C6.757 22 8.172 22 11 22h2c2.828 0 4.243 0 5.121-.879C19 20.243 19 18.828 19 16V8c0-2.828 0-4.243-.879-5.121C17.243 2 15.828 2 13 2h-2c-2.828 0-4.243 0-5.121.879M8.25 17a.75.75 0 0 1 .75-.75h3a.75.75 0 0 1 0 1.5H9a.75.75 0 0 1-.75-.75M9 12.25a.75.75 0 0 0 0 1.5h6a.75.75 0 0 0 0-1.5zM8.25 9A.75.75 0 0 1 9 8.25h6a.75.75 0 0 1 0 1.5H9A.75.75 0 0 1 8.25 9" clip-rule="evenodd"/><path fill="currentColor" d="M5.235 4.058C5 4.941 5 6.177 5 8v8c0 1.823 0 3.058.235 3.942L5 19.924c-.975-.096-1.631-.313-2.121-.803C2 18.243 2 16.828 2 14v-4c0-2.829 0-4.243.879-5.121c.49-.49 1.146-.707 2.121-.803zm13.53 15.884C19 19.058 19 17.822 19 16V8c0-1.823 0-3.059-.235-3.942l.235.018c.975.096 1.631.313 2.121.803C22 5.757 22 7.17 22 9.999v4c0 2.83 0 4.243-.879 5.122c-.49.49-1.146.707-2.121.803z" opacity=".5"/></svg></a><a class="nav-item" title="文档" href="/wiki/" style="color:#3DC550"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" fill-rule="evenodd" d="M14.25 4.48v3.057c0 .111 0 .27.02.406a.936.936 0 0 0 .445.683a.96.96 0 0 0 .783.072c.13-.04.272-.108.378-.159L17 8.005l1.124.534c.106.05.248.119.378.16a.958.958 0 0 0 .783-.073a.936.936 0 0 0 .444-.683c.021-.136.021-.295.021-.406V3.031c.113-.005.224-.01.332-.013C21.154 2.98 22 3.86 22 4.933v11.21c0 1.112-.906 2.01-2.015 2.08c-.97.06-2.108.179-2.985.41c-1.082.286-1.99 1.068-3.373 1.436c-.626.167-1.324.257-1.627.323V5.174c.32-.079 1.382-.203 1.674-.371c.184-.107.377-.216.576-.323m5.478 8.338a.75.75 0 0 1-.546.91l-4 1a.75.75 0 0 1-.364-1.456l4-1a.75.75 0 0 1 .91.546" clip-rule="evenodd"/><path fill="currentColor" d="M18.25 3.151c-.62.073-1.23.18-1.75.336a8.2 8.2 0 0 0-.75.27v3.182l.75-.356l.008-.005a1.13 1.13 0 0 1 .492-.13c.047 0 .094.004.138.01c.175.029.315.1.354.12l.009.005l.749.356V3.647z"/><path fill="currentColor" d="M12 5.214c-.334-.064-1.057-.161-1.718-.339C8.938 4.515 8.05 3.765 7 3.487c-.887-.234-2.041-.352-3.018-.412C2.886 3.007 2 3.9 2 4.998v11.146c0 1.11.906 2.01 2.015 2.079c.97.06 2.108.179 2.985.41c.486.129 1.216.431 1.873.726c1.005.451 2.052.797 3.127 1.034z" opacity=".5"/><path fill="currentColor" d="M4.273 12.818a.75.75 0 0 1 .91-.545l4 1a.75.75 0 1 1-.365 1.455l-4-1a.75.75 0 0 1-.545-.91m.909-4.545a.75.75 0 1 0-.364 1.455l4 1a.75.75 0 0 0 .364-1.455z"/></svg></a><a class="nav-item" title="探索" href="/explore/" style="color:#FA6400"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M20 12a8 8 0 1 1-16 0a8 8 0 0 1 16 0" opacity=".5"/><path fill="currentColor" d="M17.712 5.453c1.047-.193 2.006-.259 2.797-.152c.77.103 1.536.393 1.956 1.064c.446.714.312 1.542-.012 2.258c-.33.728-.918 1.499-1.672 2.268c-1.516 1.547-3.836 3.226-6.597 4.697c-2.763 1.472-5.495 2.484-7.694 2.92c-1.095.217-2.098.299-2.923.201c-.8-.095-1.6-.383-2.032-1.075c-.47-.752-.296-1.63.07-2.379c.375-.768 1.032-1.586 1.872-2.403L4 12.416c0 .219.083.71.168 1.146c.045.23.09.444.123.596c-.652.666-1.098 1.263-1.339 1.756c-.277.567-.208.825-.145.925c.072.116.305.305.937.38c.609.073 1.44.018 2.455-.183c2.02-.4 4.613-1.351 7.28-2.772c2.667-1.42 4.85-3.015 6.23-4.423c.694-.707 1.15-1.334 1.377-1.836c.233-.515.167-.75.107-.844c-.07-.112-.289-.294-.883-.374c-.542-.072-1.272-.041-2.163.112L16.87 5.656c.338-.101.658-.17.842-.203"/></svg></a><a class="nav-item" title="社交" href="/friends/" style="color:#F44336"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="m13.629 20.472l-.542.916c-.483.816-1.69.816-2.174 0l-.542-.916c-.42-.71-.63-1.066-.968-1.262c-.338-.197-.763-.204-1.613-.219c-1.256-.021-2.043-.098-2.703-.372a5 5 0 0 1-2.706-2.706C2 14.995 2 13.83 2 11.5v-1c0-3.273 0-4.91.737-6.112a5 5 0 0 1 1.65-1.651C5.59 2 7.228 2 10.5 2h3c3.273 0 4.91 0 6.113.737a5 5 0 0 1 1.65 1.65C22 5.59 22 7.228 22 10.5v1c0 2.33 0 3.495-.38 4.413a5 5 0 0 1-2.707 2.706c-.66.274-1.447.35-2.703.372c-.85.015-1.275.022-1.613.219c-.338.196-.548.551-.968 1.262" opacity=".5"/><path fill="currentColor" d="M10.99 14.308c-1.327-.978-3.49-2.84-3.49-4.593c0-2.677 2.475-3.677 4.5-1.609c2.025-2.068 4.5-1.068 4.5 1.609c0 1.752-2.163 3.615-3.49 4.593c-.454.335-.681.502-1.01.502c-.329 0-.556-.167-1.01-.502"/></svg></a></nav>
</div>
<div class="widgets">
<widget class="widget-wrapper post-list"><div class="widget-header dis-select"><span class="name">专栏：搜广推</span></div><div class="widget-body"><a class="item active" href="/2024/04/10/tui-jian-xi-tong/"><span class="title">推荐系统</span><svg class="active-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M21 11.098v4.993c0 3.096 0 4.645-.734 5.321c-.35.323-.792.526-1.263.58c-.987.113-2.14-.907-4.445-2.946c-1.02-.901-1.529-1.352-2.118-1.47a2.225 2.225 0 0 0-.88 0c-.59.118-1.099.569-2.118 1.47c-2.305 2.039-3.458 3.059-4.445 2.945a2.238 2.238 0 0 1-1.263-.579C3 20.736 3 19.188 3 16.091v-4.994C3 6.81 3 4.666 4.318 3.333C5.636 2 7.758 2 12 2c4.243 0 6.364 0 7.682 1.332C21 4.665 21 6.81 21 11.098" opacity=".5"/><path fill="currentColor" d="M9 5.25a.75.75 0 0 0 0 1.5h6a.75.75 0 0 0 0-1.5z"/></svg></a></div></widget>

<widget class="widget-wrapper post-list"><div class="widget-header dis-select"><span class="name">最近更新</span></div><div class="widget-body fs14"><a class="item title" href="/2024/05/06/ji-yu-lian-shi-tui-li-de-wen-dang-ji-shi-jian-lun-yuan-ti-qu/"><span class="title">基于链式推理的文档级事件论元提取</span></a><a class="item title" href="/2023/11/02/duo-yu-yan-mo-xing-zhong-shi-shi-zhi-shi-de-kua-yu-yan-yi-zhi-xing/"><span class="title">多语言模型中事实知识的跨语言一致性</span></a><a class="item title" href="/2023/11/02/zhi-shi-shen-jing-yuan-zhong-xin-zhi-lu-yu-yan-wu-guan-zhi-shi-shen-jing-yuan-he-jian-bing-zhi-shi-shen-jing-yuan-de-fa-xian/"><span class="title">知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现</span></a><a class="item title" href="/2023/11/02/wo-men-ke-yi-tong-guo-qing-jing-xue-xi-lai-bian-ji-shi-shi-zhi-shi-ma/"><span class="title">我们可以通过情景学习来编辑事实知识吗？</span></a><a class="item title" href="/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/"><span class="title">我们可以编辑多模态大型语言模型吗？</span></a><a class="item title" href="/2023/11/06/wang-diao-ni-xiang-wang-diao-de-dong-xi-llms-de-gao-xiao-wang-que/"><span class="title">忘掉你想忘掉的东西：LLMs的高效忘却</span></a><a class="item title" href="/2023/11/02/qing-jing-xue-xi-chuang-jian-ren-wu-xiang-liang/"><span class="title">情境学习创建任务向量</span></a><a class="item title" href="/2023/11/06/ji-yu-ti-shi-de-ling-yang-ben-guan-xi-chou-qu-fang-fa-tan-suo/"><span class="title">基于提示的零样本关系抽取方法探索</span></a><a class="item title" href="/2023/11/02/bian-ji-da-xing-yu-yan-mo-xing-wen-ti-fang-fa-he-ji-yu/"><span class="title">编辑大型语言模型：问题、方法和机遇</span></a><a class="item title" href="/2023/11/03/bing-fa-bian-cheng/"><span class="title">并发编程</span></a></div></widget>
</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://github.com/Humble2967738843" target="_blank" rel="external nofollow noopener noreferrer"><svg t="1716701113980" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4745" width="200" height="200"><path d="M512 512m-469.333333 0a469.333333 469.333333 0 1 0 938.666666 0 469.333333 469.333333 0 1 0-938.666666 0Z" fill="#434A54" p-id="4746"></path><path d="M610.688 808.149333c0-12.074667 0.426667-51.498667 0.426667-100.437333 0-34.133333-11.733333-56.448-24.832-67.84 81.493333-9.045333 167.125333-39.978667 167.125333-180.608 0-39.936-14.208-72.618667-37.674667-98.261333 3.84-9.216 16.341333-46.421333-3.584-96.853334 0 0-30.72-9.813333-100.565333 37.546667a351.658667 351.658667 0 0 0-91.733333-12.373333 350.549333 350.549333 0 0 0-91.605334 12.373333c-69.973333-47.36-100.693333-37.546667-100.693333-37.546667-19.882667 50.432-7.338667 87.637333-3.541333 96.853334a141.653333 141.653333 0 0 0-37.717334 98.261333c0 140.288 85.461333 171.690667 166.784 180.906667-10.453333 9.173333-19.968 25.301333-23.253333 48.981333-20.906667 9.386667-73.856 25.514667-106.496-30.421333 0 0-19.370667-35.157333-56.149333-37.76 0 0-35.712-0.426667-2.474667 22.272 0 0 23.978667 11.264 40.618667 53.546666 0 0 21.504 65.365333 123.349333 43.221334 0.170667 30.592 0.512 59.392 0.512 68.138666a19.968 19.968 0 0 1-2.218667 9.173334 339.925333 339.925333 0 0 0 187.904 3.114666 19.2 19.2 0 0 1-4.181333-12.288z" fill="#FFFFFF" p-id="4747"></path><path d="M180.138667 843.861333A467.882667 467.882667 0 0 0 512 981.333333c259.2 0 469.333333-210.133333 469.333333-469.333333 0-129.621333-52.522667-246.954667-137.472-331.861333L180.138667 843.861333z" fill="#231F20" opacity=".1" p-id="4748"></path></svg></a><a class="social" href="https://" target="_blank" rel="external nofollow noopener noreferrer"><svg t="1716701237965" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="12408" width="200" height="200"><path d="M276.822886 874.188342c-47.901937-0.484303-85.721573-1.942714-96.222136 64.483801-4.204628 59.150968 41.545465 67.092431 120.596869 81.461911 86.981861 4.49631 137.949214 16.482801 205.118694-49.93821l-227.814878-96.106564" fill="" p-id="12409"></path><path d="M270.939709 875.64125c-24.792995 0-64.296684-0.968605-81.098684 43.647779 2.102314 5.332833 12.327704 2.982864 18.794246-1.562977 9.691557-4.589869 5.162226 1.799625-5.552971 10.412507-11.661788 8.12858-37.660036 34.54509 6.461038 61.633019 46.801249 21.645028 193.974228 61.280799 285.678041-16.169105 0 0-180.892553-56.256158-222.493052-97.944713h-1.788618v-0.01651z" fill="#F0971C" p-id="12410"></path><path d="M764.482659 865.94419c47.896434-0.484303 85.710567-1.948218 96.222136 64.4838 4.204628 59.150968-64.731455 68.859035-120.602373 81.461912-59.734332 5.51995-137.949214 16.488305-205.118693-49.93821l227.820381-96.106564" fill="" p-id="12411"></path><path d="M770.360332 867.402601c24.787491 0 64.29118-0.974109 81.087678 43.642275-2.09681 5.332833-12.316698 2.982864-18.777736-1.562976-9.708067-4.589869-5.16773 1.799625 5.547467 10.412507 11.661788 8.123077 37.66554 34.54509-6.461038 61.633019-46.801249 21.645028-193.974228 61.275295-285.683544-16.174609 0 0 180.903559-56.250655 222.504059-97.939209l1.783114-0.011007z" fill="#F0971C" p-id="12412"></path><path d="M825.598354 441.216247c51.259035 54.307941 83.206502 107.641773 108.412255 245.337829 3.417636 83.393619-7.567229 91.153468-31.947466 113.453405-17.638524-2.905816-36.124577-30.059786-54.616134-94.070291-18.491557-63.993994-21.848655-264.720943-21.848655-264.720943z" fill="" p-id="12413"></path><path d="M847.452513 380.128069c0-15.21701-4.716448-29.894683-13.466917-43.741337C832.994977 150.420008 697.560835 0 530.619501 0 363.683671 0 228.249529 150.420008 227.25891 336.386732c-8.744965 13.852157-13.466916 28.52983-13.466917 43.735833 0 22.228392 10.043777 43.334083 28.078549 62.342963 39.349592 136.000997 153.744086 234.385984 288.748959 234.385984 135.015881 0 249.404871-98.384988 288.759967-234.385984 18.034771-19.003377 28.073045-40.109067 28.073045-62.337459zM198.035646 442.184853C146.771107 496.48729 114.834648 549.826626 89.623392 687.517178c-3.423139 83.393619 25.899186 111.031892 31.936459 113.458909 6.042777 2.410506 36.135584-30.06529 54.62714-94.064788 18.486053-63.988491 21.848655-264.726446 21.848655-264.726446z" fill="" p-id="12414"></path><path d="M864.254513 621.585973c0 200.82601-158.394492 363.634261-353.810621 363.634261-195.394116 0-353.805118-162.80825-353.805118-363.634261 0-200.831514 158.405499-363.639764 353.805118-363.639764 195.410626 0 353.810621 162.802747 353.810621 363.639764z" fill="" p-id="12415"></path><path d="M220.50619 696.245633c0 149.952216 134.702185 271.517691 300.86203 271.517691 166.154342 0 300.86203-121.565475 300.86203-271.517691s-134.707688-271.517691-300.86203-271.51769c-166.154342 0.005503-300.86203 121.565475-300.86203 271.51769z" fill="#FFFFFF" p-id="12416"></path><path d="M232.068916 322.672161s-23.950969 19.636272-18.909818 60.361724c-25.211256 30.544089-20.797498 62.54659-9.454909 95.281048 18.909818 62.788741 146.650152 162.665161 309.684043 162.665161 223.544209-15.018886 289.090173-123.392617 338.890797-180.122071 0 0 29.003126-47.280049-18.909818-129.457407l-72.491305 7.275547-528.80899-16.004002z" fill="" p-id="12417"></path><path d="M302.980734 572.85852c-3.147967 33.086678-15.128955 139.996494 3.786366 166.545086 20.170106 26.190869 44.43477 38.909317 96.112067 30.907316 23.323577-7.638774 28.364727-29.817635 26.163352-61.809129l-0.946592-92.364225-125.115193-43.279048z" fill="" p-id="12418"></path><path d="M223.246902 372.126068s83.200998 144.002998 271.033388 151.273041c83.206502 1.458411 167.662284 8.728455 292.458278-87.26804 24.583864-25.453408 29.003126-33.455409 29.003126-33.455409s8.189118 65.457909-112.820509 147.635268c0 0 82.562599 0.731957 141.812629-170.182859 0 0 29.003126 111.274043-72.480298 164.36022-93.288803 56.008504-154.432015 93.090679-309.480417 84.362224-101.874168-25.326829-254.005746-65.457909-264.726446-192.730451 0 0 6.306942-29.817635 14.49606-24.000499 0 0 0.627392 69.095683 52.943088 92.364225-0.011007 0.005503-52.326703-77.807627-42.238899-132.35772z" fill="#E71F19" p-id="12419"></path><path d="M321.059532 589.605486s-36.240149 138.185862 4.413759 161.459908c41.2813 19.273045 66.498059 14.182364 94.229891 4.727454 11.661788-16.004002 1.260288-111.63727 4.721951-121.455406-6.306942-4.006504-24.892057-8.370731-28.051031-8.370731 0 0-2.834271 40.719949-0.313696 65.463413-5.046654-19.273045-8.827517-19.273045-8.827517-71.275045-30.235897-10.550093-66.173356-30.549593-66.173357-30.549593z" fill="#E71F19" p-id="12420"></path><path d="M506.316313 242.426509c0 50.60963-22.487054 91.637771-50.218886 91.637771s-50.218885-41.028141-50.218885-91.637771 22.487054-91.637771 50.218885-91.63777c27.731832-0.005503 50.218885 41.022638 50.218886 91.63777z m147.492178 0c0 50.60963-22.498061 91.637771-50.218885 91.637771-27.731832 0-50.218885-41.028141-50.218886-91.637771s22.487054-91.637771 50.218886-91.63777c27.726328-0.005503 50.218885 41.022638 50.218885 91.63777z" fill="#FFFFFF" p-id="12421"></path><path d="M499.530572 243.395115c0 19.553721-8.469793 35.398123-18.915322 35.398123-10.440025 0-18.915322-15.844402-18.915321-35.398123 0-19.548217 8.469793-35.398123 18.915321-35.398123 10.445528 0.005503 18.915322 15.855409 18.915322 35.398123z m74.791742 10.54459c-1.794121-18.238398 8.662414-44.368729 29.151719-37.638023 14.86479 6.730706 14.028267 32.547341 13.703565 39.096434-0.324703 6.538086-8.97611 11.089431-12.922076 0-1.893183-10.36848-10.093308-41.63352-17.176235-2.371982-2.360976 9.091682-11.969981 8.915572-12.756973 0.913571z" fill="" p-id="12422"></path><path d="M676.779847 358.472035c-37.032644-15.618761-89.452905-25.381863-147.624261-25.381863-56.872544 0-108.236144 9.32833-145.109189 24.347216-30.57711 12.44878-57.648529 34.567103-56.597371 47.02689 3.153471 12.503815 13.13671 20.626891 68.176608 29.118699-19.53721-18.915322-15.21701-14.600625-15.21701-43.5212 0 28.920575 23.48868 43.669792 61.357847 59.42614 24.853533 10.340963 55.386615 16.455284 88.418259 16.455284 34.677172 0 66.602625-6.73621 92.072542-18.023764 35.805377-15.855409 53.108191-28.661913 53.108191-56.6414 0 27.979486 5.773108 19.030894-17.242276 42.310443 53.366853-9.383364 70.17986-19.757348 71.5227-32.525327 0.022014-11.238023-25.55247-31.072419-52.86604-42.591118z" fill="#F0971C" p-id="12423"></path></svg></a><a class="social" href="https://www.kaggle.com/humbleyll" target="_blank" rel="external nofollow noopener noreferrer"><svg t="1716701307894" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="13418" width="200" height="200"><path d="M803.1909 1017.952189c-0.927971 3.935877-4.991844 6.015812-11.999625 6.015812H657.27546c-7.967751 0-14.975532-3.487891-20.991344-10.591669l-220.921096-281.111215-61.790069 58.622168v218.073185c0 10.015687-4.991844 15.007531-14.975532 15.007531H234.88866c-10.079685 0-15.103528-4.991844-15.103528-15.007531V15.071529C219.785132 5.11984 224.808975 0 234.88866 0h103.708759c9.983688 0 14.975532 5.11984 14.975532 15.071529v611.948877l264.663729-267.607638c7.03978-7.03978 14.07956-10.495672 21.11934-10.495672h138.203681c6.143808 0 10.079685 2.55992 12.15962 7.67976 1.951939 6.367801 1.439955 10.87966-1.535952 13.43958l-279.67126 270.679542 291.670885 362.964657c4.063873 4.447861 4.991844 8.863723 2.975907 15.263523z" fill="#20BEFF" p-id="13419"></path></svg></a><a class="social" href="https://" target="_blank" rel="external nofollow noopener noreferrer"><svg t="1716701358358" class="icon" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="14411" width="200" height="200"><path d="M1024.16 694.816c0-149.92-143.104-271.392-319.584-271.392-176.576 0-319.68 121.504-319.68 271.392S528 966.208 704.576 966.208c55.456 0 107.648-12.096 153.184-33.248l125.984 54.528-14.592-140.544c34.784-43.392 55.04-95.808 55.04-152.128zM596.832 621.28c-25.152 0-45.472-20.352-45.472-45.472s20.32-45.472 45.472-45.472c25.12 0 45.44 20.384 45.44 45.472s-20.384 45.472-45.44 45.472z m215.392 0c-25.056 0-45.44-20.352-45.44-45.472s20.384-45.472 45.44-45.472c25.184 0 45.536 20.384 45.536 45.472s-20.352 45.472-45.536 45.472zM704.576 387.488c49.376 0 96.416 8.8 139.264 24.64 0.32-5.728 0.992-11.232 0.992-16.992 0-198.08-189.152-358.624-422.432-358.624C189.184 36.512 0.032 197.024 0.032 395.136c0 74.496 26.816 143.776 72.704 201.12L53.472 781.92l166.432-72.096c41.216 19.2 86.784 32.16 134.88 38.784-3.616-17.504-5.824-35.424-5.824-53.792 0.032-169.44 159.552-307.296 355.616-307.296z m-139.808-209.6c33.184 0 60 26.88 60 60 0 33.184-26.816 60.064-60 60.064s-60.032-26.88-60.032-60.064c0-33.152 26.88-60 60.032-60zM280.032 297.952c-33.184 0-60-26.88-60-60.064 0-33.152 26.848-60 60-60 33.184 0 60.032 26.88 60.032 60s-26.88 60.064-60.032 60.064z" fill="#51C332" p-id="14412"></path></svg></a></div></footer>
</div></aside><div class="l_main" id="main">





<div class="article banner top">
  <div class="content">
    <div class="top bread-nav footnote"><div class="left"><div class="flex-row" id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a>
<span class="sep"></span><a class="cap breadcrumb" id="menu" href="/topic">专栏</a><span class="sep"></span><a class="cap breadcrumb" id="proj" href="/2024/04/10/tui-jian-xi-tong/">搜广推</a></div>
<div class="flex-row" id="post-meta"><span class="text created">发布于：<time datetime="2024-04-10T04:09:32.000Z">2024-04-10</time></span><span class="sep updated"></span><span class="text updated">更新于：<time datetime="2024-05-26T09:25:07.225Z">2024-05-26</time></span></div></div></div>
    
    <div class="bottom only-title">
      
      <div class="text-area">
        <h1 class="text title"><span>推荐系统</span></h1>
        
      </div>
    </div>
    
  </div>
  </div><article class="md-text content"><h1 id="第一章-推荐系统概述"><a href="#第一章-推荐系统概述" class="headerlink" title="第一章 推荐系统概述"></a>第一章 推荐系统概述</h1><h2 id="1-1推荐系统的意义"><a href="#1-1推荐系统的意义" class="headerlink" title="1.1推荐系统的意义"></a>1.1推荐系统的意义</h2><p>推荐系统就是一个将信息生产者和信息消费者连接起来的桥梁。平台往往会作为推荐系统的载体，实现信息生产者和消费者之间信息的匹配。上述提到的平台方、信息生产者和消费者可以分别用平台方（如：腾讯视频、淘宝、网易云音乐等）、物品（如：视频、商品、音乐等）和用户和来指代。下面分别从这三方需求出发，介绍推荐系统的存在的意义。</p>
<h3 id="平台方"><a href="#平台方" class="headerlink" title="平台方"></a>平台方</h3><p>平台方一般是为信息生产者提供物品展示的位置，然后通过不同的方式吸引用户来到平台上寻找他们感兴趣的物品。平台通过商家对物品的展示以及用户的浏览、观看或下单等行为，就产生了所谓的”流量”。</p>
<p>对平台方而言，流量的高效利用是推荐系统存在的重要原因。以典型的电商网站一般具有如图所示的树状拓扑结构，树状结构在连通性方面有着天然的劣势，阻碍这流量的高效流通。推荐系统的出现使得原本的树状结构变成网络拓扑结构，大大增强了整个网络的连通性。推荐模块不仅使用户在当前页面有了更好的选择路径，同时也给了每个商品增加入口和展示机会，进而提高了成交概率。而推荐质量的好坏，直接决定了用户选择这条路径的可能性，进而影响着流量的利用效率。</p>
<h3 id="推荐和搜索的区别"><a href="#推荐和搜索的区别" class="headerlink" title="推荐和搜索的区别"></a>推荐和搜索的区别</h3><p>搜索和推荐都是解决互联网大数据时代信息过载的手段，但是它们也存在着许多的不同：</p>
<ol>
<li><strong>用户意图</strong>：搜索时的用户意图是非常明确的，用户通过查询的关键词主动发起搜索请求。对于推荐而言，用户的需求是不明确的，推荐系统在通过对用户历史兴趣的分析给用户推荐他们可能感兴趣的内容。</li>
<li><strong>个性化程度</strong>：对于搜索而言，由于限定的了搜索词，所以展示的内容对于用户来说是有标准答案的，所以搜索的个性化程度较低。而对于推荐来说，推荐的内容本身就是没有标准答案的，每个人都有不同的兴趣，所以每个人展示的内容，个性化程度比较强。</li>
<li><strong>优化目标</strong>：对于搜索系统而言，更希望可以快速地、准确地定位到标准答案，所以希望搜索结果中答案越靠前越好，通常评价指标有：归一化折损累计收益（NDCG）、精确率（Precision）和召回率（Recall）。对于推荐系统而言，因为没有标准的答案，所以优化目标可能会更宽泛。例如用户停留时长、点击、多样性，评分等。不同的优化目标又可以拆解成具体的不同的评价指标。</li>
<li><strong>马太效应和长尾理论</strong>：对于搜索系统来说，用户的点击基本都集中在排列靠前的内容上，对于排列靠后的很少会被关注，这就是马太效应。而对于推荐系统来说，热门物品被用户关注更多，冷门物品不怎么被关注的现象也是存在的，所以也存在马太效应。此外，在推荐系统中，冷门物品的数量远远高于热门物品的数量，所以物品的长尾性非常明显。</li>
</ol>
<h2 id="1-2推荐系统的架构"><a href="#1-2推荐系统的架构" class="headerlink" title="1.2推荐系统的架构"></a>1.2推荐系统的架构</h2><p>推荐和搜索系统核心的的任务是从海量物品中找到用户感兴趣的内容。在这个背景下，推荐系统包含的模块非常多，每个模块将会有很多专业研究的工程和研究工程师，作为刚入门的应届生或者实习生很难对每个模块都有很深的理解，实际上也大可不必，我们完全可以从学习好一个模块技术后，以点带面学习整个系统，虽然正式工作中我们放入门每个人将只会负责的也是整个系统的一部分。但是掌握推荐系统最重要的还是梳理清楚整个推荐系统的架构，知道每一个部分需要完成哪些任务，是如何做的，主要的技术栈是什么，有哪些局限和可以研究的问题，能够对我们学习推荐系统有一个提纲挈领的作用。</p>
<p>所以这篇文章将会从<strong>系统架构</strong>和<strong>算法架构</strong>两个角度出发解析推荐系统通用架构。系统架构设计思想是大数据背景下如何有效利用海量和实时数据，将推荐系统按照对数据利用情况和系统响应要求出发，将整个架构分为<strong>离线层、近线层、在线层</strong>三个模块。然后分析这三个模块分别承担推荐系统什么任务，有什么制约要求。这种角度不和召回、排序这种通俗我们理解算法架构，因为更多的是考虑推荐算法在工程技术实现上的问题，系统架构是如何权衡利弊，如何利用各种技术工具帮助我们达到想要的目的的，方便我们理解为什么推荐系统要这样设计。</p>
<p>而算法架构是从我们比较熟悉的<strong>召回、粗排、排序、重排</strong>等算法环节角度出发的，重要的是要去理解每个环节需要完成的任务，每个环节的评价体系，以及为什么要那么设计。还有一个重要问题是每个环节涉及到的技术栈和主流算法，这部分非常重要而且篇幅较大，所以我们放在下一个章节讲述。</p>
<p>架构设计是一个非常大的话题，设计的核心在于平衡和妥协。在推荐系统不同时期、不同的环境、不同的数据，架构都会面临不一样的问题。网飞官方博客有一段总结：</p>
<blockquote>
<p>We want the ability to use sophisticated machine learning algorithms that can grow to arbitrary complexity and can deal with huge amounts of data. We also want an architecture that allows for flexible and agile innovation where new approaches can be developed and plugged-in easily. Plus, we want our recommendation results to be fresh and respond quickly to new data and user actions. Finding the sweet spot between these desires is not trivial: it requires a thoughtful analysis of requirements, careful selection of technologies, and a strategic decomposition of recommendation algorithms to achieve the best outcomes for our members. <strong>“我们需要具备使用复杂机器学习算法的能力，这些算法要可以适应高度复杂性，可以处理大量数据。我们还要能够提供灵活、敏捷创新的架构，新的方法可以很容易在其基础上开发和插入。而且，我们需要我们的推荐结果足够新，能快速响应新的数据和用户行为。找到这些要求之间恰当的平衡并不容易，需要深思熟虑的需求分析，细心的技术选择，战略性的推荐算法分解，最终才能为客户达成最佳的结果。”</strong></p>
</blockquote>
<p>所以在思考推荐系统架构考虑的第一个问题是确定边界：知道推荐系统要负责哪部分问题，这就是边界内的部分。在这个基础上，架构要分为哪几个部分，每一部分需要完成的子功能是什么，每一部分依赖外界的什么。了解推荐系统架构也和上文讲到的思路一样，我们需要知道的是推荐系统要负责的是怎么问题，每一个子模块分别承担了哪些功能，它们的主流技术栈是什么。从这个角度来阅读本文，将会有助于读者抓住重点。</p>
<h3 id="系统架构"><a href="#系统架构" class="headerlink" title="系统架构"></a>系统架构</h3><p>推荐系统架构，首先从数据驱动角度，对于数据，最简单的方法是存下来，留作后续离线处理，<strong>离线层</strong>就是我们用来管理离线作业的部分架构。<strong>在线层</strong>能更快地响应最近的事件和用户交互，但必须实时完成。这会限制使用算法的复杂性和处理的数据量。离线计算对于数据数量和算法复杂度限制更少，因为它以批量方式完成，没有很强的时间要求。不过，由于没有及时加入最新的数据，所以很容易过时。</p>
<p>个性化架构的关键问题，就是如何以无缝方式结合、管理在线和离线计算过程。<strong>近线层</strong>介于两种方法之间，可以执行类似于在线计算的方法，但又不必以实时方式完成。这种设计思想最经典的就是Netflix在2013年提出的架构，整个架构分为</p>
<ol>
<li>离线层：不用实时数据，不提供实时响应；</li>
<li>近线层：使用实时数据，不保证实时响应；</li>
<li>在线层：使用实时数据，保证实时在线服务；</li>
</ol>
<h3 id="设计思想"><a href="#设计思想" class="headerlink" title="设计思想"></a>设计思想</h3><p>网飞的这个架构提出的非常早，其中的技术也许不是现在常用的技术了，但是架构模型仍然被很多公司采用。</p>
<p>这个架构为什么要这么设计，本质上是因为推荐系统是由大量数据驱动的，大数据框架最经典的就是lambda架构和kappa架构。而推荐系统在不同环节所使用的数据、处理数据的量级、需要的读取速度都是不同的，目前的技术还是很难实现一套端到端的及时响应系统，所以这种架构的设计本质上还是一种权衡后的产物，所以有了下图这种模型：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/11/MlPqAFQacgiJpwL.png" alt="image-20240411224023001"></p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/11/1BOuDUGCretljZF.png" alt="image-20240411224115381"></p>
<p>整个数据部分其实是一整个链路，主要是三块，分别是客户端及服务器实时数据处理、流处理平台准实时数据处理和大数据平台离线数据处理这三个部分。</p>
<p>看到这里，一个很直观的问题就是，为什么数据处理需要这么多步骤？这些步骤都是干嘛的，存在的意义是什么？</p>
<p>我们一个一个来说，首先是客户端和服务端的实时数据处理。这个很好理解，这个步骤的工作就是记录。将用户在平台上真实的行为记录下来，比如用户看到了哪些内容，和哪些内容发生了交互，和哪些没有发生了交互。如果再精细一点，还会记录用户停留的时间，用户使用的设备等等。除此之外还会记录行为发生的时间，行为发生的session等其他上下文信息。</p>
<p>这一部分主要是后端和客户端完成，行业术语叫做埋点。所谓的埋点其实就是记录点，因为数据这种东西需要工程师去主动记录，不记录就没有数据，记录了才有数据。既然我们要做推荐系统，要分析用户行为，还要训练模型，显然需要数据。需要数据，就需要记录。</p>
<p>第二个步骤是流处理平台准实时数据处理，这一步是干嘛的呢，其实也是记录数据，不过是记录一些准实时的数据。很多同学又会迷糊了，实时我理解，就是当下立即的意思，准实时是什么意思呢？准实时的意思也是实时，只不过没有那么即时，比如可能存在几分钟的误差。这样存在误差的即时数据，行业术语叫做准实时。那什么样的准实时数据需要记录呢？在推荐领域基本上只有一个类别，就是用户行为数据。也就是用户在观看这个内容之前还看过哪些内容，和哪些内容发生过交互。理想情况这部分数据也需要做成实时，但由于这部分数据量比较大，并且逻辑也相对复杂，所以很难做到非常实时，一般都是通过消息队列加在线缓存的方式做成准实时。</p>
<p>最后我们看第三个步骤，叫做离线数据处理，离线也就是线下处理，基本上就没有时限的要求了。</p>
<p>一般来说，离线处理才是数据处理的大头。所有“脏活累活”复杂的操作都是在离线完成的，比如说一些join操作。后端只是记录了用户交互的商品id，我们需要商品的详细信息怎么办？需要去和商品表关联查表。显然数据关联是一个非常耗时的操作，所以只能放到离线来做。</p>
<p>接下来详细介绍一下这三个模块。</p>
<h3 id="离线层"><a href="#离线层" class="headerlink" title="离线层"></a>离线层</h3><p>离线层是计算量最大的一个部分，它的特点是不依赖实时数据，也不需要实时提供服务。需要实现的主要功能模块是：</p>
<ol>
<li>数据处理、数据存储；</li>
<li>特征工程、离线特征计算；</li>
<li>离线模型的训练；</li>
</ol>
<p>这里我们可以看出离线层的任务是最接近学校中我们处理数据、训练模型这种任务的，不同可能就是需要面临更大规模的数据。离线任务一般会按照天或者更久运行，比如每天晚上定期更新这一天的数据，然后重新训练模型，第二天上线新模型。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/11/PNs4UCxmSDdvIuH.png" alt="image-20240411225231577"></p>
<h3 id="离线层优势和不足"><a href="#离线层优势和不足" class="headerlink" title="离线层优势和不足"></a>离线层优势和不足</h3><p>离线层面临的数据量级是最大的，面临主要的问题是海量数据存储、大规模特征工程、多机分布式机器学习模型训练。目前主流的做法是HDFS，收集到我们所有的业务数据，通过HIVE等工具，从全量数据中抽取出我们需要的数据，进行相应的加工，离线阶段主流使用的分布式框架一般是Spark。所以离线层有如下的优势：</p>
<ol>
<li>可以处理大量的数据，进行大规模特征工程；</li>
<li>可以进行批量处理和计算；</li>
<li>不用有响应时间要求；</li>
</ol>
<p>但是同样的，如果我们只使用用户离线数据，最大的不足就是无法反应用户的实时兴趣变化，这就促使了近线层的产生。</p>
<h3 id="近线层"><a href="#近线层" class="headerlink" title="近线层"></a>近线层</h3><p>近线层的主要特点是准实时，它可以获得实时数据，然后快速计算提供服务，但是并不要求它和在线层一样达到几十毫秒这种延时要求。近线层的产生是同时想要弥补离线层和在线层的不足，折中的产物。</p>
<p>它适合处理一些对延时比较敏感的任务，比如：</p>
<ol>
<li>特征的事实更新计算：例如统计用户对不同type的ctr，推荐系统一个老生常谈的问题就是特征分布不一致怎么办，如果使用离线算好的特征就容易出现这个问题。近线层能够获取实时数据，按照用户的实时兴趣计算就能很好避免这个问题。</li>
<li>实时训练数据的获取：比如在使用DIN、DSIN这行网络会依赖于用户的实时兴趣变化，用户几分钟前的点击就可以通过近线层获取特征输入模型。</li>
<li>模型实时训练：可以通过在线学习的方法更新模型，实时推送到线上；</li>
</ol>
<p>近线层的发展得益于最近几年大数据技术的发展，很多流处理框架的提出大大促进了近线层的进步。如今Flink、Storm等工具一统天下。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/12/cubdwvZhHnsAx87.png" alt="image-20240412182725567"></p>
<h3 id="在线层"><a href="#在线层" class="headerlink" title="在线层"></a>在线层</h3><p>在线层，就是直接面向用户的的那一层了。最大的特点是对响应延时有要求，因为它是直接面对用户群体的，你可以想象你打开抖音淘宝等界面，几乎都是秒刷出来给你的推荐结果的，不会说还需要让你等待几秒钟时间。所有的用户请求都会发送到在线层，在线层需要快速返回结果，它主要承担的工作有：</p>
<ol>
<li>模型在线服务；包括了快速召回和排序；</li>
<li>在线特征快速处理拼接：：根据传入的用户ID和场景，快速读取特征和处理；</li>
<li>AB实验或者分流：根据不同用户采用不一样的模型，比如冷启动用户和正常服务模型；</li>
<li>运筹优化和业务干预：比如要对特殊商家流量扶持、对某些内容限流；</li>
</ol>
<p>典型的在线服务是用过RESTful/RPC等提供服务，一般是公司后台服务部门调用我们的这个服务，返回给前端。具体部署应用比较多的方式就是使用Docker在K8S部署。而在线服务的数据源就是我们在离线层计算好的每个用户和商品特征，我们事先存放在数据库中，在线层只需要实时拼接，不进行复杂的特征运算，然后输入近线层或者离线层已经训练好的模型，根据推理结果进行排序，最后返回给后台服务器，后台服务器根据我们对每一个用户的打分，再返回给用户。</p>
<p>在线层最大的问题就是对实时性要求特别高，一般来说是几十毫秒，这就限制了我们能做的工作，很多任务往往无法及时完成，需要近线层协助我们做。</p>
<h3 id="算法架构"><a href="#算法架构" class="headerlink" title="算法架构"></a>算法架构</h3><p>我们在入门学习推荐系统的时候，更加关注的是哪个模型AUC更高、topK效果好，哪个模型更加牛逼的问题，从基本的协同过滤到点击率预估算法，从深度学习到强化学习，学术界都始终走在最前列。一个推荐算法从出现到在业界得到广泛应用是一个长期的过程，因为在实际的生产系统中，首先需要保证的是稳定、实时地向用户提供推荐服务，在这个前提下才能追求推荐系统的效果。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/13/otixzas2RVNmWl8.png" alt="image-20240413222420120"></p>
<ul>
<li>召回</li>
</ul>
<p>召回层的主要目标时从推荐池中选取几千上万的item，送给后续的排序模块。由于召回面对的候选集十分大，且一般需要在线输出，故召回模块必须<strong>轻量快速低延迟</strong>。由于后续还有排序模块作为保障，<strong>召回不需要十分准确，但不可遗漏</strong>（特别是搜索系统中的召回模块）。</p>
<p>如果没有召回层，每个User都能和每一个Item去在线排序阶段预测目标概率，理论上来说是效果最好，但是是不现实的，<strong>线上不延迟允许</strong>，所以召回和粗排阶段就要做一些<strong>候选集筛选</strong>的工作，保证在有限的能够给到排序层去做精排的候选集的时间里，效果最大化。另一个方面就是通过这种<strong>模型级联</strong>的方式，可以<strong>减少用排序阶段拟合多目标的压力</strong>，比如召回阶段我们现在主要是在<strong>保证Item质量的基础上注重覆盖率多样性</strong>，<strong>粗排阶段主要用简单的模型来解决不同路的召回和当前用户的相关性问题</strong>，最后截断到1k个以内的候选集，这个<strong>候选集符合一定的个性化相关性、视频质量和多样性的保证</strong>，然后做ranking去做复杂模型的predict。</p>
<p>目前基本上<strong>采用多路召回解决范式，分为非个性化召回和个性化召回</strong>。个性化召回又有content-based、behavior-based、feature-based等多种方式。</p>
<p>召回主要考虑的内容有：</p>
<ol>
<li><strong>考虑用户层面</strong>：用户兴趣的多元化，用户需求与场景的多元化：例如：新闻需求，重大要闻，相关内容沉浸阅读等等</li>
<li><strong>考虑系统层面</strong>：增强系统的鲁棒性；部分召回失效，其余召回队列兜底不会导致整个召回层失效；排序层失效，召回队列兜底不会导致整个推荐系统失效</li>
<li><strong>系统多样性内容分发</strong>：图文、视频、小视频；精准、试探、时效一定比例；召回目标的多元化，例如：相关性，沉浸时长，时效性，特色内容等等</li>
<li><strong>可解释性推荐一部分召回是有明确推荐理由的</strong>：很好的解决产品性数据的引入；</li>
</ol>
<ul>
<li>粗排</li>
</ul>
<p>粗排的原因是有时候召回的结果还是太多，精排层速度还是跟不上，所以加入粗排。<strong>粗排可以理解为精排前的一轮过滤机制，减轻精排模块的压力。粗排介于召回和精排之间，要同时兼顾精准性和低延迟</strong>。目前粗排一般也都模型化了，其训练样本类似于精排，选取曝光点击为正样本，曝光未点击为负样本。但由于粗排一般面向上万的候选集，而精排只有几百上千，其解空间大很多。</p>
<p>粗排阶段的架构设计主要是考虑三个方面，一个是<strong>根据精排模型中的重要特征，来做候选集的截断</strong>，另一部分是<strong>有一些召回设计，比如热度或者语义相关的这些结果，仅考虑了item侧的特征，可以用粗排模型来排序跟当前User之间的相关性，据此来做截断</strong>，这样是比单独的按照item侧的倒排分数截断得到更加个性化的结果，最后是算法的选型要在在线服务的性能上有保证，因为这个阶段在pipeline中完成从召回到精排的截断工作，在延迟允许的范围内能处理更多的召回候选集理论上与精排效果正相关。</p>
<ul>
<li>精排</li>
</ul>
<p>精排层，也是我们学习推荐入门最常常接触的层，我们所熟悉的算法很大一部分都来自精排层。这一层的任务是获取粗排模块的结果，对候选集进行打分和排序。精排需要在最大时延允许的情况下，保证打分的精准性，是整个系统中至关重要的一个模块，也是最复杂，研究最多的一个模块。</p>
<p>精排是推荐系统各层级中最纯粹的一层，他的目标比较单一且集中，一门心思的实现目标的调优即可。最开始的时候精排模型的常见目标是ctr,后续逐渐发展了cvr等多类目标。精排和粗排层的基本目标是一致的，都是对商品集合进行排序，但是和粗排不同的是，精排只需要对少量的商品(即粗排输出的商品集合的topN)进行排序即可。因此，精排中可以使用比粗排更多的特征，更复杂的模型和更精细的策略（用户的特征和行为在该层的大量使用和参与也是基于这个原因）。</p>
<p>精排层模型是推荐系统中涵盖的研究方向最多，有非常多的子领域值得研究探索，这也是推荐系统中技术含量最高的部分，毕竟它是直接面对用户，产生的结果对用户影响最大的一层。目前精排层深度学习已经一统天下了，精排阶段采用的方案相对通用，首先一天的样本量是几十亿的级别，我们要解决的是样本规模的问题，尽量多的喂给模型去记忆，另一个方面时效性上，用户的反馈产生的时候，怎么尽快的把新的反馈给到模型里去，学到最新的知识。</p>
<ul>
<li>重排</li>
</ul>
<p>常见的有三种优化目标：Point Wise、Pair Wise 和 List Wise。重排序阶段对精排生成的Top-N个物品的序列进行重新排序，生成一个Top-K个物品的序列，作为排序系统最后的结果，直接展现给用户。重排序的原因是因为多个物品之间往往是相互影响的，而精排序是根据PointWise得分，容易造成推荐结果同质化严重，有很多冗余信息。而重排序面对的挑战就是海量状态空间如何求解的问题，一般在精排层我们使用AUC作为指标，但是在重排序更多关注NDCG等指标。</p>
<p>重排序在业务中，获取精排的排序结果，还会根据一些策略、运营规则参与排序，比如强制去重、间隔排序、流量扶持等、运营策略、多样性、context上下文等，重新进行一个微调。重排序更多的是List Wise作为优化目标的，它关注的是列表中商品顺序的问题来优化模型，但是一般List Wise因为状态空间大，存在训练速度慢的问题。</p>
<p>由于精排模型一般比较复杂，基于系统时延考虑，一般采用point-wise方式，并行对每个item进行打分。这就使得打分时缺少了上下文感知能力。用户最终是否会点击购买一个商品，除了和它自身有关外，和它周围其他的item也息息相关。重排一般比较轻量，可以加入上下文感知能力，提升推荐整体算法效率。比如三八节对美妆类目商品提权，类目打散、同图打散、同卖家打散等保证用户体验措施。重排中规则比较多，但目前也有不少基于模型来提升重排效果的方案。</p>
<ul>
<li>混排</li>
</ul>
<p>多个业务线都想在Feeds流中获取曝光，则需要对它们的结果进行混排。比如推荐流中插入广告、视频流中插入图文和banner等。可以基于规则策略（如广告定坑）和强化学习来实现。</p>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>整篇文章从系统架构梳理了Netfliex的经典推荐系统架构，整个架构更多是偏向实时性能和效果之间trade off的结果。如果从另外的角度看推荐系统架构，比如从数据流向，或者说从推荐系统各个时序依赖来看，就是我们最熟悉的召回、粗排、精排、重排、混排等模块了。这种角度来看是把推荐系统从前往后串起来，其中每一个模块既有在离线层工作的，也有在在线层工作的。而从数据驱动角度看，更能够看到推荐系统的完整技术栈，推荐系统当前面临的局限和发展方向。</p>
<p>召回、排序这些里面单拿出任何一个模块都非常复杂。这也是为什么大家都说大厂拧螺丝的原因，因为很可能某个人只会负责其中很小的一个模块。许多人说起自己的模块来如数家珍，但对于全局缺乏认识，带来的结果是当你某天跳槽了或者是工作内容变化了，之前从工作当中的学习积累很难沉淀下来，这对于程序员的成长来说是很不利的。</p>
<p>所以希望这篇文章能够帮助大家在负责某一个模块，优化某一个功能的时候，除了能够有算法和数据，还能能够考虑对整个架构带来的影响，如何提升整体的一个性能，慢慢开阔自己的眼界，构建出一个更好的推荐系统。</p>
<h2 id="1-3推荐系统的技术栈"><a href="#1-3推荐系统的技术栈" class="headerlink" title="1.3推荐系统的技术栈"></a>1.3推荐系统的技术栈</h2><p>推荐系统是一个非常大的框架，有非常多的模块在里面，完整的一套推荐系统体系里，不仅会涉及到推荐算法工程师、后台开发工程师、数据挖掘/分析工程师、NLP/CV工程师还有前端、客户端甚至产品、运营等支持。我们作为算法工程师，需要掌握的技术栈主要就是在算法和工程两个区域了，所以这篇文章将会分别从算法和工程两个角度出发，结合两者分析当前主流的一些推荐算法技术栈。</p>
<h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>首先我们从推荐系统架构出发，一种分法是将整个推荐系统架构分为召回、粗排、精排、重排、混排等模块。它的分解方法是从一份数据如何从生产出来，到线上服务完整顺序的一个流程。因为在不同环节，我们一般会考虑不同的算法，所以这种角度出发我们来研究推荐系统主流的算法技术栈。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240417131451288.png" alt="image-20240417131451288"></p>
<p>为了帮助新手在后文方便理解，首先简单介绍这些模块的功能主要是：</p>
<ul>
<li>召回：从推荐池中选取几千上万的item，送给后续的排序模块。由于召回面对的候选集十分大，且一般需要在线输出，故召回模块必须轻量快速低延迟。由于后续还有排序模块作为保障，召回不需要十分准确，但不可遗漏（特别是搜索系统中的召回模块）。目前基本上采用多路召回解决范式，分为非个性化召回和个性化召回。个性化召回又有content-based、behavior-based、feature-based等多种方式。</li>
<li>粗排：粗拍的原因是有时候召回的结果还是太多，精排层速度还是跟不上，所以加入粗排。粗排可以理解为精排前的一轮过滤机制，减轻精排模块的压力。粗排介于召回和精排之间，要同时兼顾精准性和低延迟。一般模型也不能过于复杂</li>
<li>精排：获取粗排模块的结果，对候选集进行打分和排序。精排需要在最大时延允许的情况下，保证打分的精准性，是整个系统中至关重要的一个模块，也是最复杂，研究最多的一个模块。精排系统构建一般需要涉及样本、特征、模型三部分。</li>
<li>重排：获取精排的排序结果，基于运营策略、多样性、context上下文等，重新进行一个微调。比如三八节对美妆类目商品提权，类目打散、同图打散、同卖家打散等保证用户体验措施。重排中规则比较多，但目前也有不少基于模型来提升重排效果的方案。</li>
<li>混排：多个业务线都想在Feeds流中获取曝光，则需要对它们的结果进行混排。比如推荐流中插入广告、视频流中插入图文和banner等。可以基于规则策略（如广告定坑）和强化学习来实现。</li>
</ul>
<h3 id="画像层"><a href="#画像层" class="headerlink" title="画像层"></a>画像层</h3><p>首先是推荐系统的物料库，这部分内容里，算法主要体现在如何绘制一个用户画像和商品画像。这个环节是推荐系统架构的基础设施，一般可能新用户/商品进来，或者每周定期会重新一次整个物料库，计算其中信息，为用户打上标签，计算统计信息，为商品做内容理解等内容。其中用户画像是大家比较容易理解的，比如用户年龄、爱好通常APP会通过注册界面收集这些信息。而商品画像形式就非常多了，比如淘宝主要推荐商品，抖音主要是短视频，所以大家的物料形式比较多，内容、质量差异也比较大，所以内容画像各家的做法也不同，当前比较主流的都会涉及到一个多模态信息内容理解。下面我贴了一个微信看一看的内容画像框架，然后我们来介绍下在这一块主要使用的算法技术。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240417131728890.png" alt="image-20240417131728890"></p>
<p>一般推荐系统会加入多模态的一个内容理解。我们用短视频形式举个例子，假设用户拍摄了一条短视频，上传到了平台，从推荐角度看，首先我们有的信息是这条短视频的作者、长度、作者为它选择的标签、时间戳这些信息。但是这对于推荐来说是远远不够的，首先作者打上的标签不一定准确反映作品，原因可能是我们模型的语义空间可能和作者/现实世界不一致。其次我们需要更多维度的特征，比如有些用户喜欢看小姐姐跳舞，那我希望能够判断一条视频中是否有小姐姐，这就涉及到封面图的基于CV的内容抽取或者整个视频的抽取；再比如作品的标题一般能够反映主题信息，除了很多平台常用的用“#”加上一个标签以外，我们也希望能够通过标题抽取出基于NLP的信息。还有更多的维度可以考虑：封面图多维度的多媒体特征体系，包括人脸识别，人脸embedding，标签，一二级分类，视频embedding表示，水印，OCR识别，清晰度，低俗色情，敏感信息等多种维度。</p>
<p>这里面涉及的任务主要是CV的目标检测、语义分割等任务，NLP中的情感分析、摘要抽取、自然语言理解等任务。但是这部分算法一般团队都会有专门负责的组，不需要推荐算法工程师来负责，他们会有多模态的语意标签输出，主要形式是各种粒度的Embedding。我们只需要在我们的推荐模型中引入这些预训练的Embedding。</p>
<h3 id="文本理解"><a href="#文本理解" class="headerlink" title="文本理解"></a>文本理解</h3><p>这应该是用的最多的模态信息，包括item的标题、正文、OCR、评论等数据。这里面也可以产生不同粒度的信息，比如文本分类，把整个item做一个粗粒度的分类。</p>
<p>这里的典型算法有：RNN、TextCNN、FastText、Bert等；</p>
<h3 id="关键词标签"><a href="#关键词标签" class="headerlink" title="关键词标签"></a>关键词标签</h3><p>相比文本分类，关键词是更细粒度的信息，往往是一个mutil-hot的形式，它会对item在我们的标签库的选取最合适的关键词或者标签。</p>
<p>这里典型的算法有：TF-IDF、Bert、LSTM-CRF等。</p>
<h3 id="内容理解"><a href="#内容理解" class="headerlink" title="内容理解"></a>内容理解</h3><p>在很多场景下，推荐的主题都是视频或者图片，远远多于仅推荐文本的情况，这里视频/图片item中的内容中除了文本的内容以外，更多的信息其实来源于视频/图片内容本身, 因此需要尝试从多种模态中抽取更丰富的信息。主要包括分类信息、封面图OCR的信息、视频标签信息等</p>
<p>这里典型的算法有：TSN、RetinaFace、PSENet等。</p>
<h3 id="知识图谱"><a href="#知识图谱" class="headerlink" title="知识图谱"></a>知识图谱</h3><p>知识图谱作为知识承载系统，用于对接内外部关键词信息与词关系信息；内容画像会将原关系信息整合，并构建可业务应用的关系知识体系，其次，依赖业务中积累用户行为产生的实体关系数据，本身用户需求的标签信息，一并用于构建业务知识的兴趣图谱，基于同构网络与异构网络表示学习等核心模型，输出知识表示与表达，抽象后的图谱用于文本识别，推荐语义理解，兴趣拓展推理等场景，直接用于兴趣推理的冷启场景已经验证有很不错的收益。</p>
<p>这方面的算法有：KGAT、RippleNet等。</p>
<h3 id="召回-粗排"><a href="#召回-粗排" class="headerlink" title="召回/粗排"></a>召回/粗排</h3><p>推荐系统的召回阶段可以理解为根据用户的历史行为数据，为用户在海量的信息中粗选一批待推荐的内容，挑选出一个小的候选集的过程。粗排用到的很多技术与召回重合，所以放在一起讲，粗排也不是必需的环节，它的功能对召回的结果进行个粗略的排序，在保证一定精准的前提下，进一步减少往后传送的物品数量，这就是粗排的作用。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="http://ryluo.oss-cn-chengdu.aliyuncs.com/图片image-20220410000221817.png" alt="在这里插入图片描述"></p>
<p>召回模块面对几百上千万的推荐池物料规模，候选集十分庞大。由于后续有排序模块作为保障，故不需要十分准确，但必须保证不要遗漏和低延迟。目前主要通过多路召回来实现，一方面各路可以并行计算，另一方面取长补短。可以看到各类同类竞品的系统虽然细节上多少存在差异，但不约而同的采取了多路召回的架构，这类设计考虑如下几点问题：</p>
<ol>
<li><strong>考虑用户层面</strong>：用户兴趣的多元化，用户需求与场景的多元化：例如：新闻需求，重大要闻，相关内容沉浸阅读等等</li>
<li><strong>考虑系统层面</strong>：增强系统的鲁棒性；部分召回失效，其余召回队列兜底不会导致整个召回层失效；排序层失效，召回队列兜底不会导致整个推荐系统失效</li>
<li><strong>系统多样性内容分发</strong>：图文、视频、小视频；精准、试探、时效一定比例；召回目标的多元化，例如：相关性，沉浸时长，时效性，特色内容等等</li>
<li><strong>可解释性推荐一部分召回是有明确推荐理由的</strong>：很好的解决产品性数据的引入；</li>
</ol>
<p>介绍了召回任务的目的和场景后，接下来分析召回层面主要的技术栈，因为召回一般都是多路召回，从模型角度分析有很多召回算法，这种一般是在召回层占大部分比例点召回，除此之外，还会有探索类召回、策略运营类召回、社交类召回等。接下来我们着重介绍模型类召回。</p>
<h3 id="经典召回模型"><a href="#经典召回模型" class="headerlink" title="经典召回模型"></a>经典召回模型</h3><p>随着技术发展，在Embedding基础上的模型化召回是一个技术发展潮流方向。这种召回的范式是通过某种算法，对user和item分别打上Embedding，然后user与item在线进行KNN计算实时查询最近邻结果作为召回结果，快速找出匹配的物品。需要注意的是如果召回采用模型召回方法，优化目标最好和排序的优化目标一致，否则可能被过滤掉。</p>
<p>在这方面典型的算法有：FM、双塔DSSM、Multi-View DNN等。</p>
<h3 id="序列召回模型"><a href="#序列召回模型" class="headerlink" title="序列召回模型"></a>序列召回模型</h3><p>推荐系统主要解决的是基于用户的隐式阅读行为来做个性化推荐的问题，序列模型一些基于神经网络模型学习得到Word2Vec模型，再后面的基于RNN的语言模型，最先用的最多的Bert，这些方法都可以应用到召回的学习中。</p>
<p>用户在使用 APP 或者网站的时候，一般会产生一些针对物品的行为，比如点击一些感兴趣的物品，收藏或者互动行为，或者是购买商品等。而一般用户之所以会对物品发生行为，往往意味着这些物品是符合用户兴趣的，而不同类型的行为，可能代表了不同程度的兴趣。比如购买就是比点击更能表征用户兴趣的行为。在召回阶段，如何根据用户行为序列打 embedding，可以采取有监督的模型，比如 Next Item Prediction 的预测方式即可；也可以采用无监督的方式，比如物品只要能打出 embedding，就能无监督集成用户行为序列内容，例如 Sum Pooling。</p>
<p>这方面典型的算法有：CBOW、Skip-Gram、GRU、Bert等。</p>
<h3 id="用户序列拆分"><a href="#用户序列拆分" class="headerlink" title="用户序列拆分"></a>用户序列拆分</h3><p>上文讲了利用用户行为物品序列，打出用户兴趣 Embedding 的做法。但是，另外一个现实是：用户往往是多兴趣的，比如可能同时对娱乐、体育、收藏感兴趣。这些不同的兴趣也能从用户行为序列的物品构成上看出来，比如行为序列中大部分是娱乐类，一部分体育类，少部分收藏类等。那么能否把用户行为序列物品中，这种不同类型的用户兴趣细分，而不是都笼统地打到一个用户兴趣 Embedding 里呢？用户多兴趣拆分就是解决这类更细致刻画用户兴趣的方向。</p>
<p>本质上，把用户行为序列打到多个 embedding 上，实际它是个类似聚类的过程，就是把不同的 Item，聚类到不同的兴趣类别里去。目前常用的拆分用户兴趣 embedding 的方法，主要是胶囊网络和 Memory Network，但是理论上，很多类似聚类的方法应该都是有效的，所以完全可以在这块替换成你自己的能产生聚类效果的方法来做。</p>
<p>这方面典型的算法有：Multi-Interest Network with Dynamic Routing for Recommendation at Tmall等。</p>
<h3 id="知识图谱-1"><a href="#知识图谱-1" class="headerlink" title="知识图谱"></a>知识图谱</h3><p>知识图谱有一个独有的优势和价值，那就是对于推荐结果的可解释性；比如推荐给用户某个物品，可以在知识图谱里通过物品的关键关联路径给出合理解释，这对于推荐结果的解释性来说是很好的，因为知识图谱说到底是人编码出来让自己容易理解的一套知识体系，所以人非常容易理解其间的关系。知识图谱的可解释性往往是和图路径方法关联在一起的，而 Path 类方法，很多实验证明了，在排序角度来看，是效果最差的一类方法，但是它在可解释性方面有很好的效果，所以往往可以利用知识图谱构建一条可解释性的召回通路。</p>
<p>这方面的算法有：KGAT、RippleNet等。</p>
<h3 id="图模型"><a href="#图模型" class="headerlink" title="图模型"></a>图模型</h3><p>推荐系统中User和Item相关的行为、需求、属性和社交信息具有天然的图结构，可以使用一张复杂的异构图来表示整个推荐系统。图神经网络模型推荐就是基于这个想法，把异构网络中包含的结构和语义信息编码到结点Embedding表示中，并使用得到向量进行个性化推荐。知识图谱其实是图神经网络的一个比较特殊的具体实例，但是，知识图谱因为编码的是静态知识，而不是用户比较直接的行为数据，和具体应用距离比较远，这可能是导致两者在推荐领域表现有差异的主要原因。</p>
<p>这方面典型的算法有：GraphSAGE、PinSage等。</p>
<h3 id="精排"><a href="#精排" class="headerlink" title="精排"></a>精排</h3><p>排序模型是推荐系统中涵盖的研究方向最多，有非常多的子领域值得研究探索，这也是推荐系统中技术含量最高的部分，毕竟它是直接面对用户，产生的结果对用户影响最大的一层。目前精排层深度学习已经一统天下了，这是王喆老师《深度学习推荐算法》书中的精排层模型演化线路。具体来看分为DNN、Wide&amp;Deep两大块，实际深入还有序列建模，以及没有提到的多任务建模都是工业界非常常用的，所以我们接下来具体谈论其中每一块的技术栈。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/17/xZzb5hRlyasTJM9.png" alt="在这里插入图片描述"></p>
<h3 id="特征交叉模型"><a href="#特征交叉模型" class="headerlink" title="特征交叉模型"></a>特征交叉模型</h3><p>在深度学习推荐算法发展早期，很多论文聚焦于如何提升模型的特征组合和交叉的能力，这其中既包含隐式特征交叉Deep Crossing也有采用显式特征交叉的探究。本质上是希望模型能够摆脱人工先验的特征工程，实现端到端的一套模型。</p>
<p>在早期的推荐系统中，基本是由人工进行特征交叉的，往往凭借对业务的理解和经验，但是费时费力。于是有了很多的这方面的研究，从FM到GBDT+LR都是引入模型进行自动化的特征交叉。再往后就是深度模型，深度模型虽然有万能近似定理，但是真正想要发挥模型的潜力，显式的特征交叉还是必不可少的。</p>
<p>这方面的经典研究工作有：DCN、DeepFM、xDeepFM等；</p>
<h3 id="序列模型"><a href="#序列模型" class="headerlink" title="序列模型"></a>序列模型</h3><p>在推荐系统中，历史行为序列是非常重要的特征。在序列建模中，主要任务目标是得到用户此刻的兴趣向量（user interest vector）。如何刻画用户兴趣的广泛性，是推荐系统比较大的一个难点，用户历史行为序列建模的研究经历了从Pooling、RNN到attention、capsule再到transformer的顺序。在序列模型中，又有很多细分的方向，比如根据用户行为长度有研究用户终身行为序列的，也有聚焦当下兴趣的，还有研究如何抽取序列特征的抽取器，比如研究attention还是胶囊网络。</p>
<p>这方面典型的研究工作有：DIN、DSIN、DIEN、SIM等；</p>
<h3 id="多模态信息融合"><a href="#多模态信息融合" class="headerlink" title="多模态信息融合"></a>多模态信息融合</h3><p>在上文我们提到算法团队往往会利用内容画像信息，既有基于CV也有基于NLP抽取出来的信息。这是非常合理的，我们在逛抖音、淘宝的时候关注的不仅仅item的价格、品牌，同样会关注封面小姐姐好不好看、标题够不够震惊等信息。除此之外，在冷启动场景下，我们能够利用等信息不够多，如果能够使用多模态信息，能很大程度上解决数据稀疏的问题。</p>
<p>传统做法在多模态信息融合就是希望把不同模态信息利用起来，通过Embedding技术融合进模型。在推荐领域，主流的做法还是一套非端到端的体系，由其他模型抽取出多模态信息，推荐只需要融合入这些信息就好了。同时也有其他工作是利用注意力机制等方法来学习不同模态之间的关联，来增强多模态的表示。</p>
<p>比较典型的工作有：Image Matters: Visually modeling user behaviors using Advanced Model Server、UMPR等。</p>
<h3 id="多任务学习"><a href="#多任务学习" class="headerlink" title="多任务学习"></a>多任务学习</h3><p>很多场景下我们模型优化的目标都是CTR，有一些场景只考虑CTR是不够的，点击率模型、时长模型和完播率模型是大部分信息流产品推荐算法团队都会尝试去做的模型。单独优化点击率模型容易推出来标题党，单独优化时长模型可能推出来的都是长视频或长文章，单独优化完播率模型可能短视频短图文就容易被推出来，所以多目标就应运而生。信息流推荐中，我们不仅希望用户点进我们的item，还希望能有一个不错的完播率，即希望用户能看完我们推荐的商品。或者电商场景希望用户不仅点进来，还希望他买下或者加入购物车了。这些概率实际上就是模型要学习的目标，多种目标综合起来，包括阅读、点赞、收藏、分享等等一系列的行为，归纳到一个模型里面进行学习，这就是推荐系统的多目标学习。</p>
<p>这方面比较典型的算法有：ESSM、MMoE、DUPN等。</p>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>强化学习与一般有监督的深度学习相比有一些很显著的优势，首先强化学习能够比较灵活的定义优化的业务目标，考虑推荐系统长短期的收益，比如用户留存，在深度模型下，我们很难设计这个指标的优化函数，而强化学习是可以对长期收益下来建模。第二是能够体现用户兴趣的动态变化，比如在新闻推荐下，用户兴趣变化很快，强化学习更容易通过用户行为动态产生推荐结果。最后是EE也就是利用探索机制，这种一种当前和长期收益的权衡，强化学习能够更好的调节这里的回报。</p>
<p>这方面比较典型的算法有：DQN、Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology；</p>
<h3 id="跨域推荐"><a href="#跨域推荐" class="headerlink" title="跨域推荐"></a>跨域推荐</h3><p>一般一家公司业务线都是非常多的，比如腾讯既有腾讯视频，也有微信看一看、视频号，还有腾讯音乐，如果能够结合这几个场景的数据，同时进行推荐，一方面对于冷启动是非常有利的，另一方面也能补充更多数据，更好的进行精确推荐。</p>
<p>跨域推荐系统相比一般的推荐系统要更加复杂。在传统推荐系统中，我们只需要考虑建立当前领域内的一个推荐模型进行分析；而在跨域推荐中，我们更要关心在不同领域间要选择何种信息进行迁移，以及如何迁移这些信息，这是跨域推荐系统中非常关键的问题。</p>
<p>这方面典型的模型有：DTCDR、MV-DNN、EMCDR等；</p>
<h3 id="重排序"><a href="#重排序" class="headerlink" title="重排序"></a>重排序</h3><p>我们知道常见的有三种优化目标：Point Wise、Pair Wise 和 List Wise。重排序阶段对精排生成的Top-N个物品的序列进行重新排序，生成一个Top-K个物品的序列，作为排序系统最后的结果，直接展现给用户。重排序的原因是因为多个物品之间往往是相互影响的，而精排序是根据PointWise得分，容易造成推荐结果同质化严重，有很多冗余信息。而重排序面对的挑战就是海量状态空间如何求解的问题，一般在精排层我们使用AUC作为指标，但是在重排序更多关注NDCG等指标。</p>
<p>重排序在业务中，还会根据一些策略、运营规则参与排序，比如强制去重、间隔排序、流量扶持等，但是总计趋势上看还是算法排序越来越占据主流趋势。重排序更多的是List Wise作为优化目标的，它关注的是列表中商品顺序的问题来优化模型，但是一般List Wise因为状态空间大，存在训练速度慢的问题。这方面典型的做法，基于RNN、Transformer、强化学习的都有，这方面因为不是推荐的一个核心，所以没有展开来讲，而且这一块比较依赖实际的业务场景。</p>
<p>这里的经典算法有：MRR、DPP、RNN等；</p>
<h3 id="工程"><a href="#工程" class="headerlink" title="工程"></a>工程</h3><p>推荐系统的实现需要依托工程，很多研究界Paper的idea满天飞，却忽视了工业界能否落地，进入工业界我们很难或者很少有组是做纯research的，所以我们同样有很多工程技术需要掌握。下面列举了在推荐中主要用到的工程技术：</p>
<ul>
<li><strong>编程语言</strong>：Python、Java（scala）、C++、sql、shell；</li>
<li><strong>机器学习</strong>：Tensorflow/Pytorch、GraphLab/GraphCHI、LGB/Xgboost、SKLearn；</li>
<li><strong>数据分析</strong>：Pandas、Numpy、Seaborn、Spark；</li>
<li>数据存储：mysql、redis、mangodb、hive、kafka、es、hbase；</li>
<li>相似计算：annoy、faiss、kgraph</li>
<li>流计算：Spark Streaming、Flink</li>
<li>分布式：Hadoop、Spark</li>
</ul>
<p>上面那么多技术，我内容最重要的就是加粗的三部分，第一是语言：必须掌握的是Python，C++和JAVA中根据不同的组使用的是不同的语言，这个如果没有时间可以等进组后慢慢学习。然后是机器学习框架：Tensorflow和Pytorch至少要掌握一个吧，前期不用纠结学哪个，这个迁移成本很低，基本能够达到触类旁通，而且面试官不会为难你只会这个不会那个。最后是数据分析工具：Pandas是我们处理单机规模数据的利器，但是进入工业界，Hadoop和Spark是需要会用的，不过不用学太深，会用即可。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>本文从算法和工程两个角度分析了推荐系统的一个技术栈，但是还有很多方向遗漏，也有很多方向受限于现在的技术水平深度不够和有错误的情况，后续会不断补充和更正。</p>
<p>所以技术栈我列出的是一个非常广度的技术，实际上每一个技术钻研下去都需要非常多时间，而且不一定是你实际工作中会遇到的，所以不要被那么多技术吓到，也要避免陷入技术细节的海洋中。</p>
<p>我和非常多的大厂面试官讨论过技术深度和广度的问题，得出来的结论是对于入门的推荐算法工程师而言，实际上深度和广度的要求取决于你要去的组，有些组有很深的推荐技术沉淀，有很强的工程师团队，这样的组就会希望候选者能够在某个方面有比较深入的研究，这个方面既包含工程方面也包含研究方面。但是如果是比较新的组、或者技术沉淀不深、推荐不是主要任务的组，对深度要求就不会很高。总而言之，我认为对于应届生/实习生来说，在推荐最重要的工程技术/研究方向，至少在召回和排序模块，需要选一个作为方向，是需要较深钻研。对于其他技术/研究方向需要有一定了解，比如可以没用过强化学习，但是要知道强化学习能够在推荐中解决什么问题，剩下的可以等到真实<strong>遇到需要后再去学习</strong>。</p>
<p><strong>参考资料</strong></p>
<ul>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/aaOosZ57qJpIU6cma820Xw">万字入门推荐系统</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&amp;mid=2247496363&amp;idx=1&amp;sn=0d2b2ac176e2a72eb2e760b7b591788f&amp;chksm=fbd740c7cca0c9d16c76fdeb1a874a53f7408d8125b2e1bed3173ecb69d131167c1c9c35c71f&amp;scene=21#wechat_redirect">张俊林：技术演进趋势：召回-&gt;排序-&gt;重排</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MzU1NTMyOTI4Mw==&amp;mid=2247503484&amp;idx=2&amp;sn=e2a2cdd3a517ab09e903e69ccb1e9f94&amp;chksm=fbd77c10cca0f50642dde47439ed919aa2e61b7ff57bc4cbaacc3acaac3c620a1ed6f92684ab&amp;scene=21#wechat_redirect">微信”看一看”多模型内容策略与召回</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/u_5RdZ-BcIu_RoWNri76ig">多目标学习在推荐系统中的应用</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s?__biz=MjM5NjQ5MTI5OA==&amp;mid=2651749434&amp;idx=2&amp;sn=343e811408542dd1984582b8639240a6&amp;chksm=bd12a5778a652c61ed4297f1a17582cad4ca6b8e8d4d66843f169e0eda9f6aede988bc675743&amp;mpshare=1&amp;scene=23&amp;srcid=1115EcgbMw6GAhMnzV0URvgd#rd">强化学习在美团“猜你喜欢”的实践</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/YorzRyK0iplzqutnhEhrvw">推荐系统技术演进趋势：重排篇</a></li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/ylavFA_MXLUhIBLCqxAjLQ">阿里强化学习重排实践</a></li>
</ul>
<h1 id="第二章-推荐系统算法基础"><a href="#第二章-推荐系统算法基础" class="headerlink" title="第二章 推荐系统算法基础"></a>第二章 推荐系统算法基础</h1><h2 id="2-1经典召回模型"><a href="#2-1经典召回模型" class="headerlink" title="2.1经典召回模型"></a>2.1经典召回模型</h2><h3 id="2-1-1基于协同过滤的召回"><a href="#2-1-1基于协同过滤的召回" class="headerlink" title="2.1.1基于协同过滤的召回"></a>2.1.1基于协同过滤的召回</h3><h3 id="核心思想"><a href="#核心思想" class="headerlink" title="核心思想"></a>核心思想</h3><p>协同过滤（Collaborative Filtering）推荐算法是最经典、最常用的推荐算法。基本思想是：</p>
<ul>
<li>根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品。<ul>
<li>基于对用户历史行为数据的挖掘发现用户的喜好偏向， 并预测用户可能喜好的产品进行推荐。</li>
<li>一般是仅仅基于用户的行为数据（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）。</li>
</ul>
</li>
<li>目前应用比较广泛的协同过滤算法是基于邻域的方法，主要有：<ul>
<li>基于用户的协同过滤算法（UserCF）：给用户推荐和他兴趣相似的其他用户喜欢的产品。</li>
<li>基于物品的协同过滤算法（ItemCF）：给用户推荐和他之前喜欢的物品相似的物品。</li>
</ul>
</li>
</ul>
<p><strong>不管是 UserCF 还是 ItemCF 算法， 重点是计算用户之间（或物品之间）的相似度。</strong></p>
<h3 id="相似度度量方法"><a href="#相似度度量方法" class="headerlink" title="相似度度量方法"></a>相似度度量方法</h3><ol>
<li><p><strong>杰卡德（Jaccard）相似系数</strong></p>
<p><code>Jaccard</code> 系数是衡量两个集合的相似度一种指标，计算公式如下：</p>
<script type="math/tex; mode=display">
sim_{uv} = \frac{\left| N(u) \cap N(v) \right|}{\left| N(u) \cup N(v) \right|}</script></li>
</ol>
<ul>
<li>其中 $N(u)$，$N(v)$分别表示用户$u$和用户交$v$互物品的集合。</li>
<li><p>对于用户$u$ 和$v$，该公式反映了两个交互物品交集的数量占这两个用户交互物品并集的数量的比例。</p>
<p>由于杰卡德相似系数一般无法反映具体用户的评分喜好信息，所以常用来评估用户是否会对某物品进行打分， 而不是预估用户会对某物品打多少分。</p>
</li>
</ul>
<ol>
<li><p><strong>余弦相似度</strong> 余弦相似度衡量了两个向量的夹角，夹角越小越相似。余弦相似度的计算如下，其与杰卡德（Jaccard）相似系数只是在分母上存在差异：</p>
<script type="math/tex; mode=display">
sim_{uv} = \frac{\left| N(u) \cap N(v) \right|}{\sqrt{\left| N(u) \cdot N(v) \right|}}</script><p>从向量的角度进行描述，令矩阵$A$ 为用户-物品交互矩阵，矩阵的行表示用户，列表示物品。</p>
<ul>
<li><p>设用户和物品数量分别为$m$, $n$，交互矩阵$A$就是一个$m$行$n$列的矩阵。</p>
</li>
<li><p>矩阵中的元素均为$ 0/1$。若用户$i$对物品$j$存在交互，那么 $A_{ij} =1$，否则为 $0$。</p>
</li>
<li><p>那么，用户之间的相似度可以表示为：</p>
<script type="math/tex; mode=display">
sim_{uv} = cos(u, v) = \frac{u \cdot v}{\left| u \right| \cdot \left| v \right|}</script></li>
<li><p>向量 $u$,$v$在形式都是 one-hot 类型，$u \cdot v$表示向量点积。            </p>
</li>
</ul>
<p>上述用户-物品交互矩阵在现实中是十分稀疏的，为了节省内存，交互矩阵会采用字典进行存储。在 <code>sklearn</code> 中，余弦相似度的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br><span class="line"></span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]</span><br><span class="line">cosine_similarity([i, j])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol>
<li><p><strong>皮尔逊相关系数</strong></p>
<p>在用户之间的余弦相似度计算时，将用户向量的内积展开为各元素乘积和：</p>
<script type="math/tex; mode=display">
sim_{uv} = \frac{\sum_ir_{ui} * r_{vi}}{\sqrt{\sum_i r_{ui}^{2} \sqrt{\sum_i r_{vi}^{2}}}}</script><ul>
<li>其中，$r<em>{ui},r</em>{vi}$分别表示用户$u$和用户$v$对物品 $i$是否有交互(或具体评分值)。</li>
</ul>
<p>皮尔逊相关系数与余弦相似度的计算公式非常的类似，如下：</p>
<script type="math/tex; mode=display">
sim(u, v) = \frac{\sum_{i \in I}(r_{ui} - \bar r_u )(r_{vi} - \bar r_v)}{\sqrt{\sum_{i \in I}(r_{ui} - \bar r_u )^{2}} \sqrt{\sum_{i \in I}(r_{vi} - \bar r_v)^{2}}}</script><ul>
<li>其中，$r<em>{ui}, r</em>{vi}$分别表示用户$u$和用户$i$对物品$i$是否有交互(或具体评分值)；</li>
<li>$\bar r_u, \bar r_v$分别表示用户$u$和用户$v$交互的所有物品交互数量或者评分的平均值；</li>
</ul>
<p>相较于余弦相似度，皮尔逊相关系数通过使用用户的平均分对各独立评分进行修正，减小了用户评分偏置的影响。在<code>scipy</code>中，皮尔逊相关系数的实现：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"></span><br><span class="line">i = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">j = [<span class="number">1</span>, <span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0</span>]</span><br><span class="line">pearsonr(i, j)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="适用场景"><a href="#适用场景" class="headerlink" title="适用场景"></a>适用场景</h3><ul>
<li><em>Jaccard</em> 相似度表示两个集合的交集元素个数在并集中所占的比例 ，所以适用于隐式反馈数据（0-1）。</li>
<li>余弦相似度在度量文本相似度、用户相似度、物品相似度的时候都较为常用。</li>
<li>皮尔逊相关度，实际上也是一种余弦相似度。不过先对向量做了中心化，范围在 −1−1 到 11。<ul>
<li>相关度量的是两个变量的变化趋势是否一致，两个随机变量是不是同增同减。</li>
<li>不适合用作计算布尔值向量（0-1）之间相关度。</li>
</ul>
</li>
</ul>
<h3 id="基于用户的协同过滤"><a href="#基于用户的协同过滤" class="headerlink" title="基于用户的协同过滤"></a>基于用户的协同过滤</h3><h4 id="核心思想-1"><a href="#核心思想-1" class="headerlink" title="核心思想"></a>核心思想</h4><p>基于用户的协同过滤（UserCF）：</p>
<ul>
<li><p>例如，我们要对用户$A$进行物品推荐，可以先找到和他有相似兴趣的其他用户。</p>
</li>
<li><p>然后，将共同兴趣用户喜欢的，但用户$A$未交互过的物品推荐给 $A$。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/11/bZEseFk2vx4RM7q.png" alt="image-20240411110402334"></p>
</li>
</ul>
<h3 id="计算过程"><a href="#计算过程" class="headerlink" title="计算过程"></a>计算过程</h3><p>以下图为例，给用户推荐物品的过程可以形象化为：预测用户对物品进行打分的任务，表格里面是5个用户对于5件物品的一个打分情况，就可以理解为用户对物品的喜欢程度。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/11/n1FNf8TB9seOc7S.png" alt="image-20240411110604527"></p>
<p>UserCF算法的两个步骤：</p>
<ul>
<li>首先，根据前面的这些打分情况(或者说已有的用户向量）计算一下 Alice 和用户1， 2， 3， 4的相似程度， 找出与 Alice 最相似的 n 个用户。</li>
<li>根据这 n 个用户对物品 5 的评分情况和与 Alice 的相似程度会猜测出 Alice 对物品5的评分。如果评分比较高的话， 就把物品5推荐给用户 Alice， 否则不推荐。</li>
</ul>
<h3 id="具体过程"><a href="#具体过程" class="headerlink" title="具体过程"></a>具体过程</h3><ol>
<li><p>计算用户之间的相似度</p>
<ul>
<li>根据 1.2 节的几种方法， 我们可以计算出各用户之间的相似程度。对于用户 Alice，选取出与其最相近的$N$个用户。</li>
</ul>
</li>
<li><p>计算用户对新物品的评分预测</p>
<ul>
<li>常用的方式之一：利用目标用户与相似用户之间的相似度以及相似用户对物品的评分，来预测目标用户对候选物品的评分估计：</li>
</ul>
<script type="math/tex; mode=display">
R_{u,p} = \frac{\sum_{s \in S}(w_{u, s} \cdot R_{s, p})}{\sum_{s \in S}w_{u, s}}</script><ul>
<li><p>其中，权重 $w<em>{u, s}$是用户$u$和用户$s$ 的相似度， $R</em>{s, p}$是用户$s$对物品$p$的评分。</p>
</li>
<li><p>另一种方式：考虑到用户评分的偏置，即有的用户喜欢打高分， 有的用户喜欢打低分的情况。公式如下：</p>
<script type="math/tex; mode=display">
R_{u,p} =  \bar R_u +\frac{\sum_{s \in S}(w_{u, s} \cdot (R_{s, p} - \bar R_s))}{\sum_{s \in S}w_{u, s}}</script><ul>
<li>其中，$\bar R_s$表示用户$s$对物品的历史平均评分。</li>
</ul>
</li>
</ul>
</li>
<li><p>对用户进行物品推荐</p>
<ul>
<li>在获得用户$u$ 对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。</li>
</ul>
</li>
</ol>
<h3 id="手动计算"><a href="#手动计算" class="headerlink" title="手动计算"></a>手动计算</h3><p>根据上面的问题， 下面手动计算 Alice 对物品 5 的得分：</p>
<ol>
<li><p>计算 Alice 与其他用户的相似度（基于皮尔逊相关系数）</p>
<ul>
<li>手动计算 Alice 与用户 1 之间的相似度：</li>
</ul>
<blockquote>
<ul>
<li><p><strong>用户向量</strong>$Alice:(5,3,4,4),user1:(3,1,2,3),user2:(4,3,4,3),user3:(3,3,1,5),user4:(1,5,5,2)Alice:(5,3,4,4),user1:(3,1,2,3),user2:(4,3,4,3),user3:(3,3,1,5),user4:(1,5,5,2)$</p>
</li>
<li><p><strong>计算Alice与user1的余弦相似性:</strong></p>
</li>
</ul>
<script type="math/tex; mode=display">
sim(Alice, user_1) = cos(Alice, user_1) = \frac{5*3 + 3*1+4*2+4*3}{\sqrt(5^2+3^2+4^2+4^2)*\sqrt(3^2+1^2+2^2+3^2)}</script><ul>
<li><strong>计算Alice与user1皮尔逊相关系数:</strong><ul>
<li>计算均值：$Alice<em>{ave}=4, user</em>{1ave}=2.25$</li>
<li>各向量减去均值：$Alice:(1,-1,0,0), user_1:(0.75,-1.25,-0.25,0.75)$</li>
<li>最后计算两个新向量的余弦相似度，与上面的计算过程一致，结果是0.852</li>
</ul>
</li>
</ul>
</blockquote>
<ul>
<li><p>基于 sklearn 计算所有用户之间的皮尔逊相关系数。可以看出，与 Alice 相似度最高的用户为用户1和用户2。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/11/c8hMAeuiIvyJgxp.png" alt="image-20240411113155651"></p>
</li>
</ul>
</li>
<li><p><strong>根据相似度用户计算 Alice对物品5的最终得分</strong> 用户1对物品5的评分是3， 用户2对物品5的打分是5， 那么根据上面的计算公式， 可以计算出 Alice 对物品5的最终得分是</p>
<script type="math/tex; mode=display">
P_{Alice, item_5} = \bar R_{Alice} + \frac{\sum_{k=1}^2 (w_{Alice, userk}(R_{userk, item_5} - \bar R_{userk}))}{\sum_{k=1}^2 w_{Alice, userk}}=4+\frac{0.85*(3-2.4)+0.7*(5-3.8)}{0.85+0.7}=4.87</script></li>
<li><p><strong>根据用户评分对用户进行推荐</strong></p>
<ul>
<li>根据 Alice 的打分对物品排个序从大到小：<script type="math/tex; mode=display">
物品1>物品5>物品3=物品4>物品2</script></li>
</ul>
</li>
</ol>
<ul>
<li>如果要向 Alice 推荐2款产品的话， 我们就可以推荐物品 1 和物品 5 给 Alice。</li>
</ul>
<h3 id="UserCF编程实现"><a href="#UserCF编程实现" class="headerlink" title="UserCF编程实现"></a>UserCF编程实现</h3><ol>
<li><p>建立实验使用的数据表：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadData</span>():</span><br><span class="line">    users = &#123;<span class="string">&#x27;Alice&#x27;</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;user1&#x27;</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">3</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;user2&#x27;</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">4</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">5</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;user3&#x27;</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">3</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">4</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;user4&#x27;</span>: &#123;<span class="string">&#x27;A&#x27;</span>: <span class="number">1</span>, <span class="string">&#x27;B&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;C&#x27;</span>: <span class="number">5</span>, <span class="string">&#x27;D&#x27;</span>: <span class="number">2</span>, <span class="string">&#x27;E&#x27;</span>: <span class="number">1</span>&#125;</span><br><span class="line">             &#125;</span><br><span class="line">    <span class="keyword">return</span> users</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ul>
<li>这里使用字典来建立用户-物品的交互表。<ul>
<li>字典<code>users</code>的键表示不同用户的名字，值为一个评分字典，评分字典的键值对表示某物品被当前用户的评分。</li>
<li>由于现实场景中，用户对物品的评分比较稀疏。如果直接使用矩阵进行存储，会存在大量空缺值，故此处使用了字典。</li>
</ul>
</li>
</ul>
</li>
<li><p>计算用户相似性矩阵</p>
<ul>
<li>由于训练数据中共包含 5 个用户，所以这里的用户相似度矩阵的维度也为$5 * 5$。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">user_data = loadData()</span><br><span class="line">similarity_matrix = pd.DataFrame(</span><br><span class="line">	np.identity(<span class="built_in">len</span>(user_data)), <span class="comment">#创建对角矩阵，对角线为1，其余为0</span></span><br><span class="line">	index=user_data.keys(),</span><br><span class="line">	columns=user_data.keys(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历每条用户-物品评分数据</span></span><br><span class="line"><span class="keyword">for</span> u1, item1 <span class="keyword">in</span> user_data.items():</span><br><span class="line">	<span class="keyword">for</span> u2, item2 <span class="keyword">in</span> user_data.items():</span><br><span class="line">		<span class="keyword">if</span> u1 == u2:</span><br><span class="line">			<span class="keyword">continue</span></span><br><span class="line">		vec1, vec2 = [], []</span><br><span class="line">		<span class="keyword">for</span> item, rating1 <span class="keyword">in</span> item1.items():</span><br><span class="line">			rating2 = item2.get(item, -<span class="number">1</span>)</span><br><span class="line">			<span class="keyword">if</span> rating2 == -<span class="number">1</span>:</span><br><span class="line">				<span class="keyword">continue</span></span><br><span class="line">			vec1.append(rating1)</span><br><span class="line">			vec2.append(rating2)</span><br><span class="line">			<span class="comment"># 计算不同用户之间的皮尔逊相关系数</span></span><br><span class="line">			similarity_matrix[u1][u2] = np.corrcoef(vec1, vec2)[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">  </span><br><span class="line"><span class="built_in">print</span>(similarity_matrix)</span><br></pre></td></tr></table></figure>
</li>
</ol>
   <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">          <span class="number">1</span>         <span class="number">2</span>         <span class="number">3</span>         <span class="number">4</span>         <span class="number">5</span></span><br><span class="line"><span class="number">1</span>  <span class="number">1.000000</span>  <span class="number">0.852803</span>  <span class="number">0.707107</span>  <span class="number">0.000000</span> -<span class="number">0.792118</span></span><br><span class="line"><span class="number">2</span>  <span class="number">0.852803</span>  <span class="number">1.000000</span>  <span class="number">0.467707</span>  <span class="number">0.489956</span> -<span class="number">0.900149</span></span><br><span class="line"><span class="number">3</span>  <span class="number">0.707107</span>  <span class="number">0.467707</span>  <span class="number">1.000000</span> -<span class="number">0.161165</span> -<span class="number">0.466569</span></span><br><span class="line"><span class="number">4</span>  <span class="number">0.000000</span>  <span class="number">0.489956</span> -<span class="number">0.161165</span>  <span class="number">1.000000</span> -<span class="number">0.641503</span></span><br><span class="line"><span class="number">5</span> -<span class="number">0.792118</span> -<span class="number">0.900149</span> -<span class="number">0.466569</span> -<span class="number">0.641503</span>  <span class="number">1.000000</span></span><br></pre></td></tr></table></figure>
<ol>
<li><p>计算与 Alice 最相似的 <code>num</code> 个用户</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">target_user = <span class="string">&#x27;Alice&#x27;</span></span><br><span class="line">num = <span class="number">2</span></span><br><span class="line"><span class="comment"># 由于最相似的用户为自己，忽略本身</span></span><br><span class="line">sim_users = similarity_matrix[target_user].sort_values(ascend=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;与用户<span class="subst">&#123;target_user&#125;</span>最相似的<span class="subst">&#123;num&#125;</span>个用户为：<span class="subst">&#123;sim_users&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与用户 Alice 最相似的<span class="number">2</span>个用户为：[<span class="string">&#x27;user1&#x27;</span>, <span class="string">&#x27;user2&#x27;</span>]</span><br></pre></td></tr></table></figure>
</li>
<li><p>预测用户 Alice 对物品 <code>E</code> 的评分</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">cor_values_sum = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">target_item = <span class="string">&#x27;E&#x27;</span></span><br><span class="line"><span class="comment"># 基于皮尔逊相关稀疏预测用户评分</span></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> sim_users:</span><br><span class="line">    corr_value = similaity_matrix[target_user][user]</span><br><span class="line">    user_mean_rating = np.mean(<span class="built_in">list</span>(user_data[user].values()))</span><br><span class="line">    </span><br><span class="line">    weighted_scores += corr_value * (user_data[user][target_item] - user_mean_rating)</span><br><span class="line">    corr_values_sum += corr_value</span><br><span class="line">    </span><br><span class="line">target_user_mean_rating = np.mean(<span class="built_in">list</span>(user_data[target_user].values()))</span><br><span class="line">target_item_pred = target_user_mean_rating + weighted_scores / corr_values_sum</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;用户<span class="subst">&#123;target_user&#125;</span>对物品<span class="subst">&#123;target_item&#125;</span>的预测评分为：<span class="subst">&#123;target_item_pred&#125;</span>&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户 Alice 对物品E的预测评分为：<span class="number">4.871979899370592</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="UserCF优缺点"><a href="#UserCF优缺点" class="headerlink" title="UserCF优缺点"></a>UserCF优缺点</h3><p>User-based算法存在两个重大问题：</p>
<ol>
<li>数据稀疏性<ul>
<li>一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户。</li>
<li>这导致UserCF不适用于那些正反馈获取较困难的应用场景(如酒店预订， 大件物品购买等低频应用)。</li>
</ul>
</li>
<li>算法扩展性<ol>
<li>基于用户的协同过滤需要维护用户相似度矩阵以便快速的找出$TopN$相似用户， 该矩阵的存储开销非常大，存储空间随着用户数量的增加而增加。</li>
<li>故不适合用户数据量大的情况使用。</li>
</ol>
</li>
</ol>
<p>由于UserCF技术上的两点缺陷， 导致很多电商平台并没有采用这种算法， 而是采用了ItemCF算法实现最初的推荐系统。</p>
<h2 id="算法评估"><a href="#算法评估" class="headerlink" title="算法评估"></a>算法评估</h2><p>由于UserCF和ItemCF结果评估部分是共性知识点， 所以在这里统一标识。</p>
<h3 id="召回率"><a href="#召回率" class="headerlink" title="召回率"></a>召回率</h3><p>对用户$u$推荐 $N$个物品记为,$R(u)$ 令用户在$u$测试集上喜欢的物品集合为$T(u)$， 那么召回率定义为：</p>
<script type="math/tex; mode=display">
Recall=\frac{\sum_u \left| R(u) \cap T(u) \right|}{\sum_u \left| T(u) \right|}</script><ul>
<li>含义：<strong>在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例</strong>。</li>
</ul>
<h3 id="精确率"><a href="#精确率" class="headerlink" title="精确率"></a>精确率</h3><p>精确率定义为：</p>
<script type="math/tex; mode=display">
Precision = \frac{\sum_u \left| R(u) \cap T(u) \right|}{\sum_u \left| R(u) \right|}</script><ul>
<li>含义：<strong>推荐的物品中，对用户准确推荐的物品占总物品的比例</strong>。</li>
<li>如要确保召回率高，一般是推荐更多的物品，期望推荐的物品中会涵盖用户喜爱的物品。而实际中，推荐的物品中用户实际喜爱的物品占少数，推荐的精确率就会很低。故同时要确保高召回率和精确率往往是矛盾的，所以实际中需要在二者之间进行权衡。</li>
</ul>
<h3 id="覆盖率"><a href="#覆盖率" class="headerlink" title="覆盖率"></a>覆盖率</h3><p><strong>覆盖率反映了推荐算法发掘长尾的能力， 覆盖率越高， 说明推荐算法越能将长尾中的物品推荐给用户。</strong></p>
<script type="math/tex; mode=display">
Coverage=\frac{\left| U_{u \in U} R(u) \right|}{\left| I \right|}</script><ul>
<li>含义：<strong>推荐系统能够推荐出来的物品占总物品集合的比例</strong>。<ul>
<li>其中$I$表示所有物品的个数；</li>
<li>系统的用户集合为$U$;</li>
<li>推荐系统给每个用户推荐一个长度为$N$的物品列表$R(u)$.</li>
</ul>
</li>
<li>覆盖率表示最终的推荐列表中包含多大比例的物品。如果所有物品都被给推荐给至少一个用户， 那么覆盖率是100%。</li>
</ul>
<h3 id="新颖度"><a href="#新颖度" class="headerlink" title="新颖度"></a>新颖度</h3><p>用推荐列表中物品的平均流行度度量推荐结果的新颖度。 如果推荐出的物品都很热门， 说明推荐的新颖度较低。 由于物品的流行度分布呈长尾分布， 所以为了流行度的平均值更加稳定， 在计算平均流行度时对每个物品的流行度取对数。</p>
<ul>
<li>O’scar Celma 在博士论文 “<a target="_blank" rel="noopener" href="http://mtg.upf.edu/static/media/PhD_ocelma.pdf">Music Recommendation and Discovery in the Long Tail</a> “ 中研究了新颖度的评测。</li>
</ul>
<h3 id="基于物品的协同过滤"><a href="#基于物品的协同过滤" class="headerlink" title="基于物品的协同过滤"></a>基于物品的协同过滤</h3><h3 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h3><p>基于物品的协同过滤（ItemCF）：</p>
<ul>
<li>预先根据所有用户的历史行为数据，计算物品之间的相似性。</li>
<li>然后，把与用户喜欢的物品相类似的物品推荐给用户。</li>
</ul>
<p>举例来说，如果用户 1 喜欢物品 A ，而物品 A 和 C 非常相似，则可以将物品 C 推荐给用户1。ItemCF算法并不利用物品的内容属性计算物品之间的相似度， 主要通过分析用户的行为记录计算物品之间的相似度， 该算法认为， 物品 A 和物品 C 具有很大的相似度是因为喜欢物品 A 的用户极可能喜欢物品 C。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240427215458380.png" alt="image-20240427215458380"></p>
<h3 id="计算过程-1"><a href="#计算过程-1" class="headerlink" title="计算过程"></a>计算过程</h3><p>基于物品的协同过滤算法和基于用户的协同过滤算法很像， 所以我们这里直接还是拿上面 Alice 的那个例子来看。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/27/8viM1fWGdKuEXN3.png" alt="image-20240427215540918"></p>
<p>如果想知道 Alice 对物品5打多少分， 基于物品的协同过滤算法会这么做：</p>
<ul>
<li>首先计算一下物品5和物品1， 2， 3， 4之间的相似性。</li>
<li>在Alice找出与物品 5 最相近的 n 个物品。</li>
<li>根据 Alice 对最相近的 n 个物品的打分去计算对物品 5 的打分情况。</li>
</ul>
<p><strong>手动计算</strong>：</p>
<ol>
<li>手动计算物品之间的相似度</li>
</ol>
<blockquote>
<p><strong>物品向量</strong>：</p>
<p>物品1(3,4,3,1),物品2(1,3,3,5),物品3(2,4,1,5),物品4(3,3,5,2),物品5(3,5,4,1)</p>
<ul>
<li><strong>下面计算物品 5 和物品 1 之间的余弦相似性:</strong><script type="math/tex; mode=display">
sim(物品1,物品5)=\frac{3*3+4*5+3*4+1*1}{\sqrt{9+16+9+1}*\sqrt{9+25+16+1}}</script></li>
</ul>
<ul>
<li><strong>皮尔逊相关系数类似。</strong></li>
</ul>
</blockquote>
<ol>
<li>基于 <code>sklearn</code> 计算物品之间的皮尔逊相关系数：</li>
</ol>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/27/Ma1HrBGQpk3xsdy.png" alt="image-20240427220522091"></p>
<ol>
<li>根据皮尔逊相关系数， 可以找到与物品5最相似的2个物品是 item1 和 item4， 下面基于上面的公式计算最终得分：</li>
</ol>
<script type="math/tex; mode=display">
P_{Alice,物品5}=\bar R_{物品5}+\frac{\sum_{k=1}^2 (w_{物品5,物品k} (R_{Alice,物品k-\bar R_{物品k}}))}{\sum_{k=1}^2 w_{物品k,物品5}} \\
=\frac{13}{4}+\frac{0.97*(5-3.2)+0.58*(4-3.4)}{0.97+0.58} \\ =4.6</script><h3 id="ItemCF编程实现"><a href="#ItemCF编程实现" class="headerlink" title="ItemCF编程实现"></a>ItemCF编程实现</h3><ol>
<li><p>构建物品-用户的评分矩阵</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadData</span>():</span><br><span class="line">    items = &#123;</span><br><span class="line">        <span class="string">&#x27;A&#x27;</span>: &#123;<span class="string">&#x27;Alice&#x27;</span>: <span class="number">5.0</span>, <span class="string">&#x27;user1&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user2&#x27;</span>: <span class="number">4.0</span>, <span class="string">&#x27;user3&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user4&#x27;</span>: <span class="number">1.0</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;B&#x27;</span>: &#123;<span class="string">&#x27;Alice&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user1&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;user2&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user3&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user4&#x27;</span>: <span class="number">5.0</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;C&#x27;</span>: &#123;<span class="string">&#x27;Alice&#x27;</span>: <span class="number">4.0</span>, <span class="string">&#x27;user1&#x27;</span>: <span class="number">2.0</span>, <span class="string">&#x27;user2&#x27;</span>: <span class="number">4.0</span>, <span class="string">&#x27;user3&#x27;</span>: <span class="number">1.0</span>, <span class="string">&#x27;user4&#x27;</span>: <span class="number">5.0</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;D&#x27;</span>: &#123;<span class="string">&#x27;Alice&#x27;</span>: <span class="number">4.0</span>, <span class="string">&#x27;user1&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user2&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user3&#x27;</span>: <span class="number">5.0</span>, <span class="string">&#x27;user4&#x27;</span>: <span class="number">2.0</span>&#125;,</span><br><span class="line">             <span class="string">&#x27;E&#x27;</span>: &#123;<span class="string">&#x27;user1&#x27;</span>: <span class="number">3.0</span>, <span class="string">&#x27;user2&#x27;</span>: <span class="number">5.0</span>, <span class="string">&#x27;user3&#x27;</span>: <span class="number">4.0</span>, <span class="string">&#x27;user4&#x27;</span>: <span class="number">1.0</span>&#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> items</span><br></pre></td></tr></table></figure>
<ol>
<li>计算物品间的相似度矩阵</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">item_data = loadData()</span><br><span class="line"></span><br><span class="line">simiality_matrix = pd.DataFrame(</span><br><span class="line">    np.identity(<span class="built_in">len</span>(item_data)),</span><br><span class="line">    index=item_data.keys(),</span><br><span class="line">    columns=item_data.keys(),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment">#遍历每条物品-用户评分数据</span></span><br><span class="line"><span class="keyword">for</span> i1, user1 <span class="keyword">in</span> item_data.items():</span><br><span class="line">    <span class="keyword">for</span> i2, user2 <span class="keyword">in</span> item_data.items():</span><br><span class="line">        <span class="keyword">if</span> i1 == i2:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        vec1, vec2 = [], []</span><br><span class="line">        <span class="keyword">for</span> user, rating1 <span class="keyword">in</span> unser1.items();</span><br><span class="line">        	rating2 = user2.get(user, -<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> rating2 == -<span class="number">1</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            vec1.append(rating1)</span><br><span class="line">            vec2.append(rating2)</span><br><span class="line">    	similarity_matrix[i1][i2] = np.corrcoef(vec1, vec2)[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(similarity_matrix)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">       A         B         C         D         E</span><br><span class="line">A  1.000000 -0.476731 -0.123091  0.532181  0.969458</span><br><span class="line">B -0.476731  1.000000  0.645497 -0.310087 -0.478091</span><br><span class="line">C -0.123091  0.645497  1.000000 -0.720577 -0.427618</span><br><span class="line">D  0.532181 -0.310087 -0.720577  1.000000  0.581675</span><br><span class="line">E  0.969458 -0.478091 -0.427618  0.581675  1.000000</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<ol>
<li>从 Alice 购买过的物品中，选出与物品 <code>E</code> 最相似的 <code>num</code> 件物品。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">target_user = <span class="string">&#x27;Alice&#x27;</span></span><br><span class="line">target_item = <span class="string">&#x27;E&#x27;</span></span><br><span class="line">num = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">sim_items = []</span><br><span class="line">sim_items_list = similarity_matrix[target_item].sort_values(ascending=<span class="literal">False</span>).index.tolist()</span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> sim_items_list:</span><br><span class="line">    <span class="comment"># 如果target_user对物品item评分过</span></span><br><span class="line">    <span class="keyword">if</span> target_user <span class="keyword">in</span> item_data[item]:</span><br><span class="line">        sim_items.append(item)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sim_items) == num:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;与物品<span class="subst">&#123;target_item&#125;</span>最相似的<span class="subst">&#123;num&#125;</span>个物品为：<span class="subst">&#123;sim_items&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">与物品E最相似的2个物品为：[&#x27;A&#x27;, &#x27;D&#x27;]</span><br></pre></td></tr></table></figure>
<ol>
<li>预测用户 Alice 对物品 <code>E</code> 的评分</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">target_user_mean_rating = np.mean(<span class="built_in">list</span>(item_data[target_item].values()))</span><br><span class="line">weighted_scores = <span class="number">0.</span></span><br><span class="line">corr_values_sum = <span class="number">0.</span></span><br><span class="line"></span><br><span class="line">target_item = <span class="string">&#x27;E&#x27;</span></span><br><span class="line"><span class="keyword">for</span> item <span class="keyword">in</span> sim_items:</span><br><span class="line">    corr_value = similarity_matrix[target_item][item]</span><br><span class="line">    user_mean_rating = np.mean(<span class="built_in">list</span>(item_data[item].values()))</span><br><span class="line"></span><br><span class="line">    weighted_scores += corr_value * (item_data[item][target_user] - user_mean_rating)</span><br><span class="line">    corr_values_sum += corr_value</span><br><span class="line"></span><br><span class="line">target_item_pred = target_user_mean_rating + weighted_scores / corr_values_sum</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&#x27;用户<span class="subst">&#123;target_user&#125;</span>对物品<span class="subst">&#123;target_item&#125;</span>的预测评分为：<span class="subst">&#123;target_item_pred&#125;</span>&#x27;</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">用户 Alice 对物品E的预测评分为：4.6</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h3 id="协同过滤算法的权重改进"><a href="#协同过滤算法的权重改进" class="headerlink" title="协同过滤算法的权重改进"></a>协同过滤算法的权重改进</h3><ul>
<li><p>base 公式</p>
<script type="math/tex; mode=display">
w_{ij} = \frac{|N(i) \cap N(j) |}{|N(i)|}</script><ul>
<li>该公式表示同时喜好物品$i$和物品$j$的用户数，占喜爱物品$i$的比例。</li>
<li>缺点：若物品$j$为热门物品，那么它与任何物品的相似度都很高。</li>
</ul>
</li>
<li><p>对热门物品进行惩罚</p>
<script type="math/tex; mode=display">
w_{ij} = \frac{|N(i) \cap N(j)|}{\sqrt{|N(i)||N(j)|}}</script><ul>
<li>根据 base 公式在的问题，对物品$j$进行打压。打压的出发点很简单，就是在分母再除以一个物品$j$被购买的数量。</li>
<li>此时，若物品品$j$为热门物品，那么对应的$N(j)$也会很大，受到的惩罚更多</li>
</ul>
</li>
<li><p>控制对热门物品的惩罚力度</p>
<script type="math/tex; mode=display">
w_{ij} = \frac{|N(i) \cap N(j)|}{|N(i)|^{1-a}|N(j)|^{a}}</script><ul>
<li>除了第二点提到的办法，在计算物品之间相似度时可以对热门物品进行惩罚外。</li>
<li>可以在此基础上，进一步引入参数a ，这样可以通过控制参数a来决定对热门物品的惩罚力度。</li>
</ul>
</li>
<li><p>对活跃用户的惩罚</p>
<ul>
<li><p>在计算物品之间的相似度时，可以进一步将用户的活跃度考虑进来。</p>
<script type="math/tex; mode=display">
w_{ij} = \frac{\sum_{u \in N(i) \cap N(j)} \frac{1}{log1+|N(u)|}}{|N(i)|^{1-a}|N(j)|^{a}}</script></li>
<li><p>对于异常活跃的用户，在计算物品之间的相似度时，他的贡献应该小于非活跃用户。</p>
</li>
</ul>
</li>
</ul>
<h3 id="协同过滤算法的问题分析"><a href="#协同过滤算法的问题分析" class="headerlink" title="协同过滤算法的问题分析"></a>协同过滤算法的问题分析</h3><p>协同过滤算法存在的问题之一就是泛化能力弱：</p>
<ul>
<li>即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。</li>
<li>导致的问题是<strong>热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐</strong>。</li>
</ul>
<p>比如下面这个例子：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/28/2KL4yXo7pnV8YMJ.png" alt="image-20240428211817361"></p>
<ul>
<li>左边矩阵中，𝐴,𝐵,𝐶,𝐷<em>A</em>,<em>B</em>,<em>C</em>,<em>D</em> 表示的是物品。</li>
<li>可以看出，D是一件热门物品，其与 A、B、C的相似度比较大。因此，推荐系统更可能将D推荐给用过 A、B、C的用户。</li>
<li>但是，推荐系统无法找出 A、B、C之间相似性的原因是交互数据太稀疏， 缺乏相似性计算的直接数据。</li>
</ul>
<p>所以这就是协同过滤的天然缺陷：<strong>推荐系统头部效应明显， 处理稀疏向量的能力弱</strong>。</p>
<p>为了解决这个问题， 同时增加模型的泛化能力。2006年，<strong>矩阵分解技术(Matrix Factorization, MF</strong>)被提出：</p>
<ul>
<li>该方法在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品， 挖掘用户和物品的隐含兴趣和隐含特征。</li>
<li>在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。</li>
</ul>
<h3 id="课后思考"><a href="#课后思考" class="headerlink" title="课后思考"></a>课后思考</h3><ol>
<li><strong>什么时候使用UserCF，什么时候使用ItemCF？为什么？</strong></li>
</ol>
<blockquote>
<p>（1）UserCF</p>
<ul>
<li>由于是基于用户相似度进行推荐， 所以具备更强的社交特性， 这样的特点非常适于<strong>用户少， 物品多， 时效性较强的场合</strong>。<ul>
<li>比如新闻推荐场景， 因为新闻本身兴趣点分散， 相比用户对不同新闻的兴趣偏好， 新闻的及时性，热点性往往更加重要， 所以正好适用于发现热点，跟踪热点的趋势。</li>
<li>另外还具有推荐新信息的能力， 更有可能发现惊喜, 因为看的是人与人的相似性, 推出来的结果可能更有惊喜，可以发现用户潜在但自己尚未察觉的兴趣爱好。</li>
</ul>
</li>
</ul>
<p>（2）itemCF</p>
<ul>
<li>这个更适用于兴趣变化较为稳定的应用， 更接近于个性化的推荐， 适合<strong>物品少，用户多，用户兴趣固定持久， 物品更新速度不是太快的场合</strong>。</li>
<li>比如推荐艺术品， 音乐， 电影。</li>
</ul>
</blockquote>
<ol>
<li><strong>协同过滤在计算上有什么缺点？有什么比较好的思路可以解决（缓解）？</strong></li>
</ol>
<blockquote>
<p>该问题答案参考上一小节的<strong>协同过滤算法的问题分析</strong>。</p>
</blockquote>
<ol>
<li><strong>上面介绍的相似度计算方法有什么优劣之处？</strong></li>
</ol>
<blockquote>
<p><strong>cosine相似度计算简单方便，一般较为常用。但是，当用户的评分数据存在 bias 时，效果往往不那么好。</strong></p>
<ul>
<li>简而言之，就是不同用户评分的偏向不同。部分用户可能乐于给予好评，而部分用户习惯给予差评或者乱评分。</li>
<li>这个时候，根据cosine 相似度计算出来的推荐结果效果会打折扣。</li>
</ul>
<p>举例来说明，如下图（<code>X,Y,Z</code> 表示物品，<code>d,e,f</code>表示用户）：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/28/s7BDKTPaZ3VEGuR.png" alt="image-20240428212534039"></p>
<ul>
<li>如果使用余弦相似度进行计算，用户 d 和 e 之间较为相似。但是实际上，用户 d 和 f 之间应该更加相似。只不过由于 d 倾向于打高分，e 倾向于打低分导致二者之间的余弦相似度更高。</li>
<li>这种情况下，可以考虑使用皮尔逊相关系数计算用户之间的相似性关系。</li>
</ul>
</blockquote>
<ol>
<li><strong>协同过滤还存在其他什么缺陷？有什么比较好的思路可以解决（缓解）？</strong></li>
</ol>
<blockquote>
<ul>
<li>协同过滤的优点就是没有使用更多的用户或者物品属性信息，仅利用用户和物品之间的交互信息就能完成推荐，该算法简单高效。</li>
<li>但这也是协同过滤算法的一个弊端。由于未使用更丰富的用户和物品特征信息，这也导致协同过滤算法的模型表达能力有限。</li>
<li>对于该问题，逻辑回归模型（LR）可以更好地在推荐模型中引入更多特征信息，提高模型的表达能力。</li>
</ul>
</blockquote>
<h3 id="2-1-2基于向量的召回"><a href="#2-1-2基于向量的召回" class="headerlink" title="2.1.2基于向量的召回"></a>2.1.2基于向量的召回</h3><h2 id="FM模型结构"><a href="#FM模型结构" class="headerlink" title="FM模型结构"></a>FM模型结构</h2><p>FM 模型用于排序时，模型的公式定义如下：</p>
<script type="math/tex; mode=display">
\hat{y}(x):= w_0+\sum_{i=1}^n w_ix_i + \sum_{i=1}^{n} \sum_{j=i+1}^n \langle v_i,v_j \rangle x_ix_j</script><ul>
<li>其中，$i$表示特征的序号，$n$表示特征的数量；$x_i \in R$表示第$i$个特征的值。</li>
<li>$v<em>i,v_j \in R^k$分别表示特征$x_i,x_j$对应的隐语义向量（Embedding向量），$\langle v_i,v_j \rangle := \sum</em>{f=1}^k v<em>{i,f} \cdot v</em>{j,f}$。</li>
<li>$w_0,w_i \in R$表示需要学习的参数。</li>
</ul>
<p> <strong>FM的一阶交互特征</strong></p>
<p>在 FM 的表达式中，前两项为特征的一阶交互项。将其拆分为用户特征和物品特征的一阶特征交互项，如下：</p>
<script type="math/tex; mode=display">
w_0 + \sum_{i=1}^n w_ix_i =w_0 + \sum_{t \in I} w_tx_t + \sum_{u \in U} w_u x_u</script><ul>
<li>其中，$U$表示用户相关特征集合，$I$表示物品相关特征集合。</li>
</ul>
<p><strong>FM 的二阶特征交互</strong></p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/29/nL1HB7tQdFo2yw6.png" alt="image-20240429222930591"></p>
<ul>
<li>公式变换后，计算复杂度由$O(kn^2)$降到$O(kn)$。</li>
</ul>
<p>由于本文章需要将 FM 模型用在召回，故将二阶特征交互项拆分为用户和物品项。有：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/29/mKQL1DdTsNwu2Ec.png" alt="image-20240429223104780"></p>
<h2 id="FM用于召回"><a href="#FM用于召回" class="headerlink" title="FM用于召回"></a>FM用于召回</h2><h2 id="2-2经典排序模型"><a href="#2-2经典排序模型" class="headerlink" title="2.2经典排序模型"></a>2.2经典排序模型</h2><h3 id="2-2-3Wide-amp-Deep系列"><a href="#2-2-3Wide-amp-Deep系列" class="headerlink" title="2.2.3Wide&amp;Deep系列"></a>2.2.3Wide&amp;Deep系列</h3><h3 id="2-2-1GBDT-LR简介"><a href="#2-2-1GBDT-LR简介" class="headerlink" title="2.2.1GBDT+LR简介"></a>2.2.1GBDT+LR简介</h3><p>前面介绍的协同过滤和矩阵分解存在的劣势就是<strong>仅利用了用户与物品相互行为信息进行推荐， 忽视了用户自身特征， 物品自身特征以及上下文信息等</strong>，导致生成的结果往往会比较片面。 而这次介绍的这个模型是2014年由Facebook提出的GBDT+LR模型， 该模型利用GBDT自动进行特征组合和选择， 进而生成新的离散特征向量， 再把该特征向量当做LR模型的输入， 来产生最后的预测结果， 该模型能够综合利用用户、物品和上下文等多种不同的特征， 生成较为全面的推荐结果， 在CTR点击率预估场景下使用较为广泛。</p>
<p>下面首先会介绍逻辑回归和GBDT模型各自的原理及优缺点， 然后介绍GBDT+LR模型的工作原理和细节。</p>
<h4 id="逻辑回归模型"><a href="#逻辑回归模型" class="headerlink" title="逻辑回归模型"></a>逻辑回归模型</h4><p>逻辑回归模型非常重要， 在推荐领域里面， 相比于传统的协同过滤， 逻辑回归模型能够综合利用用户、物品、上下文等多种不同的特征生成较为“全面”的推荐结果， 关于逻辑回归的更多细节， 可以参考下面给出的链接，这里只介绍比较重要的一些细节和在推荐中的应用。</p>
<p>逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归成为了一个优秀的分类算法， 学习逻辑回归模型， 首先应该记住一句话：<strong>逻辑回归假设数据服从伯努利分布,通过极大似然的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。</strong></p>
<p>相比于协同过滤和矩阵分解利用用户的物品“相似度”进行推荐， 逻辑回归模型将问题看成了一个分类问题， 通过预测正样本的概率对物品进行排序。这里的正样本可以是用户“点击”了某个商品或者“观看”了某个视频， 均是推荐系统希望用户产生的“正反馈”行为， 因此<strong>逻辑回归模型将推荐问题转化成了一个点击率预估问题</strong>。而点击率预测就是一个典型的二分类， 正好适合逻辑回归进行处理， 那么逻辑回归是如何做推荐的呢？ 过程如下：</p>
<ol>
<li>将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转成数值型向量</li>
<li>确定逻辑回归的优化目标，比如把点击率预测转换成二分类问题， 这样就可以得到分类问题常用的损失作为目标， 训练模型</li>
<li>在预测的时候， 将特征向量输入模型产生预测， 得到用户“点击”物品的概率</li>
<li>利用点击概率对候选物品排序， 得到推荐列表</li>
</ol>
<p>推断过程可以用下图来表示：</p>
<p>这里的关键就是每个特征的权重参数$w$， 我们一般是使用梯度下降的方式， 首先会先随机初始化参数$w$， 然后将特征向量（也就是我们上面数值化出来的特征）输入到模型， 就会通过计算得到模型的预测概率， 然后通过对目标函数求导得到每个$w$的梯度， 然后进行更新$w$</p>
<script type="math/tex; mode=display">
J(w) = - \frac{1}{m}(\sum_{i=1}^m (y^ilogf_w(x^i)+(1-y^i)log(1-f_w(x^i))))</script><p>求导之后：</p>
<script type="math/tex; mode=display">
w_j \leftarrow wj- \gamma \frac{1}{m}\sum_{i=1}^m (f_w(x^i) - y^i)x_j^i</script><p>这样通过若干次迭代， 就可以得到最终的𝑤<em>w</em>了， 关于这些公式的推导，可以参考下面给出的文章链接， 下面我们分析一下逻辑回归模型的优缺点。</p>
<p><strong>优点</strong></p>
<ol>
<li>LR模型形式简单，<strong>可解释性好</strong>，从特征的权重可以看到不同的特征对最后结果的影响。</li>
<li>训练时便于<strong>并行化</strong>，在预测时只需要对特征进行线性加权，所以<strong>性能比较好</strong>，往往适合处理<strong>海量id类特征</strong>，用id类特征有一个很重要的好处，就是<strong>防止信息损失</strong>（相对于范化的 CTR 特征），对于头部资源会有更细致的描述</li>
<li>资源占用小,尤其是内存。在实际的工程应用中只需要存储权重比较大的特征及特征对应的权重。</li>
<li>方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类)</li>
</ol>
<p><strong>当然， 逻辑回归模型也有一定的局限性</strong></p>
<ol>
<li>表达能力不强， 无法进行特征交叉， 特征筛选等一系列“高级“操作（这些工作都得人工来干， 这样就需要一定的经验， 否则会走一些弯路）， 因此可能造成信息的损失</li>
<li>准确率并不是很高。因为这毕竟是一个线性模型加了个sigmoid， 形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布</li>
<li>处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据， 如果想处理非线性， 首先对连续特征的处理需要先进行<strong>离散化</strong>（离散化的目的是为了引入非线性），如上文所说，人工分桶的方式会引入多种问题。</li>
<li>LR 需要进行<strong>人工特征组合</strong>，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。</li>
</ol>
<p>所以如何<strong>自动发现有效的特征、特征组合，弥补人工经验不足，缩短LR特征实验周期</strong>，是亟需解决的问题， 而GBDT模型， 正好可以<strong>自动发现特征并进行有效组合</strong></p>
<h4 id="GBDT模型"><a href="#GBDT模型" class="headerlink" title="GBDT模型"></a>GBDT模型</h4><p>GBDT全称梯度提升决策树，在传统机器学习算法里面是对真实分布拟合的最好的几种算法之一，在前几年深度学习还没有大行其道之前，gbdt在各种竞赛是大放异彩。原因大概有几个，一是效果确实挺不错。二是即可以用于分类也可以用于回归。三是可以筛选特征， 所以这个模型依然是一个非常重要的模型。</p>
<p>GBDT是通过采用加法模型(即基函数的线性组合），以及不断减小训练过程产生的误差来达到将数据分类或者回归的算法， 其训练过程如下：</p>
<p>gbdt通过多轮迭代， 每轮迭代会产生一个弱分类器， 每个分类器在上一轮分类器的残差基础上进行训练。 gbdt对弱分类器的要求一般是足够简单， 并且低方差高偏差。 因为训练的过程是通过降低偏差来不断提高最终分类器的精度。 由于上述高偏差和简单的要求，每个分类回归树的深度不会很深。最终的总分类器是将每轮训练得到的弱分类器加权求和得到的（也就是加法模型）。</p>
<p>关于GBDT的详细细节，依然是可以参考下面给出的链接。这里想分析一下GBDT如何来进行二分类的，因为我们要明确一点就是<strong>gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的</strong>， 而这里的残差指的就是当前模型的负梯度值， 这个就要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的， 而<strong>gbdt 无论用于分类还是回归一直都是使用的CART 回归树</strong>， 那么既然是回归树， 是如何进行二分类问题的呢？</p>
<p>GBDT 来解决二分类问题和解决回归问题的本质是一样的，都是通过不断构建决策树的方式，使预测结果一步步的接近目标值， 但是二分类问题和回归问题的损失函数是不同的， 关于GBDT在回归问题上的树的生成过程， 损失函数和迭代原理可以参考给出的链接， 回归问题中一般使用的是平方损失， 而二分类问题中， GBDT和逻辑回归一样， 使用的下面这个：</p>
<script type="math/tex; mode=display">
L=arg \, min [\sum_i^n-(y_ilog(p_i)+(1-y_i)log(1-p_i))]</script><p>其中， $y_i$是第$i$个样本的观测值， 取值要么是0要么是1， 而是$p_i$第$i$个样本的预测值， 取值是0-1之间的概率，由于我们知道GBDT拟合的残差是当前模型的负梯度， 那么我们就需要求出这个模型的导数， 即$\frac{dL}{dp_i}$， 对于某个特定的样本， 求导的话就可以只考虑它本身， 去掉加和号， 那么就变成了$\frac{dl}{dp_i}$， 其中$l$如下：</p>
<script type="math/tex; mode=display">
l=-y_ilog(p_i) - (1 - y_i)log(1-p_i) \\
=-y_i log(p_i) - log(1 - p_i) + y_i log(1-p_i) \\
=-y_i(log(\frac{p_i}{1-p_i})) - log(1-p_i)</script><p>如果对逻辑回归非常熟悉的话，$log(\frac{p_i}{1-p_i})$ 一定不会陌生吧， 这就是对几率比取了个对数， 并且在逻辑回归里面这个式子会等于$\theta X$，所以才推出了$p_i=\frac{1}{1+e^{- \theta X}}$的那个形式。 这里令$\eta_i=\frac{p_i}{1-p_i}$，即$p_i=\frac{\eta_i}{1+\eta_i}$，则上面这个式子变成了：</p>
<script type="math/tex; mode=display">
l=-y_ilog(\eta_i)-log(1-\frac{e^{log(\eta_i)}}{1+e^{log(\eta_i)}}) \\
= -y_ilog(\eta_i)-log(\frac{e^{log(\eta_i)}}{1+e^{log(\eta_i)}}) \\
=-y_ilog(\eta_i)+log(1+e^{log(\eta_i)}) \\</script><p>这时候，我们对$log(\eta_i)$求导， 得</p>
<script type="math/tex; mode=display">
\frac{dl}{dlog(\eta_i)}=-y_i+\frac{e^{log(\eta_i)}}{1+e^{log(\eta_i)}}=-y_i+p_i=p_i-y_i</script><p>这样， 我们就得到了某个训练样本在当前模型的梯度值了， 那么残差就是$y_i-p_i$。GBDT二分类的这个思想，其实和逻辑回归的思想一样，<strong>逻辑回归是用一个线性模型去拟合$P(y=1|x)$这个事件的对数几率$log\frac{p}{1-p}=\theta^Tx$</strong>， GBDT二分类也是如此， 用一系列的梯度提升树去拟合这个对数几率， 其分类模型可以表达为：</p>
<script type="math/tex; mode=display">
P(Y=1|x)=\frac{1}{1+e^{-F_M(x)}}</script><p>初始化GBDT 和回归问题一样， 分类 GBDT 的初始状态也只有一个叶子节点，该节点为所有样本的初始预测值，如下：</p>
<script type="math/tex; mode=display">
F_0(x)=arg \, min(\sum_{i=1}^n L(y, \gamma))</script><p>上式里面，$F$代表GBDT模型， $F_0$是模型的初始状态， 该式子的意思是找到一个，$\gamma$使所有样本的 Loss 最小，在这里及下文中，$\gamma$都表示节点的输出，即叶子节点， 且它是一个$log(\eta_i)$形式的值(回归值)，在初始状态，$\gamma=F_0$。</p>
<p>下面看例子(该例子来自下面的第二个链接)， 假设我们有下面3条样本：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/29/IzmOvTwlacWe4XQ.png" alt="image-20240429104938229"></p>
<p>我们希望构建 GBDT 分类树，它能通过「喜欢爆米花」、「年龄」和「颜色偏好」这 3 个特征来预测某一个样本是否喜欢看电影。我们把数据代入上面的公式中求Loss:</p>
<script type="math/tex; mode=display">
Loss = L(1, \gamma) + L(1, \gamma) + L(0, \gamma)</script><p>为了令其最小， 我们求导， 且让导数为0， 则：</p>
<script type="math/tex; mode=display">
Loss = p - 1 + p - 1 + p - 0 = 0</script><p>于是， 就得到了初始值$p=\frac{2}{3}, \gamma=log(\frac{p}{1-p})=0.69$，模型的初始状态$F_0(x) = 0.69$</p>
<p>说了一大堆，<strong>实际上你却可以很容易的算出该模型的初始值，它就是正样本数比上负样本数的 log 值</strong>，例子中，正样本数为 2 个，负样本为 1 个，那么：</p>
<script type="math/tex; mode=display">
F_0(x)=\log(\frac{positive_count}{negative_count})=\log(\frac{2}{1})=0.69</script><ol>
<li><p>循环生成决策树 这里回忆一下回归树的生成步骤， 其实有4小步， 第一就是计算负梯度值得到残差， 第二步是用回归树拟合残差， 第三步是计算叶子节点的输出值， 第四步是更新模型。 下面我们一一来看：</p>
<ol>
<li><p>计算负梯度得到残差</p>
<script type="math/tex; mode=display">
r_{im} = - [\frac{\partial L(y_i, F(x_i))}{\partial F(x_i)}]_{F(x)=F_{m-1}(x)}</script><p>此处使用$m-1$棵树的模型， 计算每个样本的残差,$r_{im}$ 就是上面的$y_i- p_i$, 于是例子中， 每个样本的残差：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/29/JiMa9vt7uXpqLyD.png" alt="image-20240429110324439"></p>
</li>
<li><p>使用回归树来拟合$\gamma_{jm}$， 这里的表$i$示样本哈，回归树的建立过程可以参考下面的链接文章，简单的说就是遍历每个特征， 每个特征下遍历每个取值， 计算分裂后两组数据的平方损失， 找到最小的那个划分节点。 假如我们产生的第2棵决策树如下：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/29/HNqI53UDMw2V6zc.png" alt="image-20240429110613002"></p>
<p>对于每个叶子节点$j$, 计算最佳残差拟合值</p>
<script type="math/tex; mode=display">
\gamma_{jm}=\arg \min_r \sum_{x \in R_{ij}} L(y_i, F_{m-1}+\gamma)</script><p>意思是， 在刚构建的树$m$中， 找到每个节点$j$的输出$\gamma_{jm}$, 能使得该节点的loss最小。 那么我们看一下这个$\gamma$的求解方式， 这里非常的巧妙。 首先， 我们把损失函数写出来， 对于左边的第一个样本， 有</p>
<script type="math/tex; mode=display">
L(y_1,F_{m-1}(x_1)+\gamma)=-y_1(F_{m-1}(x_1)+\gamma)+log(1+e^{F_{m-1}(x_1)+\gamma})</script><p>这个式子就是上面推导的𝑙<em>l</em>， 因为我们要用回归树做分类， 所以这里把分类的预测概率转换成了对数几率回归的形式， 即$log(\eta_i)$， 这个就是模型的回归输出值。而如果求这个损失的最小值， 我们要求导， 解出令损失最小的$\gamma$。 但是上面这个式子求导会很麻烦， 所以这里介绍了一个技巧就是<strong>使用二阶泰勒公式来近似表示该式， 再求导</strong>， 还记得伟大的泰勒吗？</p>
<script type="math/tex; mode=display">
f(x+\Delta x) \approx f(x) + \Delta f'(x) + \frac{1}{2} \Delta x^2 f''(x) + O(\Delta x)</script><p>这里就相当于把$L(y<em>1,F</em>{m-1}(x_1))$当作常量$f(x)$，$\gamma$ 作为变量$\Delta x$，将$f(x)$二阶展开：</p>
<script type="math/tex; mode=display">
L(y_1,F_{m-1}(x_1) + \gamma) \approx L(y_1,F_{m-1}(x_1)) + L'(y_1,F_{m-1}(x_1)) \gamma + \frac{1}{2}L''(y_1,F_{m-1}(x_1)) \gamma^2</script><p>这时候再求导就简单了</p>
<script type="math/tex; mode=display">
\frac{dL}{d\gamma}=L'(y_1,F_{m-1}(x_1)) + L''(y_1,F_{m-1}(x_1))\gamma</script><p>Loss最小的时候， 上面的式子等于0， 就可以得到$\gamma$:</p>
<script type="math/tex; mode=display">
\gamma_{11}=\frac{-L'(y_1,F_{m-1}(x_1))}{L''(y_1,F_{m-1}(x_1))\gamma}</script><p><strong>因为分子就是残差(上述已经求到了)， 分母可以通过对残差求导，得到原损失函数的二阶导：</strong></p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/29/TXCVQtEjUomghYB.png" alt="image-20240429114857610"></p>
</li>
</ol>
</li>
</ol>
<pre><code>  这时候， 就可以算出该节点的输出
  $$
  \gamma_&#123;11&#125;=\frac&#123;r_&#123;11&#125;&#125;&#123;p_&#123;10&#125;(1-p_&#123;10&#125;)&#125;=\frac&#123;0.33&#125;&#123;0.67 * 0.33&#125;=1.49
  $$
  这里的下面$\gamma_&#123;jm&#125;$表示第$m$棵树的第$j$个叶子节点。 接下来是右边节点的输出， 包含样本2和样本3， 同样使用二阶泰勒公式展开：

  ![image-20240429115219850](https://s2.loli.net/2024/04/29/kMBUL7gdFIsH5TJ.png)
</code></pre><p>​                求导， 令其结果为0，就会得到， 第1棵树的第2个叶子节点的输出：</p>
<p>​                <img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/29/EqmsJanTiGpb4yt.png" alt="image-20240429115351409"></p>
<p>​                可以看出， 对于任意叶子节点， 我们可以直接计算其输出值：</p>
<p>​                </p>
<script type="math/tex; mode=display">
\gamma_{jm} = \frac{\sum_{i=1}^{R_{ij}}r_{im}}{\sum_{i=1}^{R_{ij}}p_{i,m-1}(1-p_{i,m-1})}</script><ol>
<li>更新模型$F_m(x)$</li>
</ol>
<p>​                </p>
<script type="math/tex; mode=display">
F_m(x) = F_{m-1}(x) + v \sum_{j=1}^{J_m} \gamma_m</script><p><strong>下面分析一下GBDT的优缺点：</strong></p>
<p>我们可以把树的生成过程理解成<strong>自动进行多维度的特征组合</strong>的过程，从根结点到叶子节点上的整个路径(多个特征值判断)，才能最终决定一棵树的预测值， 另外，对于<strong>连续型特征</strong>的处理，GBDT 可以拆分出一个临界阈值，比如大于 0.027 走左子树，小于等于 0.027（或者 default 值）走右子树，这样很好的规避了人工离散化的问题。这样就非常轻松的解决了逻辑回归那里<strong>自动发现特征并进行有效组合</strong>的问题， 这也是GBDT的优势所在。</p>
<p>但是GBDT也会有一些局限性， 对于<strong>海量的 id 类特征</strong>，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储；另外海量特征在也会存在性能瓶颈，当 GBDT 的 one hot 特征大于 10 万维时，就必须做分布式的训练才能保证不爆内存。所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有<strong>信息损失</strong>，对于头部资源不能有效的表达。</p>
<p>所以， 我们发现其实<strong>GBDT和LR的优缺点可以进行互补</strong>。</p>
<h4 id="GBDT-LR模型"><a href="#GBDT-LR模型" class="headerlink" title="GBDT+LR模型"></a>GBDT+LR模型</h4><p>2014年， Facebook提出了一种利用GBDT自动进行特征筛选和组合， 进而生成新的离散特征向量， 再把该特征向量当做LR模型的输入， 来产生最后的预测结果， 这就是著名的GBDT+LR模型了。GBDT+LR 使用最广泛的场景是CTR点击率预估，即预测当给用户推送的广告会不会被用户点击。</p>
<h4 id="DeepFM"><a href="#DeepFM" class="headerlink" title="DeepFM"></a>DeepFM</h4><h4 id="动机"><a href="#动机" class="headerlink" title="动机"></a>动机</h4><p>对于CTR问题，被证明的最有效的提升任务表现的策略是特征组合(Feature Interaction), 在CTR问题的探究历史上来看就是如何更好地学习特征组合，进而更加精确地描述数据的特点。可以说这是基础推荐模型到深度学习推荐模型遵循的一个主要的思想。而组合特征大牛们研究过组合二阶特征，三阶甚至更高阶，但是面临一个问题就是随着阶数的提升，复杂度就成几何倍的升高。这样即使模型的表现更好了，但是推荐系统在实时性的要求也不能满足了。所以很多模型的出现都是为了解决另外一个更加深入的问题：如何更高效的学习特征组合？</p>
<p>为了解决上述问题，出现了FM和FFM来优化LR的特征组合较差这一个问题。并且在这个时候科学家们已经发现了DNN在特征组合方面的优势，所以又出现了FNN和PNN等使用深度网络的模型。但是DNN也存在局限性。</p>
<ul>
<li><strong>DNN局限</strong> 当我们使用DNN网络解决推荐问题的时候存在网络参数过于庞大的问题，这是因为在进行特征处理的时候我们需要使用one-hot编码来处理离散特征，这会导致输入的维度猛增。这里借用AI大会的一张图片：</li>
</ul>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/20/qpC37U1iFvQArsD.png" alt="image-20240420184743955"></p>
<p>这样庞大的参数量也是不实际的。为了解决DNN参数量过大的局限性，可以采用非常经典的Field思想，将OneHot特征转换为Dense Vector</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/20/RIcVbTMsCmjkwWO.png" alt="image-20240420184904998"></p>
<p>此时通过增加全连接层就可以实现高阶的特征组合，如下图所示：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/20/ErF3zNLtidIesAv.png" alt="image-20240420185002554"></p>
<p>但是仍然缺少低阶的特征组合，于是增加FM来表示低阶的特征组合。</p>
<ul>
<li><strong>FNN和PNN</strong> 结合FM和DNN其实有两种方式，可以并行结合也可以串行结合。这两种方式各有几种代表模型。在DeepFM之前有FNN，虽然在影响力上可能并不如DeepFM，但是了解FNN的思想对我们理解DeepFM的特点和优点是很有帮助的。</li>
</ul>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/04/20/fDnehZHYQj91dLM.png" alt="image-20240420185121873"></p>
<p>FNN是使用预训练好的FM模块，得到隐向量，然后把隐向量作为DNN的输入，但是经过实验进一步发现，在Embedding layer和hidden layer1之间增加一个product层（如上图所示）可以提高模型的表现，所以提出了PNN，使用product layer替换FM预训练层。</p>
<ul>
<li><strong>Wide&amp;Deep</strong> FNN和PNN模型仍然有一个比较明显的尚未解决的缺点：对于低阶组合特征学习到的比较少，这一点主要是由于FM和DNN的串行方式导致的，也就是虽然FM学到了低阶特征组合，但是DNN的全连接结构导致低阶特征并不能在DNN的输出端较好的表现。看来我们已经找到问题了，将串行方式改进为并行方式能比较好的解决这个问题。于是Google提出了Wide&amp;Deep模型（将前几章），但是如果深入探究Wide&amp;Deep的构成方式，虽然将整个模型的结构调整为了并行结构，在实际的使用中Wide Module中的部分需要较为精巧的特征工程，换句话说人工处理对于模型的效果具有比较大的影响（这一点可以在Wide&amp;Deep模型部分得到验证）。</li>
</ul>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="C:/Users/YL.YL.000/AppData/Roaming/Typora/typora-user-images/image-20240420223257343.png" alt="image-20240420223257343"></p>
<h3 id="2-2-2特征交叉"><a href="#2-2-2特征交叉" class="headerlink" title="2.2.2特征交叉"></a>2.2.2特征交叉</h3><h2 id="DCN"><a href="#DCN" class="headerlink" title="DCN"></a>DCN</h2><h2 id="动机-1"><a href="#动机-1" class="headerlink" title="动机"></a>动机</h2><p>Wide&amp;Deep模型的提出不仅综合了“记忆能力”和“泛化能力”， 而且开启了不同网络结构融合的新思路。 所以后面就有各式各样的模型改进Wide部分或者Deep部分， 而Deep&amp;Cross模型(DCN)就是其中比较典型的一个，这是2017年斯坦福大学和谷歌的研究人员在ADKDD会议上提出的， 该模型针对W&amp;D的wide部分进行了改进， 因为Wide部分有一个不足就是需要人工进行特征的组合筛选， 过程繁琐且需要经验， 而2阶的FM模型在线性的时间复杂度中自动进行特征交互，但是这些特征交互的表现能力并不够，并且随着阶数的上升，模型复杂度会大幅度提高。于是乎，作者用一个Cross Network替换掉了Wide部分，来自动进行特征之间的交叉，并且网络的时间和空间复杂度都是线性的。 通过与Deep部分相结合，构成了深度交叉网络（Deep &amp; Cross Network），简称DCN。</p>
<h2 id="模型结构及原理"><a href="#模型结构及原理" class="headerlink" title="模型结构及原理"></a>模型结构及原理</h2><p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/05/02/1kzsxSruqDcEl2B.png" alt="image-20240502131845415"></p>
<p>这个模型的结构也是比较简洁的， 从下到上依次为：Embedding和Stacking层， Cross网络层与Deep网络层并列， 以及最后的输出层。下面也是一一为大家剖析。</p>
<h2 id="Embedding和Stacking层"><a href="#Embedding和Stacking层" class="headerlink" title="Embedding和Stacking层"></a>Embedding和Stacking层</h2><p>Embedding层我们已经非常的熟悉了吧， 这里的作用依然是把稀疏离散的类别型特征变成低维密集型。</p>
<script type="math/tex; mode=display">
X_{embed,i}=W_{embed,iX_i}</script><p>其中对于某一类稀疏分类特征（如id），$X<em>{embed,i}$是第$i$个分类值（id序号）的embedding向量。$W</em>{embed,i}$是embedding矩阵，$n_e \times n_v$维度，$n_e$是embedding维度，$n_v$是该类特征的唯一取值个数。$x_i$属于该特征的二元稀疏向量(one-hot)编码的。【实质上就是在训练得到的Embedding参数矩阵中找到属于当前样本对应的Embedding向量】。其实绝大多数基于深度学习的推荐模型都需要Embedding操作，参数学习是通过神经网络进行训练。</p>
<p>最后，该层需要将所有的密集型特征与通过embedding转换后的特征进行联合（Stacking）：</p>
<script type="math/tex; mode=display">
X_0=[X^T_{embed,1},...,X^T_{embed,k},X^T_{dense}]</script><p>一共𝑘<em>k</em>个类别特征， dense是数值型特征， 两者在特征维度拼在一块。 上面的这两个操作如果是看了前面的模型的话，应该非常容易理解了。</p>
<h2 id="Cross-NetWork"><a href="#Cross-NetWork" class="headerlink" title="Cross NetWork"></a>Cross NetWork</h2><p>这个就是本模型最大的亮点了【Cross网络】， 这个思路感觉非常Nice。设计该网络的目的是增加特征之间的交互力度。交叉网络由多个交叉层组成， 假设第$l$层的输出向量$x<em>l$， 那么对于第$l+1$层的输出向量$x</em>{l+1}$表示为：</p>
<script type="math/tex; mode=display">
x_{l+1}=x_0x_l^Tw_l+b_l+x_l=f(x_l,w_l,b_l) + x_l</script><p>可以看到， 交叉层的二阶部分非常类似PNN提到的外积操作， 在此基础上增加了外积操作的权重向量$w_l$， 以及原输入向量和$x_l$偏置向量$b_l$。 交叉层的可视化如下：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/05/02/ui65yf3hXp1zjE4.png" alt="image-20240502133426935"></p>
<p>可以看到， 每一层增加了一个$n$维的权重向量$w_l$（n表示输入向量维度）， 并且在每一层均保留了输入向量， 因此输入和输出之间的变化不会特别明显。关于这一层， 原论文里面有个具体的证明推导Cross Network为啥有效， 不过比较复杂，这里我拿一个式子简单的解释下上面这个公式的伟大之处：</p>
<blockquote>
<p><strong>我们根据上面这个公式， 尝试的写前面几层看看:</strong></p>
<p>$l=0:\mathbf{x_1}=\mathbf{x_0}\mathbf{x_0^T}\mathbf{w_0}+\mathbf{b_0}+\mathbf{x_0}$</p>
<p>$l=1:\mathbf{x_2}=\mathbf{x_0}\mathbf{x_1^T}\mathbf{w_1}+\mathbf{b_1}+\mathbf{x_1}=\mathbf{x_0}[\mathbf{x_0}\mathbf{x_0^T}\mathbf{w_0}+\mathbf{b_0}+\mathbf{x_0}]^T\mathbf{w_1}+\mathbf{b_1}+\mathbf{x_1}$</p>
<p>$l=2:\mathbf{x_3}=\mathbf{x_0}\mathbf{x_2^T}\mathbf{w_2}+\mathbf{b_2}+\mathbf{x_2}=\mathbf{x_0}[\mathbf{x_0}[\mathbf{x_0}\mathbf{x_0^T}\mathbf{w_0}+\mathbf{b_0}+\mathbf{x_0}]^T\mathbf{w_1}+\mathbf{b_1}+\mathbf{x_1}]^T\mathbf{w_2}+\mathbf{b_2}+\mathbf{x_2}$</p>
</blockquote>
<p>我们暂且写到第3层的计算， 我们会发现什么结论呢？ 给大家总结一下：</p>
<ol>
<li><p>$x_1$中包含了所有的$x_0$的1,2阶特征的交互，$x_2$包含了所有的$x_1,x_0$的1、2、3阶特征的交互，$x_3$中包含了所有的$x_2,x_1,x_0$的交互，$x_0$的1、2、3、4阶特征交互。 因此， 交叉网络层的叉乘阶数是有限的。<strong>第$l$层特征对应的最高的叉乘阶数$l+1$</strong></p>
</li>
<li><p>Cross网络的参数是共享的， 每一层的这个权重特征之间共享， 这个可以使得模型泛化到看不见的特征交互作用， 并且对噪声更具有鲁棒性。 例如两个稀疏的特征$x_i,x_j$，它们在数据中几乎不发生交互，那么学习$x_i,x_j$的权重对于预测没有任何的意义。</p>
</li>
<li><p>计算交叉网络的参数数量。 假设交叉层的数量是$L_c$，特征$x$的维度是$n$， 那么总共的参数是：</p>
<script type="math/tex; mode=display">
n \times L_c \times 2</script><p>这个就是每一层会有$w$和$b$。且$w$维度和$x$的维度是一致的。</p>
</li>
<li><p>交叉网络的时间和空间复杂度是线性的。这是因为， 每一层都只有$w$和$b$ 没有激活函数的存在，相对于深度学习网络， 交叉网络的复杂性可以忽略不计。</p>
</li>
<li><p>Cross网络是FM的泛化形式， 在FM模型中， 特征$x<em>i$的权重是$v_i$，那么交叉项$x_i,x_j$的权重为$\langle x_i,x_j \rangle$。在DCN中，$x_i$的权重为${W</em>{K}^{(i)^l}}<em>{k=1}$， 交叉项$x_i,x_j$的权重是参数${W</em>{K}^{(i)^l}}<em>{k=1}$和${W</em>{K}^{(j)^l}}_{k=1}$的乘积，这个看上面那个例子展开感受下。因此两个模型都各自学习了独立于其他特征的一些参数，并且交叉项的权重是相应参数的某种组合。FM只局限于2阶的特征交叉(一般)，而DCN可以构建更高阶的特征交互， 阶数由网络深度决定，并且交叉网络的参数只依据输入的维度线性增长。</p>
</li>
<li><p>还有一点我们也要了解，对于每一层的计算中， 都会跟着$x_0$, 这个是咱们的原始输入， 之所以会乘以一个这个，是为了保证后面不管怎么交叉，都不能偏离我们的原始输入太远，别最后交叉交叉都跑偏了。</p>
</li>
<li><p>$\mathbf{x_{l+1}}=f(\mathbf{x_l},\mathbf{w_l}.\mathbf{b_l})+\mathbf{x_l}$, 这个东西其实有点跳远连接的意思，也就是和ResNet也有点相似，无形之中还能有效的缓解梯度消失现象。</p>
</li>
<li><p>好了， 关于本模型的交叉网络的细节就介绍到这里了。这应该也是本模型的精华之处了，后面就简单了。</p>
</li>
</ol>
<h2 id="Deep-Network"><a href="#Deep-Network" class="headerlink" title="Deep Network"></a>Deep Network</h2><p>这个就和上面的D&amp;W的全连接层原理一样。这里不再过多的赘述。</p>
<script type="math/tex; mode=display">
\mathbf{h_{l+1}}=f(w_l\mathbf{h_l}+\mathbf{b_l})</script><p>具体的可以参考W&amp;D模型。</p>
<h2 id="组合输出层"><a href="#组合输出层" class="headerlink" title="组合输出层"></a>组合输出层</h2><p>这个层负责将两个网络的输出进行拼接， 并且通过简单的Logistics回归完成最后的预测：</p>
<script type="math/tex; mode=display">
p=\sigma([\mathbf{x_{L_1}^T},\mathbf{h_{L_2}^T}]\mathbf{w}_{logits})</script><p>其中$\mathbf{x<em>{L_1}^T},\mathbf{h</em>{L_2}^T}$分别表示交叉网络和深度网络的输出。 最后二分类的损失函数依然是交叉熵损失：</p>
<script type="math/tex; mode=display">
loss=-\frac{1}{N}\sum_{i=1}^Ny_i\log(p_i)+(1-y_i)\log(1-p_i)+\lambda \sum_l ||\mathbf{w}_i||^2</script><p>Cross&amp;Deep模型的原理就是这些了，其核心部分就是Cross Network， 这个可以进行特征的自动交叉， 避免了更多基于业务理解的人工特征组合。 该模型相比于W&amp;D，Cross部分表达能力更强， 使得模型具备了更强的非线性学习能力。</p>
<h3 id="2-2-4序列模型"><a href="#2-2-4序列模型" class="headerlink" title="2.2.4序列模型"></a>2.2.4序列模型</h3><h2 id="DIN"><a href="#DIN" class="headerlink" title="DIN"></a>DIN</h2><h2 id="动机-2"><a href="#动机-2" class="headerlink" title="动机"></a>动机</h2><p>Deep Interest Network(DIIN)是2018年阿里巴巴提出来的模型， 该模型基于业务的观察，从实际应用的角度进行改进，相比于之前很多“学术风”的深度模型， 该模型更加具有业务气息。该模型的应用场景是阿里巴巴的电商广告推荐业务， 这样的场景下一般<strong>会有大量的用户历史行为信息</strong>， 这个其实是很关键的，因为DIN模型的创新点或者解决的问题就是使用了注意力机制来对用户的兴趣动态模拟， 而这个模拟过程存在的前提就是用户之前有大量的历史行为了，这样我们在预测某个商品广告用户是否点击的时候，就可以参考他之前购买过或者查看过的商品，这样就能猜测出用户的大致兴趣来，这样我们的推荐才能做的更加到位，所以这个模型的使用场景是<strong>非常注重用户的历史行为特征（历史购买过的商品或者类别信息）</strong>，也希望通过这一点，能够和前面的一些深度学习模型对比一下。</p>
<p>在个性化的电商广告推荐业务场景中，也正式由于用户留下了大量的历史交互行为，才更加看出了之前的深度学习模型(作者统称Embeding&amp;MLP模型)的不足之处。如果学习了前面的各种深度学习模型，就会发现Embeding&amp;MLP模型对于这种推荐任务一般有着差不多的固定处理套路，就是大量稀疏特征先经过embedding层， 转成低维稠密的，然后进行拼接，最后喂入到多层神经网络中去。</p>
<p>这些模型在这种个性化广告点击预测任务中存在的问题就是<strong>无法表达用户广泛的兴趣</strong>，因为这些模型在得到各个特征的embedding之后，就蛮力拼接了，然后就各种交叉等。这时候根本没有考虑之前用户历史行为商品具体是什么，究竟用户历史行为中的哪个会对当前的点击预测带来积极的作用。 而实际上，对于用户点不点击当前的商品广告，很大程度上是依赖于他的历史行为的，王喆老师举了个例子</p>
<blockquote>
<p>假设广告中的商品是键盘， 如果用户历史点击的商品中有化妆品， 包包，衣服， 洗面奶等商品， 那么大概率上该用户可能是对键盘不感兴趣的， 而如果用户历史行为中的商品有鼠标， 电脑，iPad，手机等， 那么大概率该用户对键盘是感兴趣的， 而如果用户历史商品中有鼠标， 化妆品， T-shirt和洗面奶， 鼠标这个商品embedding对预测“键盘”广告的点击率的重要程度应该大于后面的那三个。</p>
</blockquote>
<p>这里也就是说如果是之前的那些深度学习模型，是没法很好的去表达出用户这广泛多样的兴趣的，如果想表达的准确些， 那么就得加大隐向量的维度，让每个特征的信息更加丰富， 那这样带来的问题就是计算量上去了，毕竟真实情景尤其是电商广告推荐的场景，特征维度的规模是非常大的。 并且根据上面的例子， 也<strong>并不是用户所有的历史行为特征都会对某个商品广告点击预测起到作用</strong>。所以对于当前某个商品广告的点击预测任务，没必要考虑之前所有的用户历史行为。</p>
<p>这样， DIN的动机就出来了，在业务的角度，我们应该自适应的去捕捉用户的兴趣变化，这样才能较为准确的实施广告推荐；而放到模型的角度， 我们应该<strong>考虑到用户的历史行为商品与当前商品广告的一个关联性</strong>，如果用户历史商品中很多与当前商品关联，那么说明该商品可能符合用户的品味，就把该广告推荐给他。而一谈到关联性的话， 我们就容易想到“注意力”的思想了， 所以为了更好的从用户的历史行为中学习到与当前商品广告的关联性，学习到用户的兴趣变化， 作者把注意力引入到了模型，设计了一个”local activation unit”结构，利用候选商品和历史问题商品之间的相关性计算出权重，这个就代表了对于当前商品广告的预测，用户历史行为的各个商品的重要程度大小， 而加入了注意力权重的深度学习网络，就是这次的主角DIN， 下面具体来看下该模型。</p>
<h2 id="DIN模型结构及原理"><a href="#DIN模型结构及原理" class="headerlink" title="DIN模型结构及原理"></a>DIN模型结构及原理</h2><p>在具体分析DIN模型之前， 我们还得先介绍两块小内容，一个是DIN模型的数据集和特征表示， 一个是上面提到的之前深度学习模型的基线模型， 有了这两个， 再看DIN模型，就感觉是水到渠成了。</p>
<h2 id="特征表示"><a href="#特征表示" class="headerlink" title="特征表示"></a>特征表示</h2><p>工业上的CTR预测数据集一般都是<code>multi-group categorial form</code>的形式，就是类别型特征最为常见，这种数据集一般长这样：</p>
<p>这里的亮点就是框出来的那个特征，这个包含着丰富的用户兴趣信息。</p>
<p>对于特征编码，作者这里举了个例子：<code>[weekday=Friday, gender=Female, visited_cate_ids=&#123;Bag,Book&#125;, ad_cate_id=Book]</code>， 这种情况我们知道一般是通过one-hot的形式对其编码， 转成系数的二值特征的形式。但是这里我们会发现一个<code>visted_cate_ids</code>， 也就是用户的历史商品列表， 对于某个用户来讲，这个值是个多值型的特征， 而且还要知道这个特征的长度不一样长，也就是用户购买的历史商品个数不一样多，这个显然。这个特征的话，我们一般是用到multi-hot编码，也就是可能不止1个1了，有哪个商品，对应位置就是1， 所以经过编码后的数据长下面这个样子：</p>
<p>这个就是喂入模型的数据格式了，这里还要注意一点 就是上面的特征里面没有任何的交互组合，也就是没有做特征交叉。这个交互信息交给后面的神经网络去学习。</p>
<h2 id="基线模型"><a href="#基线模型" class="headerlink" title="基线模型"></a>基线模型</h2><p>这里的base 模型，就是上面提到过的Embedding&amp;MLP的形式， 这个之所以要介绍，就是因为DIN网络的基准也是他，只不过在这个的基础上添加了一个新结构(注意力网络)来学习当前候选广告与用户历史行为特征的相关性，从而动态捕捉用户的兴趣。</p>
<p>基准模型的结构相对比较简单，我们前面也一直用这个基准， 分为三大模块：Embedding layer，Pooling &amp; Concat layer和MLP， 结构如下:</p>
<p>前面的大部分深度模型结构也是遵循着这个范式套路， 简介一下各个模块。</p>
<ol>
<li><p><strong>Embedding layer</strong>：这个层的作用是把高维稀疏的输入转成低维稠密向量， 每个离散特征下面都会对应着一个embedding词典， 维度是$D \times K$， 这里的$D$表示的是隐向量的维度， 而表$K$示的是当前离散特征的唯一取值个数, 这里为了好理解，这里举个例子说明，就比如上面的weekday特征：</p>
<blockquote>
<p>假设某个用户的weekday特征就是周五，化成one-hot编码的时候，就是[0,0,0,0,1,0,0]表示，这里如果再假设隐向量维度是D， 那么这个特征对应的embedding词典是一个$D\times7$的一个矩阵(每一列代表一个embedding，7列正好7个embedding向量，对应周一到周日)，那么该用户这个one-hot向量经过embedding层之后会得到一个𝐷×1<em>D</em>×1的向量，也就是周五对应的那个embedding，怎么算的，其实就是$embedding矩阵*[0,0,0,0,1,0,0]^T$。其实也就是直接把embedding矩阵中one-hot向量为1的那个位置的embedding向量拿出来。 这样就得到了稀疏特征的稠密向量了。其他离散特征也是同理，只不过上面那个multi-hot编码的那个，会得到一个embedding向量的列表，因为他开始的那个multi-hot向量不止有一个是1，这样乘以embedding矩阵，就会得到一个列表了。通过这个层，上面的输入特征都可以拿到相应的稠密embedding向量了。</p>
</blockquote>
</li>
<li><p><strong>pooling layer and Concat layer</strong>： pooling层的作用是将用户的历史行为embedding这个最终变成一个定长的向量，因为每个用户历史购买的商品数是不一样的， 也就是每个用户multi-hot中1的个数不一致，这样经过embedding层，得到的用户历史行为embedding的个数不一样多，也就是上面的embedding列表$t_i$不一样长， 那么这样的话，每个用户的历史行为特征拼起来就不一样长了。 而后面如果加全连接网络的话，我们知道，他需要定长的特征输入。 所以往往用一个pooling layer先把用户历史行为embedding变成固定长度(统一长度)，所以有了这个公式：</p>
<script type="math/tex; mode=display">
e_i=pooling(e_{i1},e_{i2},...,e_{ik})</script><p>这里的$e_{ij}$是用户历史行为的那些embedding。就$e_i$变成了定长的向量， 这里的$e_i$表示第$i$个历史特征组(是历史行为，比如历史的商品id，历史的商品类别id等)， 这里的$k$表示对应历史特种组里面用户购买过的商品数量，也就是历史embedding的数量，看上面图里面的user behaviors系列，就是那个过程了。 Concat layer层的作用就是拼接了，就是把这所有的特征embedding向量，如果再有连续特征的话也算上，从特征维度拼接整合，作为MLP的输入。</p>
</li>
<li><p><strong>MLP</strong>：这个就是普通的全连接，用了学习特征之间的各种交互。</p>
</li>
<li><p><strong>Loss</strong>: 由于这里是点击率预测任务， 二分类的问题，所以这里的损失函数用的负的log对数似然：</p>
<script type="math/tex; mode=display">
L=-\frac{1}{N}\sum_{(\mathbf{x},y)\in S}(y \log p(x) + (1-y) \log (1-p(\mathbf(x))))</script></li>
</ol>
<p>这就是base 模型的全貌， 这里应该能看出这种模型的问题， 通过上面的图也能看出来， 用户的历史行为特征和当前的候选广告特征在全都拼起来给神经网络之前，是一点交互的过程都没有， 而拼起来之后给神经网络，虽然是有了交互了，但是原来的一些信息，比如，<strong>每个历史商品的信息会丢失了一部分，因为这个与当前候选广告商品交互的是池化后的历史特征embedding， 这个embedding是综合了所有的历史商品信息， 这个通过我们前面的分析，对于预测当前广告点击率，并不是所有历史商品都有用，综合所有的商品信息反而会增加一些噪声性的信息</strong>，可以联想上面举得那个键盘鼠标的例子，如果加上了各种洗面奶，衣服啥的反而会起到反作用。其次就是这样综合起来，已经没法再看出到底用户历史行为中的哪个商品与当前商品比较相关，也就是丢失了历史行为中各个商品对当前预测的重要性程度。最后一点就是如果所有用户浏览过的历史行为商品，最后都通过embedding和pooling转换成了固定长度的embedding，这样会限制模型学习用户的多样化兴趣。</p>
<p>那么改进这个问题的思路有哪些呢？ 第一个就是加大embedding的维度，增加之前各个商品的表达能力，这样即使综合起来，embedding的表达能力也会加强， 能够蕴涵用户的兴趣信息，但是这个在大规模的真实推荐场景计算量超级大，不可取。 另外一个思路就是<strong>在当前候选广告和用户的历史行为之间引入注意力的机制</strong>，这样在预测当前广告是否点击的时候，让模型更关注于与当前广告相关的那些用户历史产品，也就是说<strong>与当前商品更加相关的历史行为更能促进用户的点击行为</strong>。 作者这里又举了之前的一个例子：</p>
<blockquote>
<p><strong>想象一下，当一个年轻母亲访问电子商务网站时，她发现展示的新手袋很可爱，就点击它。让我们来分析一下点击行为的驱动力。</strong></p>
<p><strong>展示的广告通过软搜索这位年轻母亲的历史行为，发现她最近曾浏览过类似的商品，如大手提袋和皮包，从而击中了她的相关兴趣</strong></p>
</blockquote>
<p>第二个思路就是DIN的改进之处了。DIN通过给定一个候选广告，然后去注意与该广告相关的局部兴趣的表示来模拟此过程。 <strong>DIN不会通过使用同一向量来表达所有用户的不同兴趣，而是通过考虑历史行为的相关性来自适应地计算用户兴趣的表示向量（对于给的广告）。 该表示向量随不同广告而变化。</strong>下面看一下DIN模型。</p>
<h2 id="DIN模型架构"><a href="#DIN模型架构" class="headerlink" title="DIN模型架构"></a>DIN模型架构</h2><p>上面分析完了base模型的不足和改进思路之后，DIN模型的结构就呼之欲出了，首先，它依然是采用了基模型的结构，只不过是在这个的基础上加了一个注意力机制来学习用户兴趣与当前候选广告间的关联程度， 用论文里面的话是，引入了一个新的<code>local activation unit</code>， 这个东西用在了用户历史行为特征上面， <strong>能够根据用户历史行为特征和当前广告的相关性给用户历史行为特征embedding进行加权</strong>。我们先看一下它的结构，然后看一下这个加权公式。</p>
<p>这里改进的地方已经框出来了，这里会发现相比于base model， 这里加了一个local activation unit， 这里面是一个前馈神经网络，输入是用户历史行为商品和当前的候选商品， 输出是它俩之间的相关性， 这个相关性相当于每个历史商品的权重，把这个权重与原来的历史行为embedding相乘求和就得到了用户的兴趣表示𝑣𝑈(𝐴)<strong>v*</strong>U<em>(</em>A*), 这个东西的计算公式如下：</p>
<h1 id="第四章-推荐系统算法面经"><a href="#第四章-推荐系统算法面经" class="headerlink" title="第四章 推荐系统算法面经"></a>第四章 推荐系统算法面经</h1><h2 id="4-1ML与DL基础"><a href="#4-1ML与DL基础" class="headerlink" title="4.1ML与DL基础"></a>4.1ML与DL基础</h2><h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><ol>
<li>介绍一个最熟悉的机器学习算法</li>
</ol>
<h2 id="参考解析"><a href="#参考解析" class="headerlink" title="参考解析"></a>参考解析</h2><h3 id="机器学习-1"><a href="#机器学习-1" class="headerlink" title="机器学习"></a>机器学习</h3><ol>
<li><p>介绍一个最熟悉的机器学习算法</p>
<ul>
<li><p>LR：逻辑回归是假设数据服从<strong>伯努利分布</strong>，通过<strong>极大似然估计方法</strong>，使用<strong>梯度下降</strong>来求解参数，达到二分类目的的一个模型。我们在考虑把广义线性模型用于分类的时候，需要如何确定逻辑边界，感知机模型用的是阶跃函数，但是阶跃函数不可导，不能作为广义线性模型的联系函数。逻辑回归对数几率函数代替阶跃函数。因为对数几率函数是单调可微的一个函数，所以可以作为联系函数。所以逻辑回归本质上还是广义线性模型。</p>
</li>
<li><p>LR的优缺点：</p>
<ul>
<li>形式简单，可解释性好；</li>
<li>它直接对分类概率进行建模，不需要知道真实数据的分布，这和生成式模型相区别，避免了假设错误带来的问题；</li>
<li>不仅能够预测出类别，还能够预测出概率，能够用于很多场景，比如ctr排序中；</li>
<li>对数几率函数任意阶数可导，能够很容易优化；</li>
<li>可以获得特征权重，方便我们进行特征筛选；</li>
<li>训练速度快；</li>
<li>它对稀疏特征效果比较好，因为使用的是w1 w2 w3本质上的线性模型，稀疏数据能够筛选出不稀疏的重要特征。</li>
<li>模型表达能力有限；</li>
<li>样本不均衡很难处理；</li>
<li>在非线性可分数据集上性能有限；</li>
</ul>
</li>
<li><p>LR推导：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://s2.loli.net/2024/05/02/AH9h3XGcQL6nBFr.png" alt="image-20240502234225932"></p>
</li>
</ul>
</li>
<li><p>决策树怎么建树，基尼系数公式</p>
<ul>
<li><p>决策树建树算法有三种ID3、C4.5、CART，每个算法主要考虑的事情主要有三个问题：</p>
<ul>
<li>选什么特征来当条件？</li>
<li>条件判断的属性值是什么？</li>
<li>什么时候停止分裂，达到我们需要的决策？</li>
</ul>
</li>
<li><p>CART</p>
<ul>
<li><p>CART树采用基尼系数进行最优特征的选择，构造过程中假设有K类，则样本属于第K类的概率为pk，则定义样本分布的<strong>基尼系数</strong>为：</p>
<script type="math/tex; mode=display">
Gini(p)=\sum_{k=1}^m p_k(1-p_k)=1-\sum_{k=1}^K p_k^2</script><p>根据基尼系数定义，可以得到样本集合D的<strong>基尼指数</strong>，其中ck表述样本集合中第k类的子集：</p>
<script type="math/tex; mode=display">
Gini(D)=1-\sum_{k=1}^K(\frac{C_k}{D})^2</script><p>如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。<strong>基尼指数越大，样本集合的不确定性越大</strong>。</p>
<script type="math/tex; mode=display">
Gain\_Gini(D,A)=\frac{D_1}{D}Gini(D_1)+\frac{D_2}{D}Gini(D_2)</script><blockquote>
<p>选什么特征作为最优特征分割：当我们计算完所有特征基尼指数后，选择其中最小所在特征的作为分裂特征；</p>
<p>条件判断的属性值是什么：判断特征属性是否为为此最指数来分裂；</p>
<p>什么时候停止分裂，达到我们需要的决策：分裂的最小收益小于我们的划定的阈值，或者树的深度达到我们的阈值。</p>
</blockquote>
</li>
</ul>
</li>
</ul>
</li>
<li><p>Adaboost拟合目标是什么</p>
<ul>
<li>Adaboost中每训练完一个弱分类器都就会调整权重，上一轮训练中被误分类的点的权重会增加，在本轮训练中，由于权重影响，本轮的弱分类器将更有可能把上一轮的误分类点分对，如果还是没有分对，那么分错的点的权重将继续增加，下一个弱分类器将更加关注这个点，这是adaboost目标。</li>
</ul>
</li>
<li><p>Adaboost介绍一下，每个基学习器的权重怎么得到的</p>
</li>
<li><p>介绍下GBDT</p>
</li>
<li><p>介绍XGBoost</p>
</li>
<li><p>介绍下LightGBM</p>
</li>
<li><p>LightGBM相对于XGBoost的改进</p>
</li>
<li><p>GBDT中的梯度是什么，怎么用</p>
</li>
<li><p>GBDT如何计算特征重要性</p>
</li>
<li><p>GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差</p>
</li>
</ol>
<hr>
<p>references:</p>
<ul>
<li>‘<a target="_blank" rel="noopener" href="https://datawhalechina.github.io/fun-rec/#/ch02/ch2.1/ch2.1.1/usercf">datawhale-FunRec)</a>‘</li>
<li><a target="_blank" rel="noopener" href="https://mp.weixin.qq.com/s/ZtnaQrVIpVOPJpqMdLWOcw">https://mp.weixin.qq.com/s/ZtnaQrVIpVOPJpqMdLWOcw</a></li>
<li><a target="_blank" rel="noopener" href="https://chenk.tech/posts/8ad63d9d.html">https://chenk.tech/posts/8ad63d9d.html</a></li>
<li>B站黑马推荐系统实战课程</li>
</ul>
<hr>

<div class="article-footer fs14">
    <section id="license">
      <div class="header"><span>许可协议</span></div>
      <div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div>
    </section>
    
    <section id="share">
      <div class="header"><span>分享文章</span></div>
      <div class="body">
        <div class="link"><input class="copy-area" readonly="true" id="copy-link" value="http://humble2967738843.github.io/2024/04/10/tui-jian-xi-tong/" /></div>
        <div class="social-wrap dis-select"><a class="social share-item wechat" onclick="util.toggle(&quot;qrcode-wechat&quot)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/b32ef3da1162a.svg" /></a><a class="social share-item weibo" target="_blank" rel="external nofollow noopener noreferrer" href="https://service.weibo.com/share/share.php?url=http://humble2967738843.github.io/2024/04/10/tui-jian-xi-tong/&title=推荐系统 - humbleyl&summary=第一章 推荐系统概述1.1推荐系统的意义推荐系统就是一个将信息生产者和信息消费者连接起来的桥梁。平台往往会作为推荐系统的载体，实现信息生产者和消费者之间信息的匹配。上述提到的平台方、信息生产者和消费者可以分别用平台方（如：腾讯视频、淘..."><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/80c07e4dbb303.svg" /></a><a class="social share-item email" href="mailto:?subject=推荐系统 - humbleyl&amp;body=http://humble2967738843.github.io/2024/04/10/tui-jian-xi-tong/"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/a1b00e20f425d.svg" /></a><a class="social share-item link" onclick="util.copy(&quot;copy-link&quot;, &quot;复制成功&quot;)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/8411ed322ced6.svg" /></a></div>
        
        <div class="qrcode" id="qrcode-wechat" style="opacity:0;height:0">
          <img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://api.qrserver.com/v1/create-qr-code/?size=256x256&data=http://humble2967738843.github.io/2024/04/10/tui-jian-xi-tong/"/>
        </div>
        
      </div>
    </section>
    </div>
</article>
<div class="related-wrap" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2024/05/06/ji-yu-lian-shi-tui-li-de-wen-dang-ji-shi-jian-lun-yuan-ti-qu/">基于链式推理的文档级事件论元提取</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2024/03/23/tong-ji-xue-xi-fang-fa/">2024-03-23</a></div></section></div>

<div class="related-wrap" id="related-posts"></div>


  <div class="related-wrap md-text" id="comments">
    <section class='header cmt-title cap theme'>
      <p>快来参与讨论吧~</p>

    </section>
    <section class='body cmt-body giscus'>
      

<svg class="loading" style="vertical-align:middle;fill:currentColor;overflow:hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg>

<div id="giscus" src="https://giscus.app/client.js" data-repo="Humble2967738843/giscus" data-repo-id="R_kgDOLsS5kA" data-category="Announcements" data-category-id="DIC_kwDOLsS5kM4Cel5C" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="preferred_color_scheme" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous"></div>

    </section>
  </div>



<footer class="page-footer footnote"><hr><div class="text"><center>
</br>
</br>
<script type="text/javascript">
function show_runtime() {
    window.setTimeout("show_runtime()", 1000);
    X = new Date("10/20/2023 00:00:00");
    Y = new Date();
    T = (Y.getTime() - X.getTime());
    M = 24 * 60 * 60 * 1000;
    a = T / M;
    A = Math.floor(a);
    b = (a - A) * 24;
    B = Math.floor(b);
    c = (b - B) * 60;
    C = Math.floor((b - B) * 60);
    D = Math.floor((c - C) * 60);
    runtime_span.innerHTML = "⏲️本站已运行 " + A + "天|" + B + "小时|" + C + "分|" + D + "秒⏲️"
}
show_runtime();
</script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv">🤩本站总访问量<span id="busuanzi_value_site_pv"></span>次</span><br>
<span id="runtime_span"></span>
</center>
</div></footer>
<div class="main-mask" onclick="sidebar.dismiss()"></div></div><aside class="l_right">
<div class="widgets">



<widget class="widget-wrapper toc" id="data-toc" collapse="false"><div class="widget-header dis-select"><span class="name">本文目录</span><a class="cap-action" onclick="sidebar.toggleTOC()" ><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg></a></div><div class="widget-body"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%B8%80%E7%AB%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0"><span class="toc-text">第一章 推荐系统概述</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%84%8F%E4%B9%89"><span class="toc-text">1.1推荐系统的意义</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%B3%E5%8F%B0%E6%96%B9"><span class="toc-text">平台方</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8E%A8%E8%8D%90%E5%92%8C%E6%90%9C%E7%B4%A2%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-text">推荐和搜索的区别</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="toc-text">1.2推荐系统的架构</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84"><span class="toc-text">系统架构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3"><span class="toc-text">设计思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A6%BB%E7%BA%BF%E5%B1%82"><span class="toc-text">离线层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%A6%BB%E7%BA%BF%E5%B1%82%E4%BC%98%E5%8A%BF%E5%92%8C%E4%B8%8D%E8%B6%B3"><span class="toc-text">离线层优势和不足</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%91%E7%BA%BF%E5%B1%82"><span class="toc-text">近线层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9C%A8%E7%BA%BF%E5%B1%82"><span class="toc-text">在线层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E6%9E%B6%E6%9E%84"><span class="toc-text">算法架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E6%8A%80%E6%9C%AF%E6%A0%88"><span class="toc-text">1.3推荐系统的技术栈</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%97%E6%B3%95"><span class="toc-text">算法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%BB%E5%83%8F%E5%B1%82"><span class="toc-text">画像层</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%87%E6%9C%AC%E7%90%86%E8%A7%A3"><span class="toc-text">文本理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E8%AF%8D%E6%A0%87%E7%AD%BE"><span class="toc-text">关键词标签</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AE%B9%E7%90%86%E8%A7%A3"><span class="toc-text">内容理解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1"><span class="toc-text">知识图谱</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AC%E5%9B%9E-%E7%B2%97%E6%8E%92"><span class="toc-text">召回&#x2F;粗排</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B"><span class="toc-text">经典召回模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B"><span class="toc-text">序列召回模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%94%A8%E6%88%B7%E5%BA%8F%E5%88%97%E6%8B%86%E5%88%86"><span class="toc-text">用户序列拆分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E5%9B%BE%E8%B0%B1-1"><span class="toc-text">知识图谱</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E6%A8%A1%E5%9E%8B"><span class="toc-text">图模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B2%BE%E6%8E%92"><span class="toc-text">精排</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89%E6%A8%A1%E5%9E%8B"><span class="toc-text">特征交叉模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="toc-text">序列模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E4%BF%A1%E6%81%AF%E8%9E%8D%E5%90%88"><span class="toc-text">多模态信息融合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E4%BB%BB%E5%8A%A1%E5%AD%A6%E4%B9%A0"><span class="toc-text">多任务学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0"><span class="toc-text">强化学习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%A8%E5%9F%9F%E6%8E%A8%E8%8D%90"><span class="toc-text">跨域推荐</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E6%8E%92%E5%BA%8F"><span class="toc-text">重排序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B7%A5%E7%A8%8B"><span class="toc-text">工程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-text">总结</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%97%E6%B3%95%E5%9F%BA%E7%A1%80"><span class="toc-text">第二章 推荐系统算法基础</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#2-1%E7%BB%8F%E5%85%B8%E5%8F%AC%E5%9B%9E%E6%A8%A1%E5%9E%8B"><span class="toc-text">2.1经典召回模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-1%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E5%8F%AC%E5%9B%9E"><span class="toc-text">2.1.1基于协同过滤的召回</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3"><span class="toc-text">核心思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%BA%A6%E9%87%8F%E6%96%B9%E6%B3%95"><span class="toc-text">相似度度量方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%82%E7%94%A8%E5%9C%BA%E6%99%AF"><span class="toc-text">适用场景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4"><span class="toc-text">基于用户的协同过滤</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%80%9D%E6%83%B3-1"><span class="toc-text">核心思想</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B"><span class="toc-text">计算过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E8%BF%87%E7%A8%8B"><span class="toc-text">具体过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%89%8B%E5%8A%A8%E8%AE%A1%E7%AE%97"><span class="toc-text">手动计算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UserCF%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">UserCF编程实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#UserCF%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-text">UserCF优缺点</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%AE%97%E6%B3%95%E8%AF%84%E4%BC%B0"><span class="toc-text">算法评估</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8F%AC%E5%9B%9E%E7%8E%87"><span class="toc-text">召回率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%B2%BE%E7%A1%AE%E7%8E%87"><span class="toc-text">精确率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A6%86%E7%9B%96%E7%8E%87"><span class="toc-text">覆盖率</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%B0%E9%A2%96%E5%BA%A6"><span class="toc-text">新颖度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4"><span class="toc-text">基于物品的协同过滤</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="toc-text">基本思想</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E7%AE%97%E8%BF%87%E7%A8%8B-1"><span class="toc-text">计算过程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ItemCF%E7%BC%96%E7%A8%8B%E5%AE%9E%E7%8E%B0"><span class="toc-text">ItemCF编程实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95%E7%9A%84%E6%9D%83%E9%87%8D%E6%94%B9%E8%BF%9B"><span class="toc-text">协同过滤算法的权重改进</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%AE%97%E6%B3%95%E7%9A%84%E9%97%AE%E9%A2%98%E5%88%86%E6%9E%90"><span class="toc-text">协同过滤算法的问题分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AF%BE%E5%90%8E%E6%80%9D%E8%80%83"><span class="toc-text">课后思考</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-2%E5%9F%BA%E4%BA%8E%E5%90%91%E9%87%8F%E7%9A%84%E5%8F%AC%E5%9B%9E"><span class="toc-text">2.1.2基于向量的召回</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FM%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-text">FM模型结构</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#FM%E7%94%A8%E4%BA%8E%E5%8F%AC%E5%9B%9E"><span class="toc-text">FM用于召回</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-2%E7%BB%8F%E5%85%B8%E6%8E%92%E5%BA%8F%E6%A8%A1%E5%9E%8B"><span class="toc-text">2.2经典排序模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-3Wide-amp-Deep%E7%B3%BB%E5%88%97"><span class="toc-text">2.2.3Wide&amp;Deep系列</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-1GBDT-LR%E7%AE%80%E4%BB%8B"><span class="toc-text">2.2.1GBDT+LR简介</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B"><span class="toc-text">逻辑回归模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GBDT%E6%A8%A1%E5%9E%8B"><span class="toc-text">GBDT模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GBDT-LR%E6%A8%A1%E5%9E%8B"><span class="toc-text">GBDT+LR模型</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DeepFM"><span class="toc-text">DeepFM</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8A%A8%E6%9C%BA"><span class="toc-text">动机</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-2%E7%89%B9%E5%BE%81%E4%BA%A4%E5%8F%89"><span class="toc-text">2.2.2特征交叉</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DCN"><span class="toc-text">DCN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E6%9C%BA-1"><span class="toc-text">动机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86"><span class="toc-text">模型结构及原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Embedding%E5%92%8CStacking%E5%B1%82"><span class="toc-text">Embedding和Stacking层</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Cross-NetWork"><span class="toc-text">Cross NetWork</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Deep-Network"><span class="toc-text">Deep Network</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%84%E5%90%88%E8%BE%93%E5%87%BA%E5%B1%82"><span class="toc-text">组合输出层</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-4%E5%BA%8F%E5%88%97%E6%A8%A1%E5%9E%8B"><span class="toc-text">2.2.4序列模型</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DIN"><span class="toc-text">DIN</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E6%9C%BA-2"><span class="toc-text">动机</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DIN%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84%E5%8F%8A%E5%8E%9F%E7%90%86"><span class="toc-text">DIN模型结构及原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E8%A1%A8%E7%A4%BA"><span class="toc-text">特征表示</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%BA%BF%E6%A8%A1%E5%9E%8B"><span class="toc-text">基线模型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#DIN%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84"><span class="toc-text">DIN模型架构</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%97%E6%B3%95%E9%9D%A2%E7%BB%8F"><span class="toc-text">第四章 推荐系统算法面经</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#4-1ML%E4%B8%8EDL%E5%9F%BA%E7%A1%80"><span class="toc-text">4.1ML与DL基础</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="toc-text">机器学习</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E8%A7%A3%E6%9E%90"><span class="toc-text">参考解析</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1"><span class="toc-text">机器学习</span></a></li></ol></li></ol></li></ol></div><div class="widget-footer">

<a class="top" onclick="util.scrollTop()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 12c0-4.714 0-7.071 1.464-8.536C4.93 2 7.286 2 12 2c4.714 0 7.071 0 8.535 1.464C22 4.93 22 7.286 22 12c0 4.714 0 7.071-1.465 8.535C19.072 22 16.714 22 12 22s-7.071 0-8.536-1.465C2 19.072 2 16.714 2 12Z"/><path stroke-linecap="round" stroke-linejoin="round" d="m9 15.5l3-3l3 3m-6-4l3-3l3 3"/></g></svg><span>回到顶部</span></a></div></widget>
</div></aside><div class='float-panel blur'>
  <button type='button' style='display:none' class='laptop-only rightbar-toggle mobile' onclick='sidebar.rightbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg>
  </button>
  <button type='button' style='display:none' class='mobile-only leftbar-toggle mobile' onclick='sidebar.leftbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 11c0-3.771 0-5.657 1.172-6.828C4.343 3 6.229 3 10 3h4c3.771 0 5.657 0 6.828 1.172C22 5.343 22 7.229 22 11v2c0 3.771 0 5.657-1.172 6.828C19.657 21 17.771 21 14 21h-4c-3.771 0-5.657 0-6.828-1.172C2 18.657 2 16.771 2 13z"/><path id="sep" stroke-linecap="round" d="M5.5 10h6m-5 4h4m4.5 7V3"/></g></svg>
  </button>
</div>
</div><div class="scripts">
<script type="text/javascript">
  const ctx = {
    date_suffix: {
      just: `刚刚`,
      min: `分钟前`,
      hour: `小时前`,
      day: `天前`,
    },
    root : `/`,
  };

  // required plugins (only load if needs)
  if (`local_search`) {
    ctx.search = {};
    ctx.search.service = `local_search`;
    if (ctx.search.service == 'local_search') {
      let service_obj = Object.assign({}, `{"field":"all","path":"/search.json","content":true,"codeblock":true,"sort":"-date"}`);
      ctx.search[ctx.search.service] = service_obj;
    }
  }
  const def = {
    avatar: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/3442075.svg`,
    cover: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/cover/76b86c0226ffd.svg`,
  };
  const deps = {
    jquery: `https://cdn.bootcdn.net/ajax/libs/jquery/3.7.1/jquery.min.js`,
    marked: `https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js`
  }
  

</script>

<script type="text/javascript">
  const utils = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    css: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    js: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')){
        src = ctx.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    jq: (fn) => {
      if (typeof jQuery === 'undefined') {
        utils.js(deps.jquery).then(fn)
      } else {
        fn()
      }
    },
    
    onLoading: (el) => {
      if (el) {
        $(el).append('<div class="loading-wrap"><svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" stroke-opacity=".3" d="M12 3C16.9706 3 21 7.02944 21 12C21 16.9706 16.9706 21 12 21C7.02944 21 3 16.9706 3 12C3 7.02944 7.02944 3 12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="1.3s" values="60;0"/></path><path stroke-dasharray="15" stroke-dashoffset="15" d="M12 3C16.9706 3 21 7.02944 21 12"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.3s" values="15;0"/><animateTransform attributeName="transform" dur="1.5s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></g></svg></div>');
      }
    },
    onLoadSuccess: (el) => {
      if (el) {
        $(el).find('.loading-wrap').remove();
      }
    },
    onLoadFailure: (el) => {
      if (el) {
        $(el).find('.loading-wrap svg').remove();
        $(el).find('.loading-wrap').append('<svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" d="M12 3L21 20H3L12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.5s" values="60;0"/></path><path stroke-dasharray="6" stroke-dashoffset="6" d="M12 10V14"><animate fill="freeze" attributeName="stroke-dashoffset" begin="0.6s" dur="0.2s" values="6;0"/></path></g><circle cx="12" cy="17" r="1" fill="currentColor" fill-opacity="0"><animate fill="freeze" attributeName="fill-opacity" begin="0.8s" dur="0.4s" values="0;1"/></circle></svg>');
        $(el).find('.loading-wrap').addClass('error');
      }
    },
    request: (el, url, callback, onFailure) => {
      let retryTimes = 3;
      utils.onLoading(el);
      function req() {
        return new Promise((resolve, reject) => {
          let status = 0; // 0 等待 1 完成 2 超时
          let timer = setTimeout(() => {
            if (status === 0) {
              status = 2;
              timer = null;
              reject('请求超时');
              if (retryTimes == 0) {
                onFailure();
              }
            }
          }, 5000);
          fetch(url).then(function(response) {
            if (status !== 2) {
              clearTimeout(timer);
              resolve(response);
              timer = null;
              status = 1;
            }
            if (response.ok) {
              return response.json();
            }
            throw new Error('Network response was not ok.');
          }).then(function(data) {
            retryTimes = 0;
            utils.onLoadSuccess(el);
            callback(data);
          }).catch(function(error) {
            if (retryTimes > 0) {
              retryTimes -= 1;
              setTimeout(() => {
                req();
              }, 5000);
            } else {
              utils.onLoadFailure(el);
              onFailure();
            }
          });
        });
      }
      req();
    },
  };
</script>

<script>
  const sidebar = {
    leftbar: () => {
      if (l_body) {
        l_body.toggleAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    rightbar: () => {
      if (l_body) {
        l_body.toggleAttribute('rightbar');
        l_body.removeAttribute('leftbar');
      }
    },
    dismiss: () => {
      if (l_body) {
        l_body.removeAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    toggleTOC: () => {
      document.querySelector('#data-toc').classList.toggle('collapse');
    }
  }
</script>

<!-- required -->
<script src="/js/main.js?v=1.27.0" async></script>

<!-- optional -->

  <script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const els = document.querySelectorAll("#comments #giscus");
    if (els.length === 0) return;
    els.forEach((el, i) => {
      try {
        el.innerHTML = '';
      } catch (error) {
        console.error(error);
      }
      var script = document.createElement('script');
      script.async = true;
      for (let key of Object.keys(el.attributes)) {
        let attr = el.attributes[key];
        if (['class', 'id'].includes(attr.name) === false) {
          script.setAttribute(attr.name, attr.value);
        }
      }
      el.appendChild(script);
    });
  });
</script>




<script defer>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.services = Object.assign({}, JSON.parse(`{"mdrender":{"js":"/js/services/mdrender.js"},"siteinfo":{"js":"/js/services/siteinfo.js","api":null},"ghinfo":{"js":"/js/services/ghinfo.js"},"sites":{"js":"/js/services/sites.js"},"friends":{"js":"/js/services/friends.js"},"timeline":{"js":"/js/services/timeline.js"},"fcircle":{"js":"/js/services/fcircle.js"},"weibo":{"js":"/js/services/weibo.js"},"memos":{"js":"/js/services/memos.js"}}`));
    for (let id of Object.keys(ctx.services)) {
      const js = ctx.services[id].js;
      if (id == 'siteinfo') {
        ctx.cardlinks = document.querySelectorAll('a.link-card[cardlink]');
        if (ctx.cardlinks?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            setCardLink(ctx.cardlinks);
          });
        }
      } else {
        const els = document.getElementsByClassName(`ds-${id}`);
        if (els?.length > 0) {
          utils.jq(() => {
            if (id == 'timeline' || 'memos' || 'marked') {
              utils.js(deps.marked).then(function () {
                utils.js(js, { defer: true });
              });
            } else {
              utils.js(js, { defer: true });
            }
          });
        }
      }
    }
  });
</script>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.search = {
      path: `/search.json`,
    }
    utils.js('/js/search/local-search.js', { defer: true });
  });
</script><script>
  window.FPConfig = {
    delay: 0,
    ignoreKeywords: [],
    maxRPS: 5,
    hoverDelay: 25
  };
</script>
<script defer src="https://cdn.bootcdn.net/ajax/libs/flying-pages/2.1.2/flying-pages.min.js"></script><script defer src="https://cdn.bootcdn.net/ajax/libs/vanilla-lazyload/17.8.4/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazy",
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    window.lazyLoadInstance?.update();
  });
</script><script>
  ctx.fancybox = {
    selector: `.timenode p>img`,
    css: `https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.22/fancybox/fancybox.min.css`,
    js: `https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.22/fancybox/fancybox.umd.min.js`
  };
  var selector = '[data-fancybox]:not(.error)';
  if (ctx.fancybox.selector) {
    selector += `, ${ctx.fancybox.selector}`
  }
  var needFancybox = document.querySelectorAll(selector).length !== 0;
  if (!needFancybox) {
    const els = document.getElementsByClassName('ds-memos');
    if (els != undefined && els.length > 0) {
      needFancybox = true;
    }
  }
  if (needFancybox) {
    utils.css(ctx.fancybox.css);
    utils.js(ctx.fancybox.js, { defer: true }).then(function () {
      Fancybox.bind(selector, {
        hideScrollbar: false,
        Thumbs: {
          autoStart: false,
        },
        caption: (fancybox, slide) => {
          return slide.triggerEl.alt || null
        }
      });
    })
  }
</script><script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const swiper_api = document.getElementById('swiper-api');
    if (swiper_api != undefined) {
      utils.css(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.css`);
      utils.js(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.js`, { defer: true }).then(function () {
        const effect = swiper_api.getAttribute('effect') || '';
        var swiper = new Swiper('.swiper#swiper-api', {
          slidesPerView: 'auto',
          spaceBetween: 8,
          centeredSlides: true,
          effect: effect,
          loop: true,
          pagination: {
            el: '.swiper-pagination',
            clickable: true,
          },
          navigation: {
            nextEl: '.swiper-button-next',
            prevEl: '.swiper-button-prev',
          },
        });
      })
    }
  });
</script><script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"] ],
    processEscapes: true
  }
});
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for(i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script>
<script>
  document.addEventListener('DOMContentLoaded', function () {
    window.codeElements = document.querySelectorAll('.code');
    if (window.codeElements.length > 0) {
      ctx.copycode = {
        default_text: `Copy`,
        success_text: `Copied`,
        toast: `复制成功`,
      };
      utils.js('/js/plugins/copycode.js');
    }
  });
</script>


<!-- inject -->

</div></body></html>

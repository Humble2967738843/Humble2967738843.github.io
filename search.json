[{"title":"推荐系统","path":"/2024/04/10/tui-jian-xi-tong/","content":"第一章 推荐系统概述1.1推荐系统的意义推荐系统就是一个将信息生产者和信息消费者连接起来的桥梁。平台往往会作为推荐系统的载体，实现信息生产者和消费者之间信息的匹配。上述提到的平台方、信息生产者和消费者可以分别用平台方（如：腾讯视频、淘宝、网易云音乐等）、物品（如：视频、商品、音乐等）和用户和来指代。下面分别从这三方需求出发，介绍推荐系统的存在的意义。 平台方平台方一般是为信息生产者提供物品展示的位置，然后通过不同的方式吸引用户来到平台上寻找他们感兴趣的物品。平台通过商家对物品的展示以及用户的浏览、观看或下单等行为，就产生了所谓的”流量”。 对平台方而言，流量的高效利用是推荐系统存在的重要原因。以典型的电商网站一般具有如图所示的树状拓扑结构，树状结构在连通性方面有着天然的劣势，阻碍这流量的高效流通。推荐系统的出现使得原本的树状结构变成网络拓扑结构，大大增强了整个网络的连通性。推荐模块不仅使用户在当前页面有了更好的选择路径，同时也给了每个商品增加入口和展示机会，进而提高了成交概率。而推荐质量的好坏，直接决定了用户选择这条路径的可能性，进而影响着流量的利用效率。 推荐和搜索的区别搜索和推荐都是解决互联网大数据时代信息过载的手段，但是它们也存在着许多的不同： 用户意图：搜索时的用户意图是非常明确的，用户通过查询的关键词主动发起搜索请求。对于推荐而言，用户的需求是不明确的，推荐系统在通过对用户历史兴趣的分析给用户推荐他们可能感兴趣的内容。 个性化程度：对于搜索而言，由于限定的了搜索词，所以展示的内容对于用户来说是有标准答案的，所以搜索的个性化程度较低。而对于推荐来说，推荐的内容本身就是没有标准答案的，每个人都有不同的兴趣，所以每个人展示的内容，个性化程度比较强。 优化目标：对于搜索系统而言，更希望可以快速地、准确地定位到标准答案，所以希望搜索结果中答案越靠前越好，通常评价指标有：归一化折损累计收益（NDCG）、精确率（Precision）和召回率（Recall）。对于推荐系统而言，因为没有标准的答案，所以优化目标可能会更宽泛。例如用户停留时长、点击、多样性，评分等。不同的优化目标又可以拆解成具体的不同的评价指标。 马太效应和长尾理论：对于搜索系统来说，用户的点击基本都集中在排列靠前的内容上，对于排列靠后的很少会被关注，这就是马太效应。而对于推荐系统来说，热门物品被用户关注更多，冷门物品不怎么被关注的现象也是存在的，所以也存在马太效应。此外，在推荐系统中，冷门物品的数量远远高于热门物品的数量，所以物品的长尾性非常明显。 1.2推荐系统的架构推荐和搜索系统核心的的任务是从海量物品中找到用户感兴趣的内容。在这个背景下，推荐系统包含的模块非常多，每个模块将会有很多专业研究的工程和研究工程师，作为刚入门的应届生或者实习生很难对每个模块都有很深的理解，实际上也大可不必，我们完全可以从学习好一个模块技术后，以点带面学习整个系统，虽然正式工作中我们放入门每个人将只会负责的也是整个系统的一部分。但是掌握推荐系统最重要的还是梳理清楚整个推荐系统的架构，知道每一个部分需要完成哪些任务，是如何做的，主要的技术栈是什么，有哪些局限和可以研究的问题，能够对我们学习推荐系统有一个提纲挈领的作用。 所以这篇文章将会从系统架构和算法架构两个角度出发解析推荐系统通用架构。系统架构设计思想是大数据背景下如何有效利用海量和实时数据，将推荐系统按照对数据利用情况和系统响应要求出发，将整个架构分为离线层、近线层、在线层三个模块。然后分析这三个模块分别承担推荐系统什么任务，有什么制约要求。这种角度不和召回、排序这种通俗我们理解算法架构，因为更多的是考虑推荐算法在工程技术实现上的问题，系统架构是如何权衡利弊，如何利用各种技术工具帮助我们达到想要的目的的，方便我们理解为什么推荐系统要这样设计。 而算法架构是从我们比较熟悉的召回、粗排、排序、重排等算法环节角度出发的，重要的是要去理解每个环节需要完成的任务，每个环节的评价体系，以及为什么要那么设计。还有一个重要问题是每个环节涉及到的技术栈和主流算法，这部分非常重要而且篇幅较大，所以我们放在下一个章节讲述。 架构设计是一个非常大的话题，设计的核心在于平衡和妥协。在推荐系统不同时期、不同的环境、不同的数据，架构都会面临不一样的问题。网飞官方博客有一段总结： We want the ability to use sophisticated machine learning algorithms that can grow to arbitrary complexity and can deal with huge amounts of data. We also want an architecture that allows for flexible and agile innovation where new approaches can be developed and plugged-in easily. Plus, we want our recommendation results to be fresh and respond quickly to new data and user actions. Finding the sweet spot between these desires is not trivial: it requires a thoughtful analysis of requirements, careful selection of technologies, and a strategic decomposition of recommendation algorithms to achieve the best outcomes for our members. “我们需要具备使用复杂机器学习算法的能力，这些算法要可以适应高度复杂性，可以处理大量数据。我们还要能够提供灵活、敏捷创新的架构，新的方法可以很容易在其基础上开发和插入。而且，我们需要我们的推荐结果足够新，能快速响应新的数据和用户行为。找到这些要求之间恰当的平衡并不容易，需要深思熟虑的需求分析，细心的技术选择，战略性的推荐算法分解，最终才能为客户达成最佳的结果。” 所以在思考推荐系统架构考虑的第一个问题是确定边界：知道推荐系统要负责哪部分问题，这就是边界内的部分。在这个基础上，架构要分为哪几个部分，每一部分需要完成的子功能是什么，每一部分依赖外界的什么。了解推荐系统架构也和上文讲到的思路一样，我们需要知道的是推荐系统要负责的是怎么问题，每一个子模块分别承担了哪些功能，它们的主流技术栈是什么。从这个角度来阅读本文，将会有助于读者抓住重点。 系统架构推荐系统架构，首先从数据驱动角度，对于数据，最简单的方法是存下来，留作后续离线处理，离线层就是我们用来管理离线作业的部分架构。在线层能更快地响应最近的事件和用户交互，但必须实时完成。这会限制使用算法的复杂性和处理的数据量。离线计算对于数据数量和算法复杂度限制更少，因为它以批量方式完成，没有很强的时间要求。不过，由于没有及时加入最新的数据，所以很容易过时。 个性化架构的关键问题，就是如何以无缝方式结合、管理在线和离线计算过程。近线层介于两种方法之间，可以执行类似于在线计算的方法，但又不必以实时方式完成。这种设计思想最经典的就是Netflix在2013年提出的架构，整个架构分为 离线层：不用实时数据，不提供实时响应； 近线层：使用实时数据，不保证实时响应； 在线层：使用实时数据，保证实时在线服务； 设计思想网飞的这个架构提出的非常早，其中的技术也许不是现在常用的技术了，但是架构模型仍然被很多公司采用。 这个架构为什么要这么设计，本质上是因为推荐系统是由大量数据驱动的，大数据框架最经典的就是lambda架构和kappa架构。而推荐系统在不同环节所使用的数据、处理数据的量级、需要的读取速度都是不同的，目前的技术还是很难实现一套端到端的及时响应系统，所以这种架构的设计本质上还是一种权衡后的产物，所以有了下图这种模型： 整个数据部分其实是一整个链路，主要是三块，分别是客户端及服务器实时数据处理、流处理平台准实时数据处理和大数据平台离线数据处理这三个部分。 看到这里，一个很直观的问题就是，为什么数据处理需要这么多步骤？这些步骤都是干嘛的，存在的意义是什么？ 我们一个一个来说，首先是客户端和服务端的实时数据处理。这个很好理解，这个步骤的工作就是记录。将用户在平台上真实的行为记录下来，比如用户看到了哪些内容，和哪些内容发生了交互，和哪些没有发生了交互。如果再精细一点，还会记录用户停留的时间，用户使用的设备等等。除此之外还会记录行为发生的时间，行为发生的session等其他上下文信息。 这一部分主要是后端和客户端完成，行业术语叫做埋点。所谓的埋点其实就是记录点，因为数据这种东西需要工程师去主动记录，不记录就没有数据，记录了才有数据。既然我们要做推荐系统，要分析用户行为，还要训练模型，显然需要数据。需要数据，就需要记录。 第二个步骤是流处理平台准实时数据处理，这一步是干嘛的呢，其实也是记录数据，不过是记录一些准实时的数据。很多同学又会迷糊了，实时我理解，就是当下立即的意思，准实时是什么意思呢？准实时的意思也是实时，只不过没有那么即时，比如可能存在几分钟的误差。这样存在误差的即时数据，行业术语叫做准实时。那什么样的准实时数据需要记录呢？在推荐领域基本上只有一个类别，就是用户行为数据。也就是用户在观看这个内容之前还看过哪些内容，和哪些内容发生过交互。理想情况这部分数据也需要做成实时，但由于这部分数据量比较大，并且逻辑也相对复杂，所以很难做到非常实时，一般都是通过消息队列加在线缓存的方式做成准实时。 最后我们看第三个步骤，叫做离线数据处理，离线也就是线下处理，基本上就没有时限的要求了。 一般来说，离线处理才是数据处理的大头。所有“脏活累活”复杂的操作都是在离线完成的，比如说一些join操作。后端只是记录了用户交互的商品id，我们需要商品的详细信息怎么办？需要去和商品表关联查表。显然数据关联是一个非常耗时的操作，所以只能放到离线来做。 接下来详细介绍一下这三个模块。 离线层离线层是计算量最大的一个部分，它的特点是不依赖实时数据，也不需要实时提供服务。需要实现的主要功能模块是： 数据处理、数据存储； 特征工程、离线特征计算； 离线模型的训练； 这里我们可以看出离线层的任务是最接近学校中我们处理数据、训练模型这种任务的，不同可能就是需要面临更大规模的数据。离线任务一般会按照天或者更久运行，比如每天晚上定期更新这一天的数据，然后重新训练模型，第二天上线新模型。 离线层优势和不足离线层面临的数据量级是最大的，面临主要的问题是海量数据存储、大规模特征工程、多机分布式机器学习模型训练。目前主流的做法是HDFS，收集到我们所有的业务数据，通过HIVE等工具，从全量数据中抽取出我们需要的数据，进行相应的加工，离线阶段主流使用的分布式框架一般是Spark。所以离线层有如下的优势： 可以处理大量的数据，进行大规模特征工程； 可以进行批量处理和计算； 不用有响应时间要求； 但是同样的，如果我们只使用用户离线数据，最大的不足就是无法反应用户的实时兴趣变化，这就促使了近线层的产生。 近线层近线层的主要特点是准实时，它可以获得实时数据，然后快速计算提供服务，但是并不要求它和在线层一样达到几十毫秒这种延时要求。近线层的产生是同时想要弥补离线层和在线层的不足，折中的产物。 它适合处理一些对延时比较敏感的任务，比如： 特征的事实更新计算：例如统计用户对不同type的ctr，推荐系统一个老生常谈的问题就是特征分布不一致怎么办，如果使用离线算好的特征就容易出现这个问题。近线层能够获取实时数据，按照用户的实时兴趣计算就能很好避免这个问题。 实时训练数据的获取：比如在使用DIN、DSIN这行网络会依赖于用户的实时兴趣变化，用户几分钟前的点击就可以通过近线层获取特征输入模型。 模型实时训练：可以通过在线学习的方法更新模型，实时推送到线上； 近线层的发展得益于最近几年大数据技术的发展，很多流处理框架的提出大大促进了近线层的进步。如今Flink、Storm等工具一统天下。 在线层在线层，就是直接面向用户的的那一层了。最大的特点是对响应延时有要求，因为它是直接面对用户群体的，你可以想象你打开抖音淘宝等界面，几乎都是秒刷出来给你的推荐结果的，不会说还需要让你等待几秒钟时间。所有的用户请求都会发送到在线层，在线层需要快速返回结果，它主要承担的工作有： 模型在线服务；包括了快速召回和排序； 在线特征快速处理拼接：：根据传入的用户ID和场景，快速读取特征和处理； AB实验或者分流：根据不同用户采用不一样的模型，比如冷启动用户和正常服务模型； 运筹优化和业务干预：比如要对特殊商家流量扶持、对某些内容限流； 典型的在线服务是用过RESTful/RPC等提供服务，一般是公司后台服务部门调用我们的这个服务，返回给前端。具体部署应用比较多的方式就是使用Docker在K8S部署。而在线服务的数据源就是我们在离线层计算好的每个用户和商品特征，我们事先存放在数据库中，在线层只需要实时拼接，不进行复杂的特征运算，然后输入近线层或者离线层已经训练好的模型，根据推理结果进行排序，最后返回给后台服务器，后台服务器根据我们对每一个用户的打分，再返回给用户。 在线层最大的问题就是对实时性要求特别高，一般来说是几十毫秒，这就限制了我们能做的工作，很多任务往往无法及时完成，需要近线层协助我们做。 算法架构我们在入门学习推荐系统的时候，更加关注的是哪个模型AUC更高、topK效果好，哪个模型更加牛逼的问题，从基本的协同过滤到点击率预估算法，从深度学习到强化学习，学术界都始终走在最前列。一个推荐算法从出现到在业界得到广泛应用是一个长期的过程，因为在实际的生产系统中，首先需要保证的是稳定、实时地向用户提供推荐服务，在这个前提下才能追求推荐系统的效果。 召回 召回层的主要目标时从推荐池中选取几千上万的item，送给后续的排序模块。由于召回面对的候选集十分大，且一般需要在线输出，故召回模块必须轻量快速低延迟。由于后续还有排序模块作为保障，召回不需要十分准确，但不可遗漏（特别是搜索系统中的召回模块）。 如果没有召回层，每个User都能和每一个Item去在线排序阶段预测目标概率，理论上来说是效果最好，但是是不现实的，线上不延迟允许，所以召回和粗排阶段就要做一些候选集筛选的工作，保证在有限的能够给到排序层去做精排的候选集的时间里，效果最大化。另一个方面就是通过这种模型级联的方式，可以减少用排序阶段拟合多目标的压力，比如召回阶段我们现在主要是在保证Item质量的基础上注重覆盖率多样性，粗排阶段主要用简单的模型来解决不同路的召回和当前用户的相关性问题，最后截断到1k个以内的候选集，这个候选集符合一定的个性化相关性、视频质量和多样性的保证，然后做ranking去做复杂模型的predict。 目前基本上采用多路召回解决范式，分为非个性化召回和个性化召回。个性化召回又有content-based、behavior-based、feature-based等多种方式。 召回主要考虑的内容有： 考虑用户层面：用户兴趣的多元化，用户需求与场景的多元化：例如：新闻需求，重大要闻，相关内容沉浸阅读等等 考虑系统层面：增强系统的鲁棒性；部分召回失效，其余召回队列兜底不会导致整个召回层失效；排序层失效，召回队列兜底不会导致整个推荐系统失效 系统多样性内容分发：图文、视频、小视频；精准、试探、时效一定比例；召回目标的多元化，例如：相关性，沉浸时长，时效性，特色内容等等 可解释性推荐一部分召回是有明确推荐理由的：很好的解决产品性数据的引入； 粗排 粗排的原因是有时候召回的结果还是太多，精排层速度还是跟不上，所以加入粗排。粗排可以理解为精排前的一轮过滤机制，减轻精排模块的压力。粗排介于召回和精排之间，要同时兼顾精准性和低延迟。目前粗排一般也都模型化了，其训练样本类似于精排，选取曝光点击为正样本，曝光未点击为负样本。但由于粗排一般面向上万的候选集，而精排只有几百上千，其解空间大很多。 粗排阶段的架构设计主要是考虑三个方面，一个是根据精排模型中的重要特征，来做候选集的截断，另一部分是有一些召回设计，比如热度或者语义相关的这些结果，仅考虑了item侧的特征，可以用粗排模型来排序跟当前User之间的相关性，据此来做截断，这样是比单独的按照item侧的倒排分数截断得到更加个性化的结果，最后是算法的选型要在在线服务的性能上有保证，因为这个阶段在pipeline中完成从召回到精排的截断工作，在延迟允许的范围内能处理更多的召回候选集理论上与精排效果正相关。 精排 精排层，也是我们学习推荐入门最常常接触的层，我们所熟悉的算法很大一部分都来自精排层。这一层的任务是获取粗排模块的结果，对候选集进行打分和排序。精排需要在最大时延允许的情况下，保证打分的精准性，是整个系统中至关重要的一个模块，也是最复杂，研究最多的一个模块。 精排是推荐系统各层级中最纯粹的一层，他的目标比较单一且集中，一门心思的实现目标的调优即可。最开始的时候精排模型的常见目标是ctr,后续逐渐发展了cvr等多类目标。精排和粗排层的基本目标是一致的，都是对商品集合进行排序，但是和粗排不同的是，精排只需要对少量的商品(即粗排输出的商品集合的topN)进行排序即可。因此，精排中可以使用比粗排更多的特征，更复杂的模型和更精细的策略（用户的特征和行为在该层的大量使用和参与也是基于这个原因）。 精排层模型是推荐系统中涵盖的研究方向最多，有非常多的子领域值得研究探索，这也是推荐系统中技术含量最高的部分，毕竟它是直接面对用户，产生的结果对用户影响最大的一层。目前精排层深度学习已经一统天下了，精排阶段采用的方案相对通用，首先一天的样本量是几十亿的级别，我们要解决的是样本规模的问题，尽量多的喂给模型去记忆，另一个方面时效性上，用户的反馈产生的时候，怎么尽快的把新的反馈给到模型里去，学到最新的知识。 重排 常见的有三种优化目标：Point Wise、Pair Wise 和 List Wise。重排序阶段对精排生成的Top-N个物品的序列进行重新排序，生成一个Top-K个物品的序列，作为排序系统最后的结果，直接展现给用户。重排序的原因是因为多个物品之间往往是相互影响的，而精排序是根据PointWise得分，容易造成推荐结果同质化严重，有很多冗余信息。而重排序面对的挑战就是海量状态空间如何求解的问题，一般在精排层我们使用AUC作为指标，但是在重排序更多关注NDCG等指标。 重排序在业务中，获取精排的排序结果，还会根据一些策略、运营规则参与排序，比如强制去重、间隔排序、流量扶持等、运营策略、多样性、context上下文等，重新进行一个微调。重排序更多的是List Wise作为优化目标的，它关注的是列表中商品顺序的问题来优化模型，但是一般List Wise因为状态空间大，存在训练速度慢的问题。 由于精排模型一般比较复杂，基于系统时延考虑，一般采用point-wise方式，并行对每个item进行打分。这就使得打分时缺少了上下文感知能力。用户最终是否会点击购买一个商品，除了和它自身有关外，和它周围其他的item也息息相关。重排一般比较轻量，可以加入上下文感知能力，提升推荐整体算法效率。比如三八节对美妆类目商品提权，类目打散、同图打散、同卖家打散等保证用户体验措施。重排中规则比较多，但目前也有不少基于模型来提升重排效果的方案。 混排 多个业务线都想在Feeds流中获取曝光，则需要对它们的结果进行混排。比如推荐流中插入广告、视频流中插入图文和banner等。可以基于规则策略（如广告定坑）和强化学习来实现。 总结整篇文章从系统架构梳理了Netfliex的经典推荐系统架构，整个架构更多是偏向实时性能和效果之间trade off的结果。如果从另外的角度看推荐系统架构，比如从数据流向，或者说从推荐系统各个时序依赖来看，就是我们最熟悉的召回、粗排、精排、重排、混排等模块了。这种角度来看是把推荐系统从前往后串起来，其中每一个模块既有在离线层工作的，也有在在线层工作的。而从数据驱动角度看，更能够看到推荐系统的完整技术栈，推荐系统当前面临的局限和发展方向。 召回、排序这些里面单拿出任何一个模块都非常复杂。这也是为什么大家都说大厂拧螺丝的原因，因为很可能某个人只会负责其中很小的一个模块。许多人说起自己的模块来如数家珍，但对于全局缺乏认识，带来的结果是当你某天跳槽了或者是工作内容变化了，之前从工作当中的学习积累很难沉淀下来，这对于程序员的成长来说是很不利的。 所以希望这篇文章能够帮助大家在负责某一个模块，优化某一个功能的时候，除了能够有算法和数据，还能能够考虑对整个架构带来的影响，如何提升整体的一个性能，慢慢开阔自己的眼界，构建出一个更好的推荐系统。 1.3推荐系统的技术栈推荐系统是一个非常大的框架，有非常多的模块在里面，完整的一套推荐系统体系里，不仅会涉及到推荐算法工程师、后台开发工程师、数据挖掘/分析工程师、NLP/CV工程师还有前端、客户端甚至产品、运营等支持。我们作为算法工程师，需要掌握的技术栈主要就是在算法和工程两个区域了，所以这篇文章将会分别从算法和工程两个角度出发，结合两者分析当前主流的一些推荐算法技术栈。 算法首先我们从推荐系统架构出发，一种分法是将整个推荐系统架构分为召回、粗排、精排、重排、混排等模块。它的分解方法是从一份数据如何从生产出来，到线上服务完整顺序的一个流程。因为在不同环节，我们一般会考虑不同的算法，所以这种角度出发我们来研究推荐系统主流的算法技术栈。 为了帮助新手在后文方便理解，首先简单介绍这些模块的功能主要是： 召回：从推荐池中选取几千上万的item，送给后续的排序模块。由于召回面对的候选集十分大，且一般需要在线输出，故召回模块必须轻量快速低延迟。由于后续还有排序模块作为保障，召回不需要十分准确，但不可遗漏（特别是搜索系统中的召回模块）。目前基本上采用多路召回解决范式，分为非个性化召回和个性化召回。个性化召回又有content-based、behavior-based、feature-based等多种方式。 粗排：粗拍的原因是有时候召回的结果还是太多，精排层速度还是跟不上，所以加入粗排。粗排可以理解为精排前的一轮过滤机制，减轻精排模块的压力。粗排介于召回和精排之间，要同时兼顾精准性和低延迟。一般模型也不能过于复杂 精排：获取粗排模块的结果，对候选集进行打分和排序。精排需要在最大时延允许的情况下，保证打分的精准性，是整个系统中至关重要的一个模块，也是最复杂，研究最多的一个模块。精排系统构建一般需要涉及样本、特征、模型三部分。 重排：获取精排的排序结果，基于运营策略、多样性、context上下文等，重新进行一个微调。比如三八节对美妆类目商品提权，类目打散、同图打散、同卖家打散等保证用户体验措施。重排中规则比较多，但目前也有不少基于模型来提升重排效果的方案。 混排：多个业务线都想在Feeds流中获取曝光，则需要对它们的结果进行混排。比如推荐流中插入广告、视频流中插入图文和banner等。可以基于规则策略（如广告定坑）和强化学习来实现。 画像层首先是推荐系统的物料库，这部分内容里，算法主要体现在如何绘制一个用户画像和商品画像。这个环节是推荐系统架构的基础设施，一般可能新用户/商品进来，或者每周定期会重新一次整个物料库，计算其中信息，为用户打上标签，计算统计信息，为商品做内容理解等内容。其中用户画像是大家比较容易理解的，比如用户年龄、爱好通常APP会通过注册界面收集这些信息。而商品画像形式就非常多了，比如淘宝主要推荐商品，抖音主要是短视频，所以大家的物料形式比较多，内容、质量差异也比较大，所以内容画像各家的做法也不同，当前比较主流的都会涉及到一个多模态信息内容理解。下面我贴了一个微信看一看的内容画像框架，然后我们来介绍下在这一块主要使用的算法技术。 一般推荐系统会加入多模态的一个内容理解。我们用短视频形式举个例子，假设用户拍摄了一条短视频，上传到了平台，从推荐角度看，首先我们有的信息是这条短视频的作者、长度、作者为它选择的标签、时间戳这些信息。但是这对于推荐来说是远远不够的，首先作者打上的标签不一定准确反映作品，原因可能是我们模型的语义空间可能和作者/现实世界不一致。其次我们需要更多维度的特征，比如有些用户喜欢看小姐姐跳舞，那我希望能够判断一条视频中是否有小姐姐，这就涉及到封面图的基于CV的内容抽取或者整个视频的抽取；再比如作品的标题一般能够反映主题信息，除了很多平台常用的用“#”加上一个标签以外，我们也希望能够通过标题抽取出基于NLP的信息。还有更多的维度可以考虑：封面图多维度的多媒体特征体系，包括人脸识别，人脸embedding，标签，一二级分类，视频embedding表示，水印，OCR识别，清晰度，低俗色情，敏感信息等多种维度。 这里面涉及的任务主要是CV的目标检测、语义分割等任务，NLP中的情感分析、摘要抽取、自然语言理解等任务。但是这部分算法一般团队都会有专门负责的组，不需要推荐算法工程师来负责，他们会有多模态的语意标签输出，主要形式是各种粒度的Embedding。我们只需要在我们的推荐模型中引入这些预训练的Embedding。 文本理解这应该是用的最多的模态信息，包括item的标题、正文、OCR、评论等数据。这里面也可以产生不同粒度的信息，比如文本分类，把整个item做一个粗粒度的分类。 这里的典型算法有：RNN、TextCNN、FastText、Bert等； 关键词标签相比文本分类，关键词是更细粒度的信息，往往是一个mutil-hot的形式，它会对item在我们的标签库的选取最合适的关键词或者标签。 这里典型的算法有：TF-IDF、Bert、LSTM-CRF等。 内容理解在很多场景下，推荐的主题都是视频或者图片，远远多于仅推荐文本的情况，这里视频/图片item中的内容中除了文本的内容以外，更多的信息其实来源于视频/图片内容本身, 因此需要尝试从多种模态中抽取更丰富的信息。主要包括分类信息、封面图OCR的信息、视频标签信息等 这里典型的算法有：TSN、RetinaFace、PSENet等。 知识图谱知识图谱作为知识承载系统，用于对接内外部关键词信息与词关系信息；内容画像会将原关系信息整合，并构建可业务应用的关系知识体系，其次，依赖业务中积累用户行为产生的实体关系数据，本身用户需求的标签信息，一并用于构建业务知识的兴趣图谱，基于同构网络与异构网络表示学习等核心模型，输出知识表示与表达，抽象后的图谱用于文本识别，推荐语义理解，兴趣拓展推理等场景，直接用于兴趣推理的冷启场景已经验证有很不错的收益。 这方面的算法有：KGAT、RippleNet等。 召回/粗排推荐系统的召回阶段可以理解为根据用户的历史行为数据，为用户在海量的信息中粗选一批待推荐的内容，挑选出一个小的候选集的过程。粗排用到的很多技术与召回重合，所以放在一起讲，粗排也不是必需的环节，它的功能对召回的结果进行个粗略的排序，在保证一定精准的前提下，进一步减少往后传送的物品数量，这就是粗排的作用。 召回模块面对几百上千万的推荐池物料规模，候选集十分庞大。由于后续有排序模块作为保障，故不需要十分准确，但必须保证不要遗漏和低延迟。目前主要通过多路召回来实现，一方面各路可以并行计算，另一方面取长补短。可以看到各类同类竞品的系统虽然细节上多少存在差异，但不约而同的采取了多路召回的架构，这类设计考虑如下几点问题： 考虑用户层面：用户兴趣的多元化，用户需求与场景的多元化：例如：新闻需求，重大要闻，相关内容沉浸阅读等等 考虑系统层面：增强系统的鲁棒性；部分召回失效，其余召回队列兜底不会导致整个召回层失效；排序层失效，召回队列兜底不会导致整个推荐系统失效 系统多样性内容分发：图文、视频、小视频；精准、试探、时效一定比例；召回目标的多元化，例如：相关性，沉浸时长，时效性，特色内容等等 可解释性推荐一部分召回是有明确推荐理由的：很好的解决产品性数据的引入； 介绍了召回任务的目的和场景后，接下来分析召回层面主要的技术栈，因为召回一般都是多路召回，从模型角度分析有很多召回算法，这种一般是在召回层占大部分比例点召回，除此之外，还会有探索类召回、策略运营类召回、社交类召回等。接下来我们着重介绍模型类召回。 经典召回模型随着技术发展，在Embedding基础上的模型化召回是一个技术发展潮流方向。这种召回的范式是通过某种算法，对user和item分别打上Embedding，然后user与item在线进行KNN计算实时查询最近邻结果作为召回结果，快速找出匹配的物品。需要注意的是如果召回采用模型召回方法，优化目标最好和排序的优化目标一致，否则可能被过滤掉。 在这方面典型的算法有：FM、双塔DSSM、Multi-View DNN等。 序列召回模型推荐系统主要解决的是基于用户的隐式阅读行为来做个性化推荐的问题，序列模型一些基于神经网络模型学习得到Word2Vec模型，再后面的基于RNN的语言模型，最先用的最多的Bert，这些方法都可以应用到召回的学习中。 用户在使用 APP 或者网站的时候，一般会产生一些针对物品的行为，比如点击一些感兴趣的物品，收藏或者互动行为，或者是购买商品等。而一般用户之所以会对物品发生行为，往往意味着这些物品是符合用户兴趣的，而不同类型的行为，可能代表了不同程度的兴趣。比如购买就是比点击更能表征用户兴趣的行为。在召回阶段，如何根据用户行为序列打 embedding，可以采取有监督的模型，比如 Next Item Prediction 的预测方式即可；也可以采用无监督的方式，比如物品只要能打出 embedding，就能无监督集成用户行为序列内容，例如 Sum Pooling。 这方面典型的算法有：CBOW、Skip-Gram、GRU、Bert等。 用户序列拆分上文讲了利用用户行为物品序列，打出用户兴趣 Embedding 的做法。但是，另外一个现实是：用户往往是多兴趣的，比如可能同时对娱乐、体育、收藏感兴趣。这些不同的兴趣也能从用户行为序列的物品构成上看出来，比如行为序列中大部分是娱乐类，一部分体育类，少部分收藏类等。那么能否把用户行为序列物品中，这种不同类型的用户兴趣细分，而不是都笼统地打到一个用户兴趣 Embedding 里呢？用户多兴趣拆分就是解决这类更细致刻画用户兴趣的方向。 本质上，把用户行为序列打到多个 embedding 上，实际它是个类似聚类的过程，就是把不同的 Item，聚类到不同的兴趣类别里去。目前常用的拆分用户兴趣 embedding 的方法，主要是胶囊网络和 Memory Network，但是理论上，很多类似聚类的方法应该都是有效的，所以完全可以在这块替换成你自己的能产生聚类效果的方法来做。 这方面典型的算法有：Multi-Interest Network with Dynamic Routing for Recommendation at Tmall等。 知识图谱知识图谱有一个独有的优势和价值，那就是对于推荐结果的可解释性；比如推荐给用户某个物品，可以在知识图谱里通过物品的关键关联路径给出合理解释，这对于推荐结果的解释性来说是很好的，因为知识图谱说到底是人编码出来让自己容易理解的一套知识体系，所以人非常容易理解其间的关系。知识图谱的可解释性往往是和图路径方法关联在一起的，而 Path 类方法，很多实验证明了，在排序角度来看，是效果最差的一类方法，但是它在可解释性方面有很好的效果，所以往往可以利用知识图谱构建一条可解释性的召回通路。 这方面的算法有：KGAT、RippleNet等。 图模型推荐系统中User和Item相关的行为、需求、属性和社交信息具有天然的图结构，可以使用一张复杂的异构图来表示整个推荐系统。图神经网络模型推荐就是基于这个想法，把异构网络中包含的结构和语义信息编码到结点Embedding表示中，并使用得到向量进行个性化推荐。知识图谱其实是图神经网络的一个比较特殊的具体实例，但是，知识图谱因为编码的是静态知识，而不是用户比较直接的行为数据，和具体应用距离比较远，这可能是导致两者在推荐领域表现有差异的主要原因。 这方面典型的算法有：GraphSAGE、PinSage等。 精排排序模型是推荐系统中涵盖的研究方向最多，有非常多的子领域值得研究探索，这也是推荐系统中技术含量最高的部分，毕竟它是直接面对用户，产生的结果对用户影响最大的一层。目前精排层深度学习已经一统天下了，这是王喆老师《深度学习推荐算法》书中的精排层模型演化线路。具体来看分为DNN、Wide&amp;Deep两大块，实际深入还有序列建模，以及没有提到的多任务建模都是工业界非常常用的，所以我们接下来具体谈论其中每一块的技术栈。 特征交叉模型在深度学习推荐算法发展早期，很多论文聚焦于如何提升模型的特征组合和交叉的能力，这其中既包含隐式特征交叉Deep Crossing也有采用显式特征交叉的探究。本质上是希望模型能够摆脱人工先验的特征工程，实现端到端的一套模型。 在早期的推荐系统中，基本是由人工进行特征交叉的，往往凭借对业务的理解和经验，但是费时费力。于是有了很多的这方面的研究，从FM到GBDT+LR都是引入模型进行自动化的特征交叉。再往后就是深度模型，深度模型虽然有万能近似定理，但是真正想要发挥模型的潜力，显式的特征交叉还是必不可少的。 这方面的经典研究工作有：DCN、DeepFM、xDeepFM等； 序列模型在推荐系统中，历史行为序列是非常重要的特征。在序列建模中，主要任务目标是得到用户此刻的兴趣向量（user interest vector）。如何刻画用户兴趣的广泛性，是推荐系统比较大的一个难点，用户历史行为序列建模的研究经历了从Pooling、RNN到attention、capsule再到transformer的顺序。在序列模型中，又有很多细分的方向，比如根据用户行为长度有研究用户终身行为序列的，也有聚焦当下兴趣的，还有研究如何抽取序列特征的抽取器，比如研究attention还是胶囊网络。 这方面典型的研究工作有：DIN、DSIN、DIEN、SIM等； 多模态信息融合在上文我们提到算法团队往往会利用内容画像信息，既有基于CV也有基于NLP抽取出来的信息。这是非常合理的，我们在逛抖音、淘宝的时候关注的不仅仅item的价格、品牌，同样会关注封面小姐姐好不好看、标题够不够震惊等信息。除此之外，在冷启动场景下，我们能够利用等信息不够多，如果能够使用多模态信息，能很大程度上解决数据稀疏的问题。 传统做法在多模态信息融合就是希望把不同模态信息利用起来，通过Embedding技术融合进模型。在推荐领域，主流的做法还是一套非端到端的体系，由其他模型抽取出多模态信息，推荐只需要融合入这些信息就好了。同时也有其他工作是利用注意力机制等方法来学习不同模态之间的关联，来增强多模态的表示。 比较典型的工作有：Image Matters: Visually modeling user behaviors using Advanced Model Server、UMPR等。 多任务学习很多场景下我们模型优化的目标都是CTR，有一些场景只考虑CTR是不够的，点击率模型、时长模型和完播率模型是大部分信息流产品推荐算法团队都会尝试去做的模型。单独优化点击率模型容易推出来标题党，单独优化时长模型可能推出来的都是长视频或长文章，单独优化完播率模型可能短视频短图文就容易被推出来，所以多目标就应运而生。信息流推荐中，我们不仅希望用户点进我们的item，还希望能有一个不错的完播率，即希望用户能看完我们推荐的商品。或者电商场景希望用户不仅点进来，还希望他买下或者加入购物车了。这些概率实际上就是模型要学习的目标，多种目标综合起来，包括阅读、点赞、收藏、分享等等一系列的行为，归纳到一个模型里面进行学习，这就是推荐系统的多目标学习。 这方面比较典型的算法有：ESSM、MMoE、DUPN等。 强化学习强化学习与一般有监督的深度学习相比有一些很显著的优势，首先强化学习能够比较灵活的定义优化的业务目标，考虑推荐系统长短期的收益，比如用户留存，在深度模型下，我们很难设计这个指标的优化函数，而强化学习是可以对长期收益下来建模。第二是能够体现用户兴趣的动态变化，比如在新闻推荐下，用户兴趣变化很快，强化学习更容易通过用户行为动态产生推荐结果。最后是EE也就是利用探索机制，这种一种当前和长期收益的权衡，强化学习能够更好的调节这里的回报。 这方面比较典型的算法有：DQN、Reinforcement Learning for Slate-based Recommender Systems: A Tractable Decomposition and Practical Methodology； 跨域推荐一般一家公司业务线都是非常多的，比如腾讯既有腾讯视频，也有微信看一看、视频号，还有腾讯音乐，如果能够结合这几个场景的数据，同时进行推荐，一方面对于冷启动是非常有利的，另一方面也能补充更多数据，更好的进行精确推荐。 跨域推荐系统相比一般的推荐系统要更加复杂。在传统推荐系统中，我们只需要考虑建立当前领域内的一个推荐模型进行分析；而在跨域推荐中，我们更要关心在不同领域间要选择何种信息进行迁移，以及如何迁移这些信息，这是跨域推荐系统中非常关键的问题。 这方面典型的模型有：DTCDR、MV-DNN、EMCDR等； 重排序我们知道常见的有三种优化目标：Point Wise、Pair Wise 和 List Wise。重排序阶段对精排生成的Top-N个物品的序列进行重新排序，生成一个Top-K个物品的序列，作为排序系统最后的结果，直接展现给用户。重排序的原因是因为多个物品之间往往是相互影响的，而精排序是根据PointWise得分，容易造成推荐结果同质化严重，有很多冗余信息。而重排序面对的挑战就是海量状态空间如何求解的问题，一般在精排层我们使用AUC作为指标，但是在重排序更多关注NDCG等指标。 重排序在业务中，还会根据一些策略、运营规则参与排序，比如强制去重、间隔排序、流量扶持等，但是总计趋势上看还是算法排序越来越占据主流趋势。重排序更多的是List Wise作为优化目标的，它关注的是列表中商品顺序的问题来优化模型，但是一般List Wise因为状态空间大，存在训练速度慢的问题。这方面典型的做法，基于RNN、Transformer、强化学习的都有，这方面因为不是推荐的一个核心，所以没有展开来讲，而且这一块比较依赖实际的业务场景。 这里的经典算法有：MRR、DPP、RNN等； 工程推荐系统的实现需要依托工程，很多研究界Paper的idea满天飞，却忽视了工业界能否落地，进入工业界我们很难或者很少有组是做纯research的，所以我们同样有很多工程技术需要掌握。下面列举了在推荐中主要用到的工程技术： 编程语言：Python、Java（scala）、C++、sql、shell； 机器学习：Tensorflow/Pytorch、GraphLab/GraphCHI、LGB/Xgboost、SKLearn； 数据分析：Pandas、Numpy、Seaborn、Spark； 数据存储：mysql、redis、mangodb、hive、kafka、es、hbase； 相似计算：annoy、faiss、kgraph 流计算：Spark Streaming、Flink 分布式：Hadoop、Spark 上面那么多技术，我内容最重要的就是加粗的三部分，第一是语言：必须掌握的是Python，C++和JAVA中根据不同的组使用的是不同的语言，这个如果没有时间可以等进组后慢慢学习。然后是机器学习框架：Tensorflow和Pytorch至少要掌握一个吧，前期不用纠结学哪个，这个迁移成本很低，基本能够达到触类旁通，而且面试官不会为难你只会这个不会那个。最后是数据分析工具：Pandas是我们处理单机规模数据的利器，但是进入工业界，Hadoop和Spark是需要会用的，不过不用学太深，会用即可。 总结本文从算法和工程两个角度分析了推荐系统的一个技术栈，但是还有很多方向遗漏，也有很多方向受限于现在的技术水平深度不够和有错误的情况，后续会不断补充和更正。 所以技术栈我列出的是一个非常广度的技术，实际上每一个技术钻研下去都需要非常多时间，而且不一定是你实际工作中会遇到的，所以不要被那么多技术吓到，也要避免陷入技术细节的海洋中。 我和非常多的大厂面试官讨论过技术深度和广度的问题，得出来的结论是对于入门的推荐算法工程师而言，实际上深度和广度的要求取决于你要去的组，有些组有很深的推荐技术沉淀，有很强的工程师团队，这样的组就会希望候选者能够在某个方面有比较深入的研究，这个方面既包含工程方面也包含研究方面。但是如果是比较新的组、或者技术沉淀不深、推荐不是主要任务的组，对深度要求就不会很高。总而言之，我认为对于应届生/实习生来说，在推荐最重要的工程技术/研究方向，至少在召回和排序模块，需要选一个作为方向，是需要较深钻研。对于其他技术/研究方向需要有一定了解，比如可以没用过强化学习，但是要知道强化学习能够在推荐中解决什么问题，剩下的可以等到真实遇到需要后再去学习。 参考资料 万字入门推荐系统 张俊林：技术演进趋势：召回-&gt;排序-&gt;重排 微信”看一看”多模型内容策略与召回 多目标学习在推荐系统中的应用 强化学习在美团“猜你喜欢”的实践 推荐系统技术演进趋势：重排篇 阿里强化学习重排实践 第二章 推荐系统算法基础2.1经典召回模型2.1.1基于协同过滤的召回核心思想协同过滤（Collaborative Filtering）推荐算法是最经典、最常用的推荐算法。基本思想是： 根据用户之前的喜好以及其他兴趣相近的用户的选择来给用户推荐物品。 基于对用户历史行为数据的挖掘发现用户的喜好偏向， 并预测用户可能喜好的产品进行推荐。 一般是仅仅基于用户的行为数据（评价、购买、下载等）, 而不依赖于项的任何附加信息（物品自身特征）或者用户的任何附加信息（年龄， 性别等）。 目前应用比较广泛的协同过滤算法是基于邻域的方法，主要有： 基于用户的协同过滤算法（UserCF）：给用户推荐和他兴趣相似的其他用户喜欢的产品。 基于物品的协同过滤算法（ItemCF）：给用户推荐和他之前喜欢的物品相似的物品。 不管是 UserCF 还是 ItemCF 算法， 重点是计算用户之间（或物品之间）的相似度。 相似度度量方法 杰卡德（Jaccard）相似系数 Jaccard 系数是衡量两个集合的相似度一种指标，计算公式如下： sim_{uv} = \\frac{\\left| N(u) \\cap N(v) \\right|}{\\left| N(u) \\cup N(v) \\right|} 其中 $N(u)$，$N(v)$分别表示用户$u$和用户交$v$互物品的集合。 对于用户$u$ 和$v$，该公式反映了两个交互物品交集的数量占这两个用户交互物品并集的数量的比例。 由于杰卡德相似系数一般无法反映具体用户的评分喜好信息，所以常用来评估用户是否会对某物品进行打分， 而不是预估用户会对某物品打多少分。 余弦相似度 余弦相似度衡量了两个向量的夹角，夹角越小越相似。余弦相似度的计算如下，其与杰卡德（Jaccard）相似系数只是在分母上存在差异： sim_{uv} = \\frac{\\left| N(u) \\cap N(v) \\right|}{\\sqrt{\\left| N(u) \\cdot N(v) \\right|}}从向量的角度进行描述，令矩阵$A$ 为用户-物品交互矩阵，矩阵的行表示用户，列表示物品。 设用户和物品数量分别为$m$, $n$，交互矩阵$A$就是一个$m$行$n$列的矩阵。 矩阵中的元素均为$ 0/1$。若用户$i$对物品$j$存在交互，那么 $A_{ij} =1$，否则为 $0$。 那么，用户之间的相似度可以表示为： sim_{uv} = cos(u, v) = \\frac{u \\cdot v}{\\left| u \\right| \\cdot \\left| v \\right|} 向量 $u$,$v$在形式都是 one-hot 类型，$u \\cdot v$表示向量点积。 上述用户-物品交互矩阵在现实中是十分稀疏的，为了节省内存，交互矩阵会采用字典进行存储。在 sklearn 中，余弦相似度的实现： 123456from sklearn.metrics.pairwise import cosine_similarityi = [1, 0, 0, 0]j = [1, 0, 1, 0]cosine_similarity([i, j]) 皮尔逊相关系数 在用户之间的余弦相似度计算时，将用户向量的内积展开为各元素乘积和： sim_{uv} = \\frac{\\sum_ir_{ui} * r_{vi}}{\\sqrt{\\sum_i r_{ui}^{2} \\sqrt{\\sum_i r_{vi}^{2}}}} 其中，$r{ui},r{vi}$分别表示用户$u$和用户$v$对物品 $i$是否有交互(或具体评分值)。 皮尔逊相关系数与余弦相似度的计算公式非常的类似，如下： sim(u, v) = \\frac{\\sum_{i \\in I}(r_{ui} - \\bar r_u )(r_{vi} - \\bar r_v)}{\\sqrt{\\sum_{i \\in I}(r_{ui} - \\bar r_u )^{2}} \\sqrt{\\sum_{i \\in I}(r_{vi} - \\bar r_v)^{2}}} 其中，$r{ui}, r{vi}$分别表示用户$u$和用户$i$对物品$i$是否有交互(或具体评分值)； $\\bar r_u, \\bar r_v$分别表示用户$u$和用户$v$交互的所有物品交互数量或者评分的平均值； 相较于余弦相似度，皮尔逊相关系数通过使用用户的平均分对各独立评分进行修正，减小了用户评分偏置的影响。在scipy中，皮尔逊相关系数的实现： 123456from scipy.stats import pearsonri = [1, 0, 0, 0]j = [1, 0.5, 0.5, 0]pearsonr(i, j) 适用场景 Jaccard 相似度表示两个集合的交集元素个数在并集中所占的比例 ，所以适用于隐式反馈数据（0-1）。 余弦相似度在度量文本相似度、用户相似度、物品相似度的时候都较为常用。 皮尔逊相关度，实际上也是一种余弦相似度。不过先对向量做了中心化，范围在 −1−1 到 11。 相关度量的是两个变量的变化趋势是否一致，两个随机变量是不是同增同减。 不适合用作计算布尔值向量（0-1）之间相关度。 基于用户的协同过滤核心思想基于用户的协同过滤（UserCF）： 例如，我们要对用户$A$进行物品推荐，可以先找到和他有相似兴趣的其他用户。 然后，将共同兴趣用户喜欢的，但用户$A$未交互过的物品推荐给 $A$。 计算过程以下图为例，给用户推荐物品的过程可以形象化为：预测用户对物品进行打分的任务，表格里面是5个用户对于5件物品的一个打分情况，就可以理解为用户对物品的喜欢程度。 UserCF算法的两个步骤： 首先，根据前面的这些打分情况(或者说已有的用户向量）计算一下 Alice 和用户1， 2， 3， 4的相似程度， 找出与 Alice 最相似的 n 个用户。 根据这 n 个用户对物品 5 的评分情况和与 Alice 的相似程度会猜测出 Alice 对物品5的评分。如果评分比较高的话， 就把物品5推荐给用户 Alice， 否则不推荐。 具体过程 计算用户之间的相似度 根据 1.2 节的几种方法， 我们可以计算出各用户之间的相似程度。对于用户 Alice，选取出与其最相近的$N$个用户。 计算用户对新物品的评分预测 常用的方式之一：利用目标用户与相似用户之间的相似度以及相似用户对物品的评分，来预测目标用户对候选物品的评分估计： R_{u,p} = \\frac{\\sum_{s \\in S}(w_{u, s} \\cdot R_{s, p})}{\\sum_{s \\in S}w_{u, s}} 其中，权重 $w{u, s}$是用户$u$和用户$s$ 的相似度， $R{s, p}$是用户$s$对物品$p$的评分。 另一种方式：考虑到用户评分的偏置，即有的用户喜欢打高分， 有的用户喜欢打低分的情况。公式如下： R_{u,p} = \\bar R_u +\\frac{\\sum_{s \\in S}(w_{u, s} \\cdot (R_{s, p} - \\bar R_s))}{\\sum_{s \\in S}w_{u, s}} 其中，$\\bar R_s$表示用户$s$对物品的历史平均评分。 对用户进行物品推荐 在获得用户$u$ 对不同物品的评价预测后， 最终的推荐列表根据预测评分进行排序得到。 手动计算根据上面的问题， 下面手动计算 Alice 对物品 5 的得分： 计算 Alice 与其他用户的相似度（基于皮尔逊相关系数） 手动计算 Alice 与用户 1 之间的相似度： 用户向量$Alice:(5,3,4,4),user1:(3,1,2,3),user2:(4,3,4,3),user3:(3,3,1,5),user4:(1,5,5,2)Alice:(5,3,4,4),user1:(3,1,2,3),user2:(4,3,4,3),user3:(3,3,1,5),user4:(1,5,5,2)$ 计算Alice与user1的余弦相似性: sim(Alice, user_1) = cos(Alice, user_1) = \\frac{5*3 + 3*1+4*2+4*3}{\\sqrt(5^2+3^2+4^2+4^2)*\\sqrt(3^2+1^2+2^2+3^2)} 计算Alice与user1皮尔逊相关系数: 计算均值：$Alice{ave}=4, user{1ave}=2.25$ 各向量减去均值：$Alice:(1,-1,0,0), user_1:(0.75,-1.25,-0.25,0.75)$ 最后计算两个新向量的余弦相似度，与上面的计算过程一致，结果是0.852 基于 sklearn 计算所有用户之间的皮尔逊相关系数。可以看出，与 Alice 相似度最高的用户为用户1和用户2。 根据相似度用户计算 Alice对物品5的最终得分 用户1对物品5的评分是3， 用户2对物品5的打分是5， 那么根据上面的计算公式， 可以计算出 Alice 对物品5的最终得分是 P_{Alice, item_5} = \\bar R_{Alice} + \\frac{\\sum_{k=1}^2 (w_{Alice, userk}(R_{userk, item_5} - \\bar R_{userk}))}{\\sum_{k=1}^2 w_{Alice, userk}}=4+\\frac{0.85*(3-2.4)+0.7*(5-3.8)}{0.85+0.7}=4.87 根据用户评分对用户进行推荐 根据 Alice 的打分对物品排个序从大到小： 物品1>物品5>物品3=物品4>物品2 如果要向 Alice 推荐2款产品的话， 我们就可以推荐物品 1 和物品 5 给 Alice。 UserCF编程实现 建立实验使用的数据表： 12345678910111213import numpy as npimport pandas as pddef loadData(): users = &#123;&#x27;Alice&#x27;: &#123;&#x27;A&#x27;: 5, &#x27;B&#x27;: 3, &#x27;C&#x27;: 4, &#x27;D&#x27;: 4&#125;, &#x27;user1&#x27;: &#123;&#x27;A&#x27;: 3, &#x27;B&#x27;: 1, &#x27;C&#x27;: 2, &#x27;D&#x27;: 3, &#x27;E&#x27;: 3&#125;, &#x27;user2&#x27;: &#123;&#x27;A&#x27;: 4, &#x27;B&#x27;: 3, &#x27;C&#x27;: 4, &#x27;D&#x27;: 3, &#x27;E&#x27;: 5&#125;, &#x27;user3&#x27;: &#123;&#x27;A&#x27;: 3, &#x27;B&#x27;: 3, &#x27;C&#x27;: 1, &#x27;D&#x27;: 5, &#x27;E&#x27;: 4&#125;, &#x27;user4&#x27;: &#123;&#x27;A&#x27;: 1, &#x27;B&#x27;: 5, &#x27;C&#x27;: 5, &#x27;D&#x27;: 2, &#x27;E&#x27;: 1&#125; &#125; return users 这里使用字典来建立用户-物品的交互表。 字典users的键表示不同用户的名字，值为一个评分字典，评分字典的键值对表示某物品被当前用户的评分。 由于现实场景中，用户对物品的评分比较稀疏。如果直接使用矩阵进行存储，会存在大量空缺值，故此处使用了字典。 计算用户相似性矩阵 由于训练数据中共包含 5 个用户，所以这里的用户相似度矩阵的维度也为$5 * 5$。 1234567891011121314151617181920212223user_data = loadData()similarity_matrix = pd.DataFrame(\tnp.identity(len(user_data)), #创建对角矩阵，对角线为1，其余为0\tindex=user_data.keys(),\tcolumns=user_data.keys(),)# 遍历每条用户-物品评分数据for u1, item1 in user_data.items():\tfor u2, item2 in user_data.items(): if u1 == u2: continue vec1, vec2 = [], [] for item, rating1 in item1.items(): rating2 = item2.get(item, -1) if rating2 == -1: continue vec1.append(rating1) vec2.append(rating2) # 计算不同用户之间的皮尔逊相关系数 similarity_matrix[u1][u2] = np.corrcoef(vec1, vec2)[0][1] print(similarity_matrix) 123456 1 2 3 4 51 1.000000 0.852803 0.707107 0.000000 -0.7921182 0.852803 1.000000 0.467707 0.489956 -0.9001493 0.707107 0.467707 1.000000 -0.161165 -0.4665694 0.000000 0.489956 -0.161165 1.000000 -0.6415035 -0.792118 -0.900149 -0.466569 -0.641503 1.000000 计算与 Alice 最相似的 num 个用户 12345target_user = &#x27;Alice&#x27;num = 2# 由于最相似的用户为自己，忽略本身sim_users = similarity_matrix[target_user].sort_values(ascend=False)print(f&#x27;与用户&#123;target_user&#125;最相似的&#123;num&#125;个用户为：&#123;sim_users&#125;&#x27;) 1与用户 Alice 最相似的2个用户为：[&#x27;user1&#x27;, &#x27;user2&#x27;] 预测用户 Alice 对物品 E 的评分 123456789101112131415weighted_scores = 0.cor_values_sum = 0.target_item = &#x27;E&#x27;# 基于皮尔逊相关稀疏预测用户评分for user in sim_users: corr_value = similaity_matrix[target_user][user] user_mean_rating = np.mean(list(user_data[user].values())) weighted_scores += corr_value * (user_data[user][target_item] - user_mean_rating) corr_values_sum += corr_value target_user_mean_rating = np.mean(list(user_data[target_user].values()))target_item_pred = target_user_mean_rating + weighted_scores / corr_values_sumprint(f&#x27;用户&#123;target_user&#125;对物品&#123;target_item&#125;的预测评分为：&#123;target_item_pred&#125;&#x27;) 1用户 Alice 对物品E的预测评分为：4.871979899370592 UserCF优缺点User-based算法存在两个重大问题： 数据稀疏性 一个大型的电子商务推荐系统一般有非常多的物品，用户可能买的其中不到1%的物品，不同用户之间买的物品重叠性较低，导致算法无法找到一个用户的邻居，即偏好相似的用户。 这导致UserCF不适用于那些正反馈获取较困难的应用场景(如酒店预订， 大件物品购买等低频应用)。 算法扩展性 基于用户的协同过滤需要维护用户相似度矩阵以便快速的找出$TopN$相似用户， 该矩阵的存储开销非常大，存储空间随着用户数量的增加而增加。 故不适合用户数据量大的情况使用。 由于UserCF技术上的两点缺陷， 导致很多电商平台并没有采用这种算法， 而是采用了ItemCF算法实现最初的推荐系统。 算法评估由于UserCF和ItemCF结果评估部分是共性知识点， 所以在这里统一标识。 召回率对用户$u$推荐 $N$个物品记为,$R(u)$ 令用户在$u$测试集上喜欢的物品集合为$T(u)$， 那么召回率定义为： Recall=\\frac{\\sum_u \\left| R(u) \\cap T(u) \\right|}{\\sum_u \\left| T(u) \\right|} 含义：在模型召回预测的物品中，预测准确的物品占用户实际喜欢的物品的比例。 精确率精确率定义为： Precision = \\frac{\\sum_u \\left| R(u) \\cap T(u) \\right|}{\\sum_u \\left| R(u) \\right|} 含义：推荐的物品中，对用户准确推荐的物品占总物品的比例。 如要确保召回率高，一般是推荐更多的物品，期望推荐的物品中会涵盖用户喜爱的物品。而实际中，推荐的物品中用户实际喜爱的物品占少数，推荐的精确率就会很低。故同时要确保高召回率和精确率往往是矛盾的，所以实际中需要在二者之间进行权衡。 覆盖率覆盖率反映了推荐算法发掘长尾的能力， 覆盖率越高， 说明推荐算法越能将长尾中的物品推荐给用户。 Coverage=\\frac{\\left| U_{u \\in U} R(u) \\right|}{\\left| I \\right|} 含义：推荐系统能够推荐出来的物品占总物品集合的比例。 其中$I$表示所有物品的个数； 系统的用户集合为$U$; 推荐系统给每个用户推荐一个长度为$N$的物品列表$R(u)$. 覆盖率表示最终的推荐列表中包含多大比例的物品。如果所有物品都被给推荐给至少一个用户， 那么覆盖率是100%。 新颖度用推荐列表中物品的平均流行度度量推荐结果的新颖度。 如果推荐出的物品都很热门， 说明推荐的新颖度较低。 由于物品的流行度分布呈长尾分布， 所以为了流行度的平均值更加稳定， 在计算平均流行度时对每个物品的流行度取对数。 O’scar Celma 在博士论文 “Music Recommendation and Discovery in the Long Tail “ 中研究了新颖度的评测。 基于物品的协同过滤基本思想基于物品的协同过滤（ItemCF）： 预先根据所有用户的历史行为数据，计算物品之间的相似性。 然后，把与用户喜欢的物品相类似的物品推荐给用户。 举例来说，如果用户 1 喜欢物品 A ，而物品 A 和 C 非常相似，则可以将物品 C 推荐给用户1。ItemCF算法并不利用物品的内容属性计算物品之间的相似度， 主要通过分析用户的行为记录计算物品之间的相似度， 该算法认为， 物品 A 和物品 C 具有很大的相似度是因为喜欢物品 A 的用户极可能喜欢物品 C。 计算过程基于物品的协同过滤算法和基于用户的协同过滤算法很像， 所以我们这里直接还是拿上面 Alice 的那个例子来看。 如果想知道 Alice 对物品5打多少分， 基于物品的协同过滤算法会这么做： 首先计算一下物品5和物品1， 2， 3， 4之间的相似性。 在Alice找出与物品 5 最相近的 n 个物品。 根据 Alice 对最相近的 n 个物品的打分去计算对物品 5 的打分情况。 手动计算： 手动计算物品之间的相似度 物品向量： 物品1(3,4,3,1),物品2(1,3,3,5),物品3(2,4,1,5),物品4(3,3,5,2),物品5(3,5,4,1) 下面计算物品 5 和物品 1 之间的余弦相似性: sim(物品1,物品5)=\\frac{3*3+4*5+3*4+1*1}{\\sqrt{9+16+9+1}*\\sqrt{9+25+16+1}} 皮尔逊相关系数类似。 基于 sklearn 计算物品之间的皮尔逊相关系数： 根据皮尔逊相关系数， 可以找到与物品5最相似的2个物品是 item1 和 item4， 下面基于上面的公式计算最终得分： P_{Alice,物品5}=\\bar R_{物品5}+\\frac{\\sum_{k=1}^2 (w_{物品5,物品k} (R_{Alice,物品k-\\bar R_{物品k}}))}{\\sum_{k=1}^2 w_{物品k,物品5}} \\\\ =\\frac{13}{4}+\\frac{0.97*(5-3.2)+0.58*(4-3.4)}{0.97+0.58} \\\\ =4.6ItemCF编程实现 构建物品-用户的评分矩阵 12345678910111213import numpy as npimport pandas as pddef loadData(): items = &#123; &#x27;A&#x27;: &#123;&#x27;Alice&#x27;: 5.0, &#x27;user1&#x27;: 3.0, &#x27;user2&#x27;: 4.0, &#x27;user3&#x27;: 3.0, &#x27;user4&#x27;: 1.0&#125;, &#x27;B&#x27;: &#123;&#x27;Alice&#x27;: 3.0, &#x27;user1&#x27;: 1.0, &#x27;user2&#x27;: 3.0, &#x27;user3&#x27;: 3.0, &#x27;user4&#x27;: 5.0&#125;, &#x27;C&#x27;: &#123;&#x27;Alice&#x27;: 4.0, &#x27;user1&#x27;: 2.0, &#x27;user2&#x27;: 4.0, &#x27;user3&#x27;: 1.0, &#x27;user4&#x27;: 5.0&#125;, &#x27;D&#x27;: &#123;&#x27;Alice&#x27;: 4.0, &#x27;user1&#x27;: 3.0, &#x27;user2&#x27;: 3.0, &#x27;user3&#x27;: 5.0, &#x27;user4&#x27;: 2.0&#125;, &#x27;E&#x27;: &#123;&#x27;user1&#x27;: 3.0, &#x27;user2&#x27;: 5.0, &#x27;user3&#x27;: 4.0, &#x27;user4&#x27;: 1.0&#125; &#125; return items 计算物品间的相似度矩阵 1234567891011121314151617181920212223item_data = loadData()simiality_matrix = pd.DataFrame( np.identity(len(item_data)), index=item_data.keys(), columns=item_data.keys(),)#遍历每条物品-用户评分数据for i1, user1 in item_data.items(): for i2, user2 in item_data.items(): if i1 == i2: continue vec1, vec2 = [], [] for user, rating1 in unser1.items(); rating2 = user2.get(user, -1) if rating2 == -1: continue vec1.append(rating1) vec2.append(rating2) similarity_matrix[i1][i2] = np.corrcoef(vec1, vec2)[0][1]print(similarity_matrix) 1234567 A B C D EA 1.000000 -0.476731 -0.123091 0.532181 0.969458B -0.476731 1.000000 0.645497 -0.310087 -0.478091C -0.123091 0.645497 1.000000 -0.720577 -0.427618D 0.532181 -0.310087 -0.720577 1.000000 0.581675E 0.969458 -0.478091 -0.427618 0.581675 1.000000 从 Alice 购买过的物品中，选出与物品 E 最相似的 num 件物品。 1234567891011121314target_user = &#x27;Alice&#x27;target_item = &#x27;E&#x27;num = 2sim_items = []sim_items_list = similarity_matrix[target_item].sort_values(ascending=False).index.tolist()for item in sim_items_list: # 如果target_user对物品item评分过 if target_user in item_data[item]: sim_items.append(item) if len(sim_items) == num: breakprint(f&#x27;与物品&#123;target_item&#125;最相似的&#123;num&#125;个物品为：&#123;sim_items&#125;&#x27;) 1与物品E最相似的2个物品为：[&#x27;A&#x27;, &#x27;D&#x27;] 预测用户 Alice 对物品 E 的评分 123456789101112131415target_user_mean_rating = np.mean(list(item_data[target_item].values()))weighted_scores = 0.corr_values_sum = 0.target_item = &#x27;E&#x27;for item in sim_items: corr_value = similarity_matrix[target_item][item] user_mean_rating = np.mean(list(item_data[item].values())) weighted_scores += corr_value * (item_data[item][target_user] - user_mean_rating) corr_values_sum += corr_valuetarget_item_pred = target_user_mean_rating + weighted_scores / corr_values_sumprint(f&#x27;用户&#123;target_user&#125;对物品&#123;target_item&#125;的预测评分为：&#123;target_item_pred&#125;&#x27;) 1用户 Alice 对物品E的预测评分为：4.6 协同过滤算法的权重改进 base 公式 w_{ij} = \\frac{|N(i) \\cap N(j) |}{|N(i)|} 该公式表示同时喜好物品$i$和物品$j$的用户数，占喜爱物品$i$的比例。 缺点：若物品$j$为热门物品，那么它与任何物品的相似度都很高。 对热门物品进行惩罚 w_{ij} = \\frac{|N(i) \\cap N(j)|}{\\sqrt{|N(i)||N(j)|}} 根据 base 公式在的问题，对物品$j$进行打压。打压的出发点很简单，就是在分母再除以一个物品$j$被购买的数量。 此时，若物品品$j$为热门物品，那么对应的$N(j)$也会很大，受到的惩罚更多 控制对热门物品的惩罚力度 w_{ij} = \\frac{|N(i) \\cap N(j)|}{|N(i)|^{1-a}|N(j)|^{a}} 除了第二点提到的办法，在计算物品之间相似度时可以对热门物品进行惩罚外。 可以在此基础上，进一步引入参数a ，这样可以通过控制参数a来决定对热门物品的惩罚力度。 对活跃用户的惩罚 在计算物品之间的相似度时，可以进一步将用户的活跃度考虑进来。 w_{ij} = \\frac{\\sum_{u \\in N(i) \\cap N(j)} \\frac{1}{log1+|N(u)|}}{|N(i)|^{1-a}|N(j)|^{a}} 对于异常活跃的用户，在计算物品之间的相似度时，他的贡献应该小于非活跃用户。 协同过滤算法的问题分析协同过滤算法存在的问题之一就是泛化能力弱： 即协同过滤无法将两个物品相似的信息推广到其他物品的相似性上。 导致的问题是热门物品具有很强的头部效应， 容易跟大量物品产生相似， 而尾部物品由于特征向量稀疏， 导致很少被推荐。 比如下面这个例子： 左边矩阵中，𝐴,𝐵,𝐶,𝐷A,B,C,D 表示的是物品。 可以看出，D是一件热门物品，其与 A、B、C的相似度比较大。因此，推荐系统更可能将D推荐给用过 A、B、C的用户。 但是，推荐系统无法找出 A、B、C之间相似性的原因是交互数据太稀疏， 缺乏相似性计算的直接数据。 所以这就是协同过滤的天然缺陷：推荐系统头部效应明显， 处理稀疏向量的能力弱。 为了解决这个问题， 同时增加模型的泛化能力。2006年，矩阵分解技术(Matrix Factorization, MF)被提出： 该方法在协同过滤共现矩阵的基础上， 使用更稠密的隐向量表示用户和物品， 挖掘用户和物品的隐含兴趣和隐含特征。 在一定程度上弥补协同过滤模型处理稀疏矩阵能力不足的问题。 课后思考 什么时候使用UserCF，什么时候使用ItemCF？为什么？ （1）UserCF 由于是基于用户相似度进行推荐， 所以具备更强的社交特性， 这样的特点非常适于用户少， 物品多， 时效性较强的场合。 比如新闻推荐场景， 因为新闻本身兴趣点分散， 相比用户对不同新闻的兴趣偏好， 新闻的及时性，热点性往往更加重要， 所以正好适用于发现热点，跟踪热点的趋势。 另外还具有推荐新信息的能力， 更有可能发现惊喜, 因为看的是人与人的相似性, 推出来的结果可能更有惊喜，可以发现用户潜在但自己尚未察觉的兴趣爱好。 （2）itemCF 这个更适用于兴趣变化较为稳定的应用， 更接近于个性化的推荐， 适合物品少，用户多，用户兴趣固定持久， 物品更新速度不是太快的场合。 比如推荐艺术品， 音乐， 电影。 协同过滤在计算上有什么缺点？有什么比较好的思路可以解决（缓解）？ 该问题答案参考上一小节的协同过滤算法的问题分析。 上面介绍的相似度计算方法有什么优劣之处？ cosine相似度计算简单方便，一般较为常用。但是，当用户的评分数据存在 bias 时，效果往往不那么好。 简而言之，就是不同用户评分的偏向不同。部分用户可能乐于给予好评，而部分用户习惯给予差评或者乱评分。 这个时候，根据cosine 相似度计算出来的推荐结果效果会打折扣。 举例来说明，如下图（X,Y,Z 表示物品，d,e,f表示用户）： 如果使用余弦相似度进行计算，用户 d 和 e 之间较为相似。但是实际上，用户 d 和 f 之间应该更加相似。只不过由于 d 倾向于打高分，e 倾向于打低分导致二者之间的余弦相似度更高。 这种情况下，可以考虑使用皮尔逊相关系数计算用户之间的相似性关系。 协同过滤还存在其他什么缺陷？有什么比较好的思路可以解决（缓解）？ 协同过滤的优点就是没有使用更多的用户或者物品属性信息，仅利用用户和物品之间的交互信息就能完成推荐，该算法简单高效。 但这也是协同过滤算法的一个弊端。由于未使用更丰富的用户和物品特征信息，这也导致协同过滤算法的模型表达能力有限。 对于该问题，逻辑回归模型（LR）可以更好地在推荐模型中引入更多特征信息，提高模型的表达能力。 2.1.2基于向量的召回FM模型结构FM 模型用于排序时，模型的公式定义如下： \\hat{y}(x):= w_0+\\sum_{i=1}^n w_ix_i + \\sum_{i=1}^{n} \\sum_{j=i+1}^n \\langle v_i,v_j \\rangle x_ix_j 其中，$i$表示特征的序号，$n$表示特征的数量；$x_i \\in R$表示第$i$个特征的值。 $vi,v_j \\in R^k$分别表示特征$x_i,x_j$对应的隐语义向量（Embedding向量），$\\langle v_i,v_j \\rangle := \\sum{f=1}^k v{i,f} \\cdot v{j,f}$。 $w_0,w_i \\in R$表示需要学习的参数。 FM的一阶交互特征 在 FM 的表达式中，前两项为特征的一阶交互项。将其拆分为用户特征和物品特征的一阶特征交互项，如下： w_0 + \\sum_{i=1}^n w_ix_i =w_0 + \\sum_{t \\in I} w_tx_t + \\sum_{u \\in U} w_u x_u 其中，$U$表示用户相关特征集合，$I$表示物品相关特征集合。 FM 的二阶特征交互 公式变换后，计算复杂度由$O(kn^2)$降到$O(kn)$。 由于本文章需要将 FM 模型用在召回，故将二阶特征交互项拆分为用户和物品项。有： FM用于召回2.2经典排序模型2.2.3Wide&amp;Deep系列2.2.1GBDT+LR简介前面介绍的协同过滤和矩阵分解存在的劣势就是仅利用了用户与物品相互行为信息进行推荐， 忽视了用户自身特征， 物品自身特征以及上下文信息等，导致生成的结果往往会比较片面。 而这次介绍的这个模型是2014年由Facebook提出的GBDT+LR模型， 该模型利用GBDT自动进行特征组合和选择， 进而生成新的离散特征向量， 再把该特征向量当做LR模型的输入， 来产生最后的预测结果， 该模型能够综合利用用户、物品和上下文等多种不同的特征， 生成较为全面的推荐结果， 在CTR点击率预估场景下使用较为广泛。 下面首先会介绍逻辑回归和GBDT模型各自的原理及优缺点， 然后介绍GBDT+LR模型的工作原理和细节。 逻辑回归模型逻辑回归模型非常重要， 在推荐领域里面， 相比于传统的协同过滤， 逻辑回归模型能够综合利用用户、物品、上下文等多种不同的特征生成较为“全面”的推荐结果， 关于逻辑回归的更多细节， 可以参考下面给出的链接，这里只介绍比较重要的一些细节和在推荐中的应用。 逻辑回归是在线性回归的基础上加了一个 Sigmoid 函数（非线形）映射，使得逻辑回归成为了一个优秀的分类算法， 学习逻辑回归模型， 首先应该记住一句话：逻辑回归假设数据服从伯努利分布,通过极大似然的方法，运用梯度下降来求解参数，来达到将数据二分类的目的。 相比于协同过滤和矩阵分解利用用户的物品“相似度”进行推荐， 逻辑回归模型将问题看成了一个分类问题， 通过预测正样本的概率对物品进行排序。这里的正样本可以是用户“点击”了某个商品或者“观看”了某个视频， 均是推荐系统希望用户产生的“正反馈”行为， 因此逻辑回归模型将推荐问题转化成了一个点击率预估问题。而点击率预测就是一个典型的二分类， 正好适合逻辑回归进行处理， 那么逻辑回归是如何做推荐的呢？ 过程如下： 将用户年龄、性别、物品属性、物品描述、当前时间、当前地点等特征转成数值型向量 确定逻辑回归的优化目标，比如把点击率预测转换成二分类问题， 这样就可以得到分类问题常用的损失作为目标， 训练模型 在预测的时候， 将特征向量输入模型产生预测， 得到用户“点击”物品的概率 利用点击概率对候选物品排序， 得到推荐列表 推断过程可以用下图来表示： 这里的关键就是每个特征的权重参数$w$， 我们一般是使用梯度下降的方式， 首先会先随机初始化参数$w$， 然后将特征向量（也就是我们上面数值化出来的特征）输入到模型， 就会通过计算得到模型的预测概率， 然后通过对目标函数求导得到每个$w$的梯度， 然后进行更新$w$ J(w) = - \\frac{1}{m}(\\sum_{i=1}^m (y^ilogf_w(x^i)+(1-y^i)log(1-f_w(x^i))))求导之后： w_j \\leftarrow wj- \\gamma \\frac{1}{m}\\sum_{i=1}^m (f_w(x^i) - y^i)x_j^i这样通过若干次迭代， 就可以得到最终的𝑤w了， 关于这些公式的推导，可以参考下面给出的文章链接， 下面我们分析一下逻辑回归模型的优缺点。 优点 LR模型形式简单，可解释性好，从特征的权重可以看到不同的特征对最后结果的影响。 训练时便于并行化，在预测时只需要对特征进行线性加权，所以性能比较好，往往适合处理海量id类特征，用id类特征有一个很重要的好处，就是防止信息损失（相对于范化的 CTR 特征），对于头部资源会有更细致的描述 资源占用小,尤其是内存。在实际的工程应用中只需要存储权重比较大的特征及特征对应的权重。 方便输出结果调整。逻辑回归可以很方便的得到最后的分类结果，因为输出的是每个样本的概率分数，我们可以很容易的对这些概率分数进行cutoff，也就是划分阈值(大于某个阈值的是一类，小于某个阈值的是一类) 当然， 逻辑回归模型也有一定的局限性 表达能力不强， 无法进行特征交叉， 特征筛选等一系列“高级“操作（这些工作都得人工来干， 这样就需要一定的经验， 否则会走一些弯路）， 因此可能造成信息的损失 准确率并不是很高。因为这毕竟是一个线性模型加了个sigmoid， 形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布 处理非线性数据较麻烦。逻辑回归在不引入其他方法的情况下，只能处理线性可分的数据， 如果想处理非线性， 首先对连续特征的处理需要先进行离散化（离散化的目的是为了引入非线性），如上文所说，人工分桶的方式会引入多种问题。 LR 需要进行人工特征组合，这就需要开发者有非常丰富的领域经验，才能不走弯路。这样的模型迁移起来比较困难，换一个领域又需要重新进行大量的特征工程。 所以如何自动发现有效的特征、特征组合，弥补人工经验不足，缩短LR特征实验周期，是亟需解决的问题， 而GBDT模型， 正好可以自动发现特征并进行有效组合 GBDT模型GBDT全称梯度提升决策树，在传统机器学习算法里面是对真实分布拟合的最好的几种算法之一，在前几年深度学习还没有大行其道之前，gbdt在各种竞赛是大放异彩。原因大概有几个，一是效果确实挺不错。二是即可以用于分类也可以用于回归。三是可以筛选特征， 所以这个模型依然是一个非常重要的模型。 GBDT是通过采用加法模型(即基函数的线性组合），以及不断减小训练过程产生的误差来达到将数据分类或者回归的算法， 其训练过程如下： gbdt通过多轮迭代， 每轮迭代会产生一个弱分类器， 每个分类器在上一轮分类器的残差基础上进行训练。 gbdt对弱分类器的要求一般是足够简单， 并且低方差高偏差。 因为训练的过程是通过降低偏差来不断提高最终分类器的精度。 由于上述高偏差和简单的要求，每个分类回归树的深度不会很深。最终的总分类器是将每轮训练得到的弱分类器加权求和得到的（也就是加法模型）。 关于GBDT的详细细节，依然是可以参考下面给出的链接。这里想分析一下GBDT如何来进行二分类的，因为我们要明确一点就是gbdt 每轮的训练是在上一轮的训练的残差基础之上进行训练的， 而这里的残差指的就是当前模型的负梯度值， 这个就要求每轮迭代的时候，弱分类器的输出的结果相减是有意义的， 而gbdt 无论用于分类还是回归一直都是使用的CART 回归树， 那么既然是回归树， 是如何进行二分类问题的呢？ GBDT 来解决二分类问题和解决回归问题的本质是一样的，都是通过不断构建决策树的方式，使预测结果一步步的接近目标值， 但是二分类问题和回归问题的损失函数是不同的， 关于GBDT在回归问题上的树的生成过程， 损失函数和迭代原理可以参考给出的链接， 回归问题中一般使用的是平方损失， 而二分类问题中， GBDT和逻辑回归一样， 使用的下面这个： L=arg \\, min [\\sum_i^n-(y_ilog(p_i)+(1-y_i)log(1-p_i))]其中， $y_i$是第$i$个样本的观测值， 取值要么是0要么是1， 而是$p_i$第$i$个样本的预测值， 取值是0-1之间的概率，由于我们知道GBDT拟合的残差是当前模型的负梯度， 那么我们就需要求出这个模型的导数， 即$\\frac{dL}{dp_i}$， 对于某个特定的样本， 求导的话就可以只考虑它本身， 去掉加和号， 那么就变成了$\\frac{dl}{dp_i}$， 其中$l$如下： l=-y_ilog(p_i) - (1 - y_i)log(1-p_i) \\\\ =-y_i log(p_i) - log(1 - p_i) + y_i log(1-p_i) \\\\ =-y_i(log(\\frac{p_i}{1-p_i})) - log(1-p_i)如果对逻辑回归非常熟悉的话，$log(\\frac{p_i}{1-p_i})$ 一定不会陌生吧， 这就是对几率比取了个对数， 并且在逻辑回归里面这个式子会等于$\\theta X$，所以才推出了$p_i=\\frac{1}{1+e^{- \\theta X}}$的那个形式。 这里令$\\eta_i=\\frac{p_i}{1-p_i}$，即$p_i=\\frac{\\eta_i}{1+\\eta_i}$，则上面这个式子变成了： l=-y_ilog(\\eta_i)-log(1-\\frac{e^{log(\\eta_i)}}{1+e^{log(\\eta_i)}}) \\\\ = -y_ilog(\\eta_i)-log(\\frac{e^{log(\\eta_i)}}{1+e^{log(\\eta_i)}}) \\\\ =-y_ilog(\\eta_i)+log(1+e^{log(\\eta_i)}) \\\\这时候，我们对$log(\\eta_i)$求导， 得 \\frac{dl}{dlog(\\eta_i)}=-y_i+\\frac{e^{log(\\eta_i)}}{1+e^{log(\\eta_i)}}=-y_i+p_i=p_i-y_i这样， 我们就得到了某个训练样本在当前模型的梯度值了， 那么残差就是$y_i-p_i$。GBDT二分类的这个思想，其实和逻辑回归的思想一样，逻辑回归是用一个线性模型去拟合$P(y=1|x)$这个事件的对数几率$log\\frac{p}{1-p}=\\theta^Tx$， GBDT二分类也是如此， 用一系列的梯度提升树去拟合这个对数几率， 其分类模型可以表达为： P(Y=1|x)=\\frac{1}{1+e^{-F_M(x)}}初始化GBDT 和回归问题一样， 分类 GBDT 的初始状态也只有一个叶子节点，该节点为所有样本的初始预测值，如下： F_0(x)=arg \\, min(\\sum_{i=1}^n L(y, \\gamma))上式里面，$F$代表GBDT模型， $F_0$是模型的初始状态， 该式子的意思是找到一个，$\\gamma$使所有样本的 Loss 最小，在这里及下文中，$\\gamma$都表示节点的输出，即叶子节点， 且它是一个$log(\\eta_i)$形式的值(回归值)，在初始状态，$\\gamma=F_0$。 下面看例子(该例子来自下面的第二个链接)， 假设我们有下面3条样本： 我们希望构建 GBDT 分类树，它能通过「喜欢爆米花」、「年龄」和「颜色偏好」这 3 个特征来预测某一个样本是否喜欢看电影。我们把数据代入上面的公式中求Loss: Loss = L(1, \\gamma) + L(1, \\gamma) + L(0, \\gamma)为了令其最小， 我们求导， 且让导数为0， 则： Loss = p - 1 + p - 1 + p - 0 = 0于是， 就得到了初始值$p=\\frac{2}{3}, \\gamma=log(\\frac{p}{1-p})=0.69$，模型的初始状态$F_0(x) = 0.69$ 说了一大堆，实际上你却可以很容易的算出该模型的初始值，它就是正样本数比上负样本数的 log 值，例子中，正样本数为 2 个，负样本为 1 个，那么： F_0(x)=\\log(\\frac{positive_count}{negative_count})=\\log(\\frac{2}{1})=0.69 循环生成决策树 这里回忆一下回归树的生成步骤， 其实有4小步， 第一就是计算负梯度值得到残差， 第二步是用回归树拟合残差， 第三步是计算叶子节点的输出值， 第四步是更新模型。 下面我们一一来看： 计算负梯度得到残差 r_{im} = - [\\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}]_{F(x)=F_{m-1}(x)}此处使用$m-1$棵树的模型， 计算每个样本的残差,$r_{im}$ 就是上面的$y_i- p_i$, 于是例子中， 每个样本的残差： 使用回归树来拟合$\\gamma_{jm}$， 这里的表$i$示样本哈，回归树的建立过程可以参考下面的链接文章，简单的说就是遍历每个特征， 每个特征下遍历每个取值， 计算分裂后两组数据的平方损失， 找到最小的那个划分节点。 假如我们产生的第2棵决策树如下： 对于每个叶子节点$j$, 计算最佳残差拟合值 \\gamma_{jm}=\\arg \\min_r \\sum_{x \\in R_{ij}} L(y_i, F_{m-1}+\\gamma)意思是， 在刚构建的树$m$中， 找到每个节点$j$的输出$\\gamma_{jm}$, 能使得该节点的loss最小。 那么我们看一下这个$\\gamma$的求解方式， 这里非常的巧妙。 首先， 我们把损失函数写出来， 对于左边的第一个样本， 有 L(y_1,F_{m-1}(x_1)+\\gamma)=-y_1(F_{m-1}(x_1)+\\gamma)+log(1+e^{F_{m-1}(x_1)+\\gamma})这个式子就是上面推导的𝑙l， 因为我们要用回归树做分类， 所以这里把分类的预测概率转换成了对数几率回归的形式， 即$log(\\eta_i)$， 这个就是模型的回归输出值。而如果求这个损失的最小值， 我们要求导， 解出令损失最小的$\\gamma$。 但是上面这个式子求导会很麻烦， 所以这里介绍了一个技巧就是使用二阶泰勒公式来近似表示该式， 再求导， 还记得伟大的泰勒吗？ f(x+\\Delta x) \\approx f(x) + \\Delta f'(x) + \\frac{1}{2} \\Delta x^2 f''(x) + O(\\Delta x)这里就相当于把$L(y1,F{m-1}(x_1))$当作常量$f(x)$，$\\gamma$ 作为变量$\\Delta x$，将$f(x)$二阶展开： L(y_1,F_{m-1}(x_1) + \\gamma) \\approx L(y_1,F_{m-1}(x_1)) + L'(y_1,F_{m-1}(x_1)) \\gamma + \\frac{1}{2}L''(y_1,F_{m-1}(x_1)) \\gamma^2这时候再求导就简单了 \\frac{dL}{d\\gamma}=L'(y_1,F_{m-1}(x_1)) + L''(y_1,F_{m-1}(x_1))\\gammaLoss最小的时候， 上面的式子等于0， 就可以得到$\\gamma$: \\gamma_{11}=\\frac{-L'(y_1,F_{m-1}(x_1))}{L''(y_1,F_{m-1}(x_1))\\gamma}因为分子就是残差(上述已经求到了)， 分母可以通过对残差求导，得到原损失函数的二阶导： 这时候， 就可以算出该节点的输出 $$ \\gamma_&#123;11&#125;=\\frac&#123;r_&#123;11&#125;&#125;&#123;p_&#123;10&#125;(1-p_&#123;10&#125;)&#125;=\\frac&#123;0.33&#125;&#123;0.67 * 0.33&#125;=1.49 $$ 这里的下面$\\gamma_&#123;jm&#125;$表示第$m$棵树的第$j$个叶子节点。 接下来是右边节点的输出， 包含样本2和样本3， 同样使用二阶泰勒公式展开： ![image-20240429115219850](https://s2.loli.net/2024/04/29/kMBUL7gdFIsH5TJ.png) ​ 求导， 令其结果为0，就会得到， 第1棵树的第2个叶子节点的输出： ​ ​ 可以看出， 对于任意叶子节点， 我们可以直接计算其输出值： ​ \\gamma_{jm} = \\frac{\\sum_{i=1}^{R_{ij}}r_{im}}{\\sum_{i=1}^{R_{ij}}p_{i,m-1}(1-p_{i,m-1})} 更新模型$F_m(x)$ ​ F_m(x) = F_{m-1}(x) + v \\sum_{j=1}^{J_m} \\gamma_m下面分析一下GBDT的优缺点： 我们可以把树的生成过程理解成自动进行多维度的特征组合的过程，从根结点到叶子节点上的整个路径(多个特征值判断)，才能最终决定一棵树的预测值， 另外，对于连续型特征的处理，GBDT 可以拆分出一个临界阈值，比如大于 0.027 走左子树，小于等于 0.027（或者 default 值）走右子树，这样很好的规避了人工离散化的问题。这样就非常轻松的解决了逻辑回归那里自动发现特征并进行有效组合的问题， 这也是GBDT的优势所在。 但是GBDT也会有一些局限性， 对于海量的 id 类特征，GBDT 由于树的深度和棵树限制（防止过拟合），不能有效的存储；另外海量特征在也会存在性能瓶颈，当 GBDT 的 one hot 特征大于 10 万维时，就必须做分布式的训练才能保证不爆内存。所以 GBDT 通常配合少量的反馈 CTR 特征来表达，这样虽然具有一定的范化能力，但是同时会有信息损失，对于头部资源不能有效的表达。 所以， 我们发现其实GBDT和LR的优缺点可以进行互补。 GBDT+LR模型2014年， Facebook提出了一种利用GBDT自动进行特征筛选和组合， 进而生成新的离散特征向量， 再把该特征向量当做LR模型的输入， 来产生最后的预测结果， 这就是著名的GBDT+LR模型了。GBDT+LR 使用最广泛的场景是CTR点击率预估，即预测当给用户推送的广告会不会被用户点击。 DeepFM动机对于CTR问题，被证明的最有效的提升任务表现的策略是特征组合(Feature Interaction), 在CTR问题的探究历史上来看就是如何更好地学习特征组合，进而更加精确地描述数据的特点。可以说这是基础推荐模型到深度学习推荐模型遵循的一个主要的思想。而组合特征大牛们研究过组合二阶特征，三阶甚至更高阶，但是面临一个问题就是随着阶数的提升，复杂度就成几何倍的升高。这样即使模型的表现更好了，但是推荐系统在实时性的要求也不能满足了。所以很多模型的出现都是为了解决另外一个更加深入的问题：如何更高效的学习特征组合？ 为了解决上述问题，出现了FM和FFM来优化LR的特征组合较差这一个问题。并且在这个时候科学家们已经发现了DNN在特征组合方面的优势，所以又出现了FNN和PNN等使用深度网络的模型。但是DNN也存在局限性。 DNN局限 当我们使用DNN网络解决推荐问题的时候存在网络参数过于庞大的问题，这是因为在进行特征处理的时候我们需要使用one-hot编码来处理离散特征，这会导致输入的维度猛增。这里借用AI大会的一张图片： 这样庞大的参数量也是不实际的。为了解决DNN参数量过大的局限性，可以采用非常经典的Field思想，将OneHot特征转换为Dense Vector 此时通过增加全连接层就可以实现高阶的特征组合，如下图所示： 但是仍然缺少低阶的特征组合，于是增加FM来表示低阶的特征组合。 FNN和PNN 结合FM和DNN其实有两种方式，可以并行结合也可以串行结合。这两种方式各有几种代表模型。在DeepFM之前有FNN，虽然在影响力上可能并不如DeepFM，但是了解FNN的思想对我们理解DeepFM的特点和优点是很有帮助的。 FNN是使用预训练好的FM模块，得到隐向量，然后把隐向量作为DNN的输入，但是经过实验进一步发现，在Embedding layer和hidden layer1之间增加一个product层（如上图所示）可以提高模型的表现，所以提出了PNN，使用product layer替换FM预训练层。 Wide&amp;Deep FNN和PNN模型仍然有一个比较明显的尚未解决的缺点：对于低阶组合特征学习到的比较少，这一点主要是由于FM和DNN的串行方式导致的，也就是虽然FM学到了低阶特征组合，但是DNN的全连接结构导致低阶特征并不能在DNN的输出端较好的表现。看来我们已经找到问题了，将串行方式改进为并行方式能比较好的解决这个问题。于是Google提出了Wide&amp;Deep模型（将前几章），但是如果深入探究Wide&amp;Deep的构成方式，虽然将整个模型的结构调整为了并行结构，在实际的使用中Wide Module中的部分需要较为精巧的特征工程，换句话说人工处理对于模型的效果具有比较大的影响（这一点可以在Wide&amp;Deep模型部分得到验证）。 2.2.2特征交叉DCN动机Wide&amp;Deep模型的提出不仅综合了“记忆能力”和“泛化能力”， 而且开启了不同网络结构融合的新思路。 所以后面就有各式各样的模型改进Wide部分或者Deep部分， 而Deep&amp;Cross模型(DCN)就是其中比较典型的一个，这是2017年斯坦福大学和谷歌的研究人员在ADKDD会议上提出的， 该模型针对W&amp;D的wide部分进行了改进， 因为Wide部分有一个不足就是需要人工进行特征的组合筛选， 过程繁琐且需要经验， 而2阶的FM模型在线性的时间复杂度中自动进行特征交互，但是这些特征交互的表现能力并不够，并且随着阶数的上升，模型复杂度会大幅度提高。于是乎，作者用一个Cross Network替换掉了Wide部分，来自动进行特征之间的交叉，并且网络的时间和空间复杂度都是线性的。 通过与Deep部分相结合，构成了深度交叉网络（Deep &amp; Cross Network），简称DCN。 模型结构及原理 这个模型的结构也是比较简洁的， 从下到上依次为：Embedding和Stacking层， Cross网络层与Deep网络层并列， 以及最后的输出层。下面也是一一为大家剖析。 Embedding和Stacking层Embedding层我们已经非常的熟悉了吧， 这里的作用依然是把稀疏离散的类别型特征变成低维密集型。 X_{embed,i}=W_{embed,iX_i}其中对于某一类稀疏分类特征（如id），$X{embed,i}$是第$i$个分类值（id序号）的embedding向量。$W{embed,i}$是embedding矩阵，$n_e \\times n_v$维度，$n_e$是embedding维度，$n_v$是该类特征的唯一取值个数。$x_i$属于该特征的二元稀疏向量(one-hot)编码的。【实质上就是在训练得到的Embedding参数矩阵中找到属于当前样本对应的Embedding向量】。其实绝大多数基于深度学习的推荐模型都需要Embedding操作，参数学习是通过神经网络进行训练。 最后，该层需要将所有的密集型特征与通过embedding转换后的特征进行联合（Stacking）： X_0=[X^T_{embed,1},...,X^T_{embed,k},X^T_{dense}]一共𝑘k个类别特征， dense是数值型特征， 两者在特征维度拼在一块。 上面的这两个操作如果是看了前面的模型的话，应该非常容易理解了。 Cross NetWork这个就是本模型最大的亮点了【Cross网络】， 这个思路感觉非常Nice。设计该网络的目的是增加特征之间的交互力度。交叉网络由多个交叉层组成， 假设第$l$层的输出向量$xl$， 那么对于第$l+1$层的输出向量$x{l+1}$表示为： x_{l+1}=x_0x_l^Tw_l+b_l+x_l=f(x_l,w_l,b_l) + x_l可以看到， 交叉层的二阶部分非常类似PNN提到的外积操作， 在此基础上增加了外积操作的权重向量$w_l$， 以及原输入向量和$x_l$偏置向量$b_l$。 交叉层的可视化如下： 可以看到， 每一层增加了一个$n$维的权重向量$w_l$（n表示输入向量维度）， 并且在每一层均保留了输入向量， 因此输入和输出之间的变化不会特别明显。关于这一层， 原论文里面有个具体的证明推导Cross Network为啥有效， 不过比较复杂，这里我拿一个式子简单的解释下上面这个公式的伟大之处： 我们根据上面这个公式， 尝试的写前面几层看看: $l=0:\\mathbf{x_1}=\\mathbf{x_0}\\mathbf{x_0^T}\\mathbf{w_0}+\\mathbf{b_0}+\\mathbf{x_0}$ $l=1:\\mathbf{x_2}=\\mathbf{x_0}\\mathbf{x_1^T}\\mathbf{w_1}+\\mathbf{b_1}+\\mathbf{x_1}=\\mathbf{x_0}[\\mathbf{x_0}\\mathbf{x_0^T}\\mathbf{w_0}+\\mathbf{b_0}+\\mathbf{x_0}]^T\\mathbf{w_1}+\\mathbf{b_1}+\\mathbf{x_1}$ $l=2:\\mathbf{x_3}=\\mathbf{x_0}\\mathbf{x_2^T}\\mathbf{w_2}+\\mathbf{b_2}+\\mathbf{x_2}=\\mathbf{x_0}[\\mathbf{x_0}[\\mathbf{x_0}\\mathbf{x_0^T}\\mathbf{w_0}+\\mathbf{b_0}+\\mathbf{x_0}]^T\\mathbf{w_1}+\\mathbf{b_1}+\\mathbf{x_1}]^T\\mathbf{w_2}+\\mathbf{b_2}+\\mathbf{x_2}$ 我们暂且写到第3层的计算， 我们会发现什么结论呢？ 给大家总结一下： $x_1$中包含了所有的$x_0$的1,2阶特征的交互，$x_2$包含了所有的$x_1,x_0$的1、2、3阶特征的交互，$x_3$中包含了所有的$x_2,x_1,x_0$的交互，$x_0$的1、2、3、4阶特征交互。 因此， 交叉网络层的叉乘阶数是有限的。第$l$层特征对应的最高的叉乘阶数$l+1$ Cross网络的参数是共享的， 每一层的这个权重特征之间共享， 这个可以使得模型泛化到看不见的特征交互作用， 并且对噪声更具有鲁棒性。 例如两个稀疏的特征$x_i,x_j$，它们在数据中几乎不发生交互，那么学习$x_i,x_j$的权重对于预测没有任何的意义。 计算交叉网络的参数数量。 假设交叉层的数量是$L_c$，特征$x$的维度是$n$， 那么总共的参数是： n \\times L_c \\times 2这个就是每一层会有$w$和$b$。且$w$维度和$x$的维度是一致的。 交叉网络的时间和空间复杂度是线性的。这是因为， 每一层都只有$w$和$b$ 没有激活函数的存在，相对于深度学习网络， 交叉网络的复杂性可以忽略不计。 Cross网络是FM的泛化形式， 在FM模型中， 特征$xi$的权重是$v_i$，那么交叉项$x_i,x_j$的权重为$\\langle x_i,x_j \\rangle$。在DCN中，$x_i$的权重为${W{K}^{(i)^l}}{k=1}$， 交叉项$x_i,x_j$的权重是参数${W{K}^{(i)^l}}{k=1}$和${W{K}^{(j)^l}}_{k=1}$的乘积，这个看上面那个例子展开感受下。因此两个模型都各自学习了独立于其他特征的一些参数，并且交叉项的权重是相应参数的某种组合。FM只局限于2阶的特征交叉(一般)，而DCN可以构建更高阶的特征交互， 阶数由网络深度决定，并且交叉网络的参数只依据输入的维度线性增长。 还有一点我们也要了解，对于每一层的计算中， 都会跟着$x_0$, 这个是咱们的原始输入， 之所以会乘以一个这个，是为了保证后面不管怎么交叉，都不能偏离我们的原始输入太远，别最后交叉交叉都跑偏了。 $\\mathbf{x_{l+1}}=f(\\mathbf{x_l},\\mathbf{w_l}.\\mathbf{b_l})+\\mathbf{x_l}$, 这个东西其实有点跳远连接的意思，也就是和ResNet也有点相似，无形之中还能有效的缓解梯度消失现象。 好了， 关于本模型的交叉网络的细节就介绍到这里了。这应该也是本模型的精华之处了，后面就简单了。 Deep Network这个就和上面的D&amp;W的全连接层原理一样。这里不再过多的赘述。 \\mathbf{h_{l+1}}=f(w_l\\mathbf{h_l}+\\mathbf{b_l})具体的可以参考W&amp;D模型。 组合输出层这个层负责将两个网络的输出进行拼接， 并且通过简单的Logistics回归完成最后的预测： p=\\sigma([\\mathbf{x_{L_1}^T},\\mathbf{h_{L_2}^T}]\\mathbf{w}_{logits})其中$\\mathbf{x{L_1}^T},\\mathbf{h{L_2}^T}$分别表示交叉网络和深度网络的输出。 最后二分类的损失函数依然是交叉熵损失： loss=-\\frac{1}{N}\\sum_{i=1}^Ny_i\\log(p_i)+(1-y_i)\\log(1-p_i)+\\lambda \\sum_l ||\\mathbf{w}_i||^2Cross&amp;Deep模型的原理就是这些了，其核心部分就是Cross Network， 这个可以进行特征的自动交叉， 避免了更多基于业务理解的人工特征组合。 该模型相比于W&amp;D，Cross部分表达能力更强， 使得模型具备了更强的非线性学习能力。 2.2.4序列模型DIN动机Deep Interest Network(DIIN)是2018年阿里巴巴提出来的模型， 该模型基于业务的观察，从实际应用的角度进行改进，相比于之前很多“学术风”的深度模型， 该模型更加具有业务气息。该模型的应用场景是阿里巴巴的电商广告推荐业务， 这样的场景下一般会有大量的用户历史行为信息， 这个其实是很关键的，因为DIN模型的创新点或者解决的问题就是使用了注意力机制来对用户的兴趣动态模拟， 而这个模拟过程存在的前提就是用户之前有大量的历史行为了，这样我们在预测某个商品广告用户是否点击的时候，就可以参考他之前购买过或者查看过的商品，这样就能猜测出用户的大致兴趣来，这样我们的推荐才能做的更加到位，所以这个模型的使用场景是非常注重用户的历史行为特征（历史购买过的商品或者类别信息），也希望通过这一点，能够和前面的一些深度学习模型对比一下。 在个性化的电商广告推荐业务场景中，也正式由于用户留下了大量的历史交互行为，才更加看出了之前的深度学习模型(作者统称Embeding&amp;MLP模型)的不足之处。如果学习了前面的各种深度学习模型，就会发现Embeding&amp;MLP模型对于这种推荐任务一般有着差不多的固定处理套路，就是大量稀疏特征先经过embedding层， 转成低维稠密的，然后进行拼接，最后喂入到多层神经网络中去。 这些模型在这种个性化广告点击预测任务中存在的问题就是无法表达用户广泛的兴趣，因为这些模型在得到各个特征的embedding之后，就蛮力拼接了，然后就各种交叉等。这时候根本没有考虑之前用户历史行为商品具体是什么，究竟用户历史行为中的哪个会对当前的点击预测带来积极的作用。 而实际上，对于用户点不点击当前的商品广告，很大程度上是依赖于他的历史行为的，王喆老师举了个例子 假设广告中的商品是键盘， 如果用户历史点击的商品中有化妆品， 包包，衣服， 洗面奶等商品， 那么大概率上该用户可能是对键盘不感兴趣的， 而如果用户历史行为中的商品有鼠标， 电脑，iPad，手机等， 那么大概率该用户对键盘是感兴趣的， 而如果用户历史商品中有鼠标， 化妆品， T-shirt和洗面奶， 鼠标这个商品embedding对预测“键盘”广告的点击率的重要程度应该大于后面的那三个。 这里也就是说如果是之前的那些深度学习模型，是没法很好的去表达出用户这广泛多样的兴趣的，如果想表达的准确些， 那么就得加大隐向量的维度，让每个特征的信息更加丰富， 那这样带来的问题就是计算量上去了，毕竟真实情景尤其是电商广告推荐的场景，特征维度的规模是非常大的。 并且根据上面的例子， 也并不是用户所有的历史行为特征都会对某个商品广告点击预测起到作用。所以对于当前某个商品广告的点击预测任务，没必要考虑之前所有的用户历史行为。 这样， DIN的动机就出来了，在业务的角度，我们应该自适应的去捕捉用户的兴趣变化，这样才能较为准确的实施广告推荐；而放到模型的角度， 我们应该考虑到用户的历史行为商品与当前商品广告的一个关联性，如果用户历史商品中很多与当前商品关联，那么说明该商品可能符合用户的品味，就把该广告推荐给他。而一谈到关联性的话， 我们就容易想到“注意力”的思想了， 所以为了更好的从用户的历史行为中学习到与当前商品广告的关联性，学习到用户的兴趣变化， 作者把注意力引入到了模型，设计了一个”local activation unit”结构，利用候选商品和历史问题商品之间的相关性计算出权重，这个就代表了对于当前商品广告的预测，用户历史行为的各个商品的重要程度大小， 而加入了注意力权重的深度学习网络，就是这次的主角DIN， 下面具体来看下该模型。 DIN模型结构及原理在具体分析DIN模型之前， 我们还得先介绍两块小内容，一个是DIN模型的数据集和特征表示， 一个是上面提到的之前深度学习模型的基线模型， 有了这两个， 再看DIN模型，就感觉是水到渠成了。 特征表示工业上的CTR预测数据集一般都是multi-group categorial form的形式，就是类别型特征最为常见，这种数据集一般长这样： 这里的亮点就是框出来的那个特征，这个包含着丰富的用户兴趣信息。 对于特征编码，作者这里举了个例子：[weekday=Friday, gender=Female, visited_cate_ids=&#123;Bag,Book&#125;, ad_cate_id=Book]， 这种情况我们知道一般是通过one-hot的形式对其编码， 转成系数的二值特征的形式。但是这里我们会发现一个visted_cate_ids， 也就是用户的历史商品列表， 对于某个用户来讲，这个值是个多值型的特征， 而且还要知道这个特征的长度不一样长，也就是用户购买的历史商品个数不一样多，这个显然。这个特征的话，我们一般是用到multi-hot编码，也就是可能不止1个1了，有哪个商品，对应位置就是1， 所以经过编码后的数据长下面这个样子： 这个就是喂入模型的数据格式了，这里还要注意一点 就是上面的特征里面没有任何的交互组合，也就是没有做特征交叉。这个交互信息交给后面的神经网络去学习。 基线模型这里的base 模型，就是上面提到过的Embedding&amp;MLP的形式， 这个之所以要介绍，就是因为DIN网络的基准也是他，只不过在这个的基础上添加了一个新结构(注意力网络)来学习当前候选广告与用户历史行为特征的相关性，从而动态捕捉用户的兴趣。 基准模型的结构相对比较简单，我们前面也一直用这个基准， 分为三大模块：Embedding layer，Pooling &amp; Concat layer和MLP， 结构如下: 前面的大部分深度模型结构也是遵循着这个范式套路， 简介一下各个模块。 Embedding layer：这个层的作用是把高维稀疏的输入转成低维稠密向量， 每个离散特征下面都会对应着一个embedding词典， 维度是$D \\times K$， 这里的$D$表示的是隐向量的维度， 而表$K$示的是当前离散特征的唯一取值个数, 这里为了好理解，这里举个例子说明，就比如上面的weekday特征： 假设某个用户的weekday特征就是周五，化成one-hot编码的时候，就是[0,0,0,0,1,0,0]表示，这里如果再假设隐向量维度是D， 那么这个特征对应的embedding词典是一个$D\\times7$的一个矩阵(每一列代表一个embedding，7列正好7个embedding向量，对应周一到周日)，那么该用户这个one-hot向量经过embedding层之后会得到一个𝐷×1D×1的向量，也就是周五对应的那个embedding，怎么算的，其实就是$embedding矩阵*[0,0,0,0,1,0,0]^T$。其实也就是直接把embedding矩阵中one-hot向量为1的那个位置的embedding向量拿出来。 这样就得到了稀疏特征的稠密向量了。其他离散特征也是同理，只不过上面那个multi-hot编码的那个，会得到一个embedding向量的列表，因为他开始的那个multi-hot向量不止有一个是1，这样乘以embedding矩阵，就会得到一个列表了。通过这个层，上面的输入特征都可以拿到相应的稠密embedding向量了。 pooling layer and Concat layer： pooling层的作用是将用户的历史行为embedding这个最终变成一个定长的向量，因为每个用户历史购买的商品数是不一样的， 也就是每个用户multi-hot中1的个数不一致，这样经过embedding层，得到的用户历史行为embedding的个数不一样多，也就是上面的embedding列表$t_i$不一样长， 那么这样的话，每个用户的历史行为特征拼起来就不一样长了。 而后面如果加全连接网络的话，我们知道，他需要定长的特征输入。 所以往往用一个pooling layer先把用户历史行为embedding变成固定长度(统一长度)，所以有了这个公式： e_i=pooling(e_{i1},e_{i2},...,e_{ik})这里的$e_{ij}$是用户历史行为的那些embedding。就$e_i$变成了定长的向量， 这里的$e_i$表示第$i$个历史特征组(是历史行为，比如历史的商品id，历史的商品类别id等)， 这里的$k$表示对应历史特种组里面用户购买过的商品数量，也就是历史embedding的数量，看上面图里面的user behaviors系列，就是那个过程了。 Concat layer层的作用就是拼接了，就是把这所有的特征embedding向量，如果再有连续特征的话也算上，从特征维度拼接整合，作为MLP的输入。 MLP：这个就是普通的全连接，用了学习特征之间的各种交互。 Loss: 由于这里是点击率预测任务， 二分类的问题，所以这里的损失函数用的负的log对数似然： L=-\\frac{1}{N}\\sum_{(\\mathbf{x},y)\\in S}(y \\log p(x) + (1-y) \\log (1-p(\\mathbf(x)))) 这就是base 模型的全貌， 这里应该能看出这种模型的问题， 通过上面的图也能看出来， 用户的历史行为特征和当前的候选广告特征在全都拼起来给神经网络之前，是一点交互的过程都没有， 而拼起来之后给神经网络，虽然是有了交互了，但是原来的一些信息，比如，每个历史商品的信息会丢失了一部分，因为这个与当前候选广告商品交互的是池化后的历史特征embedding， 这个embedding是综合了所有的历史商品信息， 这个通过我们前面的分析，对于预测当前广告点击率，并不是所有历史商品都有用，综合所有的商品信息反而会增加一些噪声性的信息，可以联想上面举得那个键盘鼠标的例子，如果加上了各种洗面奶，衣服啥的反而会起到反作用。其次就是这样综合起来，已经没法再看出到底用户历史行为中的哪个商品与当前商品比较相关，也就是丢失了历史行为中各个商品对当前预测的重要性程度。最后一点就是如果所有用户浏览过的历史行为商品，最后都通过embedding和pooling转换成了固定长度的embedding，这样会限制模型学习用户的多样化兴趣。 那么改进这个问题的思路有哪些呢？ 第一个就是加大embedding的维度，增加之前各个商品的表达能力，这样即使综合起来，embedding的表达能力也会加强， 能够蕴涵用户的兴趣信息，但是这个在大规模的真实推荐场景计算量超级大，不可取。 另外一个思路就是在当前候选广告和用户的历史行为之间引入注意力的机制，这样在预测当前广告是否点击的时候，让模型更关注于与当前广告相关的那些用户历史产品，也就是说与当前商品更加相关的历史行为更能促进用户的点击行为。 作者这里又举了之前的一个例子： 想象一下，当一个年轻母亲访问电子商务网站时，她发现展示的新手袋很可爱，就点击它。让我们来分析一下点击行为的驱动力。 展示的广告通过软搜索这位年轻母亲的历史行为，发现她最近曾浏览过类似的商品，如大手提袋和皮包，从而击中了她的相关兴趣 第二个思路就是DIN的改进之处了。DIN通过给定一个候选广告，然后去注意与该广告相关的局部兴趣的表示来模拟此过程。 DIN不会通过使用同一向量来表达所有用户的不同兴趣，而是通过考虑历史行为的相关性来自适应地计算用户兴趣的表示向量（对于给的广告）。 该表示向量随不同广告而变化。下面看一下DIN模型。 DIN模型架构上面分析完了base模型的不足和改进思路之后，DIN模型的结构就呼之欲出了，首先，它依然是采用了基模型的结构，只不过是在这个的基础上加了一个注意力机制来学习用户兴趣与当前候选广告间的关联程度， 用论文里面的话是，引入了一个新的local activation unit， 这个东西用在了用户历史行为特征上面， 能够根据用户历史行为特征和当前广告的相关性给用户历史行为特征embedding进行加权。我们先看一下它的结构，然后看一下这个加权公式。 这里改进的地方已经框出来了，这里会发现相比于base model， 这里加了一个local activation unit， 这里面是一个前馈神经网络，输入是用户历史行为商品和当前的候选商品， 输出是它俩之间的相关性， 这个相关性相当于每个历史商品的权重，把这个权重与原来的历史行为embedding相乘求和就得到了用户的兴趣表示𝑣𝑈(𝐴)v*U(A*), 这个东西的计算公式如下： 第四章 推荐系统算法面经4.1ML与DL基础机器学习 介绍一个最熟悉的机器学习算法 参考解析机器学习 介绍一个最熟悉的机器学习算法 LR：逻辑回归是假设数据服从伯努利分布，通过极大似然估计方法，使用梯度下降来求解参数，达到二分类目的的一个模型。我们在考虑把广义线性模型用于分类的时候，需要如何确定逻辑边界，感知机模型用的是阶跃函数，但是阶跃函数不可导，不能作为广义线性模型的联系函数。逻辑回归对数几率函数代替阶跃函数。因为对数几率函数是单调可微的一个函数，所以可以作为联系函数。所以逻辑回归本质上还是广义线性模型。 LR的优缺点： 形式简单，可解释性好； 它直接对分类概率进行建模，不需要知道真实数据的分布，这和生成式模型相区别，避免了假设错误带来的问题； 不仅能够预测出类别，还能够预测出概率，能够用于很多场景，比如ctr排序中； 对数几率函数任意阶数可导，能够很容易优化； 可以获得特征权重，方便我们进行特征筛选； 训练速度快； 它对稀疏特征效果比较好，因为使用的是w1 w2 w3本质上的线性模型，稀疏数据能够筛选出不稀疏的重要特征。 模型表达能力有限； 样本不均衡很难处理； 在非线性可分数据集上性能有限； LR推导： 决策树怎么建树，基尼系数公式 决策树建树算法有三种ID3、C4.5、CART，每个算法主要考虑的事情主要有三个问题： 选什么特征来当条件？ 条件判断的属性值是什么？ 什么时候停止分裂，达到我们需要的决策？ CART CART树采用基尼系数进行最优特征的选择，构造过程中假设有K类，则样本属于第K类的概率为pk，则定义样本分布的基尼系数为： Gini(p)=\\sum_{k=1}^m p_k(1-p_k)=1-\\sum_{k=1}^K p_k^2根据基尼系数定义，可以得到样本集合D的基尼指数，其中ck表述样本集合中第k类的子集： Gini(D)=1-\\sum_{k=1}^K(\\frac{C_k}{D})^2如果数据集D根据特征A在某一取值a上进行分割，得到D1,D2两部分后，那么在特征A下集合D的基尼系数如下所示。其中基尼系数Gini(D)表示集合D的不确定性，基尼系数Gini(D,A)表示A=a分割后集合D的不确定性。基尼指数越大，样本集合的不确定性越大。 Gain\\_Gini(D,A)=\\frac{D_1}{D}Gini(D_1)+\\frac{D_2}{D}Gini(D_2) 选什么特征作为最优特征分割：当我们计算完所有特征基尼指数后，选择其中最小所在特征的作为分裂特征； 条件判断的属性值是什么：判断特征属性是否为为此最指数来分裂； 什么时候停止分裂，达到我们需要的决策：分裂的最小收益小于我们的划定的阈值，或者树的深度达到我们的阈值。 Adaboost拟合目标是什么 Adaboost中每训练完一个弱分类器都就会调整权重，上一轮训练中被误分类的点的权重会增加，在本轮训练中，由于权重影响，本轮的弱分类器将更有可能把上一轮的误分类点分对，如果还是没有分对，那么分错的点的权重将继续增加，下一个弱分类器将更加关注这个点，这是adaboost目标。 Adaboost介绍一下，每个基学习器的权重怎么得到的 介绍下GBDT 介绍XGBoost 介绍下LightGBM LightGBM相对于XGBoost的改进 GBDT中的梯度是什么，怎么用 GBDT如何计算特征重要性 GBDT讲一下，GBDT拟合残差，是真实的误差嘛，在什么情况下看做是真实的误差 references: ‘datawhale-FunRec)‘ https://mp.weixin.qq.com/s/ZtnaQrVIpVOPJpqMdLWOcw https://chenk.tech/posts/8ad63d9d.html B站黑马推荐系统实战课程","tags":["DeepFM","FFM"],"categories":["搜广推"]},{"path":"/2024/03/23/tong-ji-xue-xi-fang-fa/","content":"10.1 提升方法AdaBoost算法"},{"path":"/2024/01/16/53-li-jie-dropout/","content":"53 理解 Dropout53.1Why does drop-out work?"},{"path":"/2024/01/15/xi-gua-shu/","content":"0204 K折交叉验证 0205 测试集分割留出法 0206 验证集 0207 均方误差 0208错误率与精确度公式 0209查准率与查全率 0210P-R反向关系原理 0211 P-R反向关系图 0212 P-R加权调和平均数 0213 macro_microP-R 0214 使用P-R曲线比较不同模型 0215 ROC曲线 0216 排序损失rank-loss 0217 AUC与rank-loss 0403 熵的度量：一般分布 0404 信息的度量：信息增益 0405 决策树ID3算法举例：好坏西瓜 0406 好坏西瓜继续分叉 0407 增益率简述 0408 决策树CART基尼指数 0409 基尼指数计算：第一次分叉 0410 基尼指数计算：第二次分叉 0411 基尼指数计算：第三次分叉与终止 0412 CART算法回归树 多个特征，第一步算法SSR最小值 防止过拟合：如设定小于20不再分割 0413 剪枝处理在训练集上使用信息增益得到决策树： 0414 预剪枝一预剪枝是一种贪心算法，能剪就剪 0415 预剪枝二 最后这里，全部是坏瓜，所以也不再分 经过预剪枝，得到的图是： 0416 后剪枝后剪枝是能不剪就不剪 最终是： 补充：Prune Regression trees 0417 连续值C4.5二分法信息增益最大准则 0418 决策树缺失值处理 0701 贝叶斯分类器综述0702 贝叶斯定理—一个应用https://www.matongxue.com/madocs/279/ 0703 贝叶斯定理https://www.matongxue.com/madocs/279/ 0704 预热——一个半朴素贝叶斯的例子 0705贝叶斯决策论 0706 最大似然估计https://www.matongxue.com/madocs/447/ 0709 最大似然估计——公式与取对数 0711 拉普拉斯修正 拉普拉斯修正避免了因为训练集不充分而导致的概率估计为0的情况 拉普拉斯修正实际上是假设了属性值与类别均匀分布，这是在朴素贝叶斯学习过程中额外引入的关于数据的先验 0712 EM算法 0802 集成学习的威力 0803 AdaBoost原理看个例子包懂 https://zhuanlan.zhihu.com/p/27126737 0804 AdaBoost算法实现0807 Gradient Boosting简述"},{"path":"/2024/01/13/52-dropout-zheng-ze-hua/","content":"52 Dropout 正则化52.1Dropout regularization 52.2Implementing dropout(“Inverted dropout”) 52.3Making predictions at test time"},{"path":"/2024/01/06/51-wei-shi-me-zheng-ze-hua-ke-yi-jian-shao-guo-ni-he/","content":"51 为什么正则化可以减少过拟合？51.1 How does regularization prevent overfitting?"},{"path":"/2024/01/05/50-zheng-ze-hua/","content":"50 正则化logistic regression Neural network"},{"path":"/2024/01/05/49-ji-qi-xue-xi-ji-chu/","content":"49 机器学习基础Basic recipe for machine learning"},{"path":"/2024/01/05/48-pian-chai-he-fang-chai/","content":"48 偏差和方差Bias and Variance High bias and high variance"},{"path":"/2024/01/02/41-da-jian-shen-ceng-shen-jing-wang-luo-kuai/","content":"41 搭建深层神经网络块41.1 Forward and backward functions"},{"path":"/2024/01/02/43-zhe-he-da-nao-you-shi-me-guan-xi/","content":"43 这和大脑有什么关系？43.1 Forward and backward propagation"},{"path":"/2024/01/02/47-xun-lian-kai-fa-ce-shi-ji-he/","content":"47 训练 开发 测试集合47.1 Applied ML is a highly iterative process 47.2 Train/dev/test sets Mismatched train/test distribution"},{"path":"/2024/01/02/42-can-shu-vs-chao-can-shu/","content":"42 参数 vs 超参数42.1 What are hyperparameters? 42.2 Applied deep learning is a very empirical proecss"},{"path":"/2024/01/02/40-wei-shi-me-yao-shi-yong-shen-ceng-biao-shi/","content":"40 为什么要使用深层表示？40.1 Intuition about deep representation在人脸识别系统中 第一层：可以当作 feature detector 或者 edge detector 其他层：有的在找眼睛，有的在找鼻子 最后一层：综合各个层找到的结果，就可以做人脸识别 从左往右，层数逐渐加深的过程中，神经元可以探测到的区域是逐渐变大，或者逐渐具有现实意义的。例如：一开始探测只是像素边缘，而后可以探测到眼睛、鼻子、嘴巴，最后，就可以探测到整张人脸了 上述使用的金字塔形状的深度网络，也可以用于探测其他类似于图像的数据，如 Audio 前几层就是得到比较低层次的音频波形的一些特征，比如音调的高低、分别白噪声等 把这些低层次的波形进行组合，就可以去探测声音的基本单元，叫做音位 有了音位就可以辨识出单词发音 有了单词发音，就可以得到词组，再得到句子 Circuit theory and deep learning"},{"path":"/2024/01/02/building-recommender-system-with-gnn/","content":"Building Recommender System with GNNPart1: Intro to GNN"},{"path":"/2024/01/02/graph-neural-networks/","content":"Graph Neural Networks"},{"path":"/2024/01/01/bert-and-its-family/","content":"BERT and its family Pre-train Model Bigger Model Smaller Model Network Architecture How to finetune NLP tasks Input Output Copy from Input(BERT) General Sequence(V1) General Sequence(V2) Adaptor Weighted Features Why Pre-train Models? Why Fine-tune?"},{"path":"/2024/01/01/transformer/","content":"TransformerSequence-to-sequence (Seq2seq) Hokkien（闽南语、台语） Text to Speech (TTS) Synthesis Seq2Seq for Chatbot Most Natural Language Processing appliactions… Seq2seq for Syntactic Parsing Seq2seq for Multi-label Classification Seq2seq for Object Detection Seq2seq Encoder Batch Norm：同一个 dimension ，不同 feature，不同 example，去计算 mean $m$ 和 standard deviation $\\sigma$ Layer Norm：同一个 example，同一个 feature，不同的 dimension 去计算 mean $m$ 和 standard deviation $\\sigma$ To learn more…… Autoregressive Self-attention-&gt;Masked Self-attention AT vs NAT Transformer Cross Attention Training Copy Mechanism Guided Attention Beam Search Optimizing Evaluation Metrics? exposure bias"},{"path":"/2023/12/29/zi-zhu-yi-li-ji-zhi/","content":"自注意力机制Sophisticated input Vector Set as Input What is the output? Sequence Labeling Self-attention 矩阵计算 Multi-head Self-attention Position Encoding Self-attention for Speech Self-attention for Image Self-Attention GAN Self-attention v.s. CNN Self-attention v.s. RNN Self-attention v.s. GNN To Leain More"},{"path":"/2023/12/24/39-he-dui-ju-zhen-de-wei-shu/","content":"39 核对矩阵的维数深度学习中深度神经网络的正向传播和反向传播的过程。包含神经网络中各个层的参数和输入特性的尺寸，以及如何确保矩阵和向量的尺寸一致性。通过正向传播，我们可以计算出网络的输出和损失函数，而反向传播则用与计算梯度并更新参数。还包含深度神经网络相较于浅层表示的优势。 深度神经网络的调试方法，包括价差代码正确性、计算矩阵和向量的维数等。 深度神经网络的搭建和维度解释 矩阵运算规则和向量化实现 关键概念和训练集大小 矩阵运算和尺寸一致性对神经网络反向传播的重要性，并提到了正向传播 矩阵维度和尺寸确定：在神经网络中，确定矩阵的维度和尺寸对于实现反向传播非常重要 深度神经网络的优势：深度神经网络相较于浅层表示更好，但其优势是什么？ 正向传播和效率：介绍了正向传播的过程，并强调了在实现深度神经网络时保持矩阵和向量尺寸的一致性可以提高效率。 39.1 Parameters $W^{[l]} \\space and \\space b^{[l]}$ 39.2 Vectorized implementation"},{"path":"/2023/12/23/38-shen-ceng-wang-luo-zhong-de-qian-xiang-chuan-bo/","content":"38 深层网络中的前向传播深度神经网络的符号和如何在深度网络中执行正向传播。对于单个训练实例 x 的前向传播，首先计算第一层的激活，然后计算第二层到输出层的激活，最终得到预测输出。还讨论向量化方法和循环实现正向传播的必要性。 深度神经网络的正向传播过程，对于单个训练实例的前向传播的计算方法 深度神经网络的正向传播 向量化方法在整个训练集上的正向传播 输入特征向量 x 在计算激活时等于零层的激活 深度神经网络的符号和正向传播的实现，包括将小写字母替换为大写字母、循环计算激活量等。 深度学习的符号和表示法 训练集和表示和预测结果 深度神经网络的实现和符号表示 38.1 Forward propagation in a deep network"},{"path":"/2023/12/22/37-qian-xiang-chuan-bo-he-fan-xiang-chuan-bo/","content":"37 前向传播和反向传播实现深度神经网路的基本模块，包括前向传播和反向传播。其中，前向传播涉及输入、减去一个、和产出，反向传播涉及导数、梯度和损失函数。通过实现这些方程，可以得到正确的前向传播和反向传播的实现，以及所需的导数。作者建议在开始编程联系前，先研究微积分和线性代数，以便于更好地理解推导。 如何实现深度神经网络的基本模型，包括前向传播和反向传播步骤。 介绍深度神经网络的基本模型和前向传播的步骤 讨论从零开始的前向传播和计算激活函数的方法 提供四个方程用于实现反向传播，并给出向量化的版本 如何实现三层神经网络的前向传播和反向传播，以及如何计算损失函数的导数。 讨论如何计算数据库 介绍三层神经网络的前向和反向传播 证明损失函数对输出导数等于导入乘以导数乘以 a 的公式 机器学习中向量化的实现方法，包括初始化向量化和反向传播 学习如何初始化向量化的反向传播 机器学习中最困难的推导之一 超参数和参数在神经网络中的重要性 37.1 Forward propagation for layer $l$ 37.2 Backward propagation for layer $l$ 37.3 Summary"},{"path":"/2023/12/22/36-shen-du-shen-jing-wang-luo/","content":"36 深度神经网络深度神经网络的概念和符号表示。与逻辑回归不同，深度神经网络可以处理更深层次的模型，以解决更加复杂的问题。本文介绍符号表示，如层数、节点数、激活函数等，以及如何实现一个深度神经网络。同时，在选择神经网络的层数时需要考虑一些问题，如交叉验证进和超参数等。 深度神经网络的概念和符号表示，以及实现自己的深度神经网络的方法 前向传播和后向传播，实现深度神经网络 神经网络有不同的层数，每个层都有节点数量 使用符号 $l$ 表示网络中的层数，$n^{[l]}$表示节点数量 深度神经网络的符号表示，包括节点数量、激活、权重，以及符号的含义和使用 描述节点数量的符号：$n^{[l]}$ 符号：$a^{[l]}$表示$l$层的激活函数、$W^{[l]}$表示参数矩阵、$b^{[l]}$表示偏置项 36.1 What is a deep neural network? 36.2 Deep neural network notation"},{"path":"/2023/12/22/35-sui-ji-chu-shi-hua/","content":"35 随机初始化在训练神经网络时，初始化权重的重要性，对于随机初始化的权重，将其全部初始化为零时是不合适的，因为这将导致隐藏单位集散相同的函数，没有区别。相反，初始化为权重为随机值可以解决对称性破缺问题，是隐藏单元计算不同的函数。此外，初始化偏置项和权重的其它注意事项，可以帮助训练更加有效的神经网络。 在训练神经网络时，初始化权重的重要性，特别时对于随机初始化的神经网络 初始化权重对神经网络训练非常重要 初始化为零会导致隐藏单元计算相同的函数 初始化参数为随机值可以解决这个问题 如何初始化神经网络的参数，以及为什么通常将权重初始化为较小的随机值 初始化权重为小随机值可以避免对称性破缺问题 初始化权重过大可能导致激活函数饱和（计算出的值，落在激活函数的平缓部分，则斜率非常小），减缓学习速度 初始化常数为0.01（浅层网络一般适用）或其他相对较小的数字（其他深层的网络，则需要仔细选择） 35.1 What happens if you initialize weights to zero 35.2 Random initialization"},{"path":"/2023/12/21/33.shen-jing-wang-luo-de-ti-du-xia-jiang-fa/","content":"33.神经网络的梯度下降法33.1 Gradient descent for neural networks 33.2 Formulas for computing derivatives 33.3 Computing gradients 这里迷糊了好久，终于理清楚了：P20 2.14证明了$dw=xdz^T$，这里写成了dw=dzx，实际上效果一样，对于矩阵相乘来说，左右互换结果就不一样了，dz写在前面，等同做了一次转置 〖(A∗B)〗^T=B^T∗A^T 33.4 Summary of gradient descent"},{"path":"/2023/12/20/30-ji-huo-han-shu/","content":"30 激活函数30.1 Activation functionssigmoid函数总是表现要比 tanh双曲正切函数要好一点，这是因为它的值域在 [-1, 1] 之间，激活函数的平均值是 0， 你可能需要平移所有数据，让数据平均值为 0， 使用 tanh 而不是 sigmoid 也有类似于庶几乎中心化的效果 它们均有的缺点是，当向两边无限延申的时候，他俩的斜率都非常地小，这会拖慢梯度下降法。 经验法则：当你二分类地时候，最好使用 sigmoid 函数，其他默认选择 ReLU 或者 leakyReLU 30.2 Pros and cons of activation functions 30.3 Why use activation function? 30.4 Sigmoid activation function"},{"path":"/2023/12/19/25-shen-jing-wang-luo/","content":"25 神经网络25.1 What is a Neural Network ? 25.2 Neural Network Representation 向量化的一条经验法则：当我们在一层中有不同的节点，那就将它们竖向堆叠 25.3 Neural Network Representation learning 25.4 Vectorizing across multiple examples 25.5 Justification for vectorized implementation 25.6 Recap of vectorizing across multiple examples"},{"path":"/2023/12/19/21-python-de-guang-bo-ji-zhi/","content":"21 Python 的广播机制21.1 Broadcasting example【举例】 【举例】 21.2 General Principle 21.3 Python-Numpy vectors直接使用 np.random.randn(5)，得到是秩为1的数组，最终将会运算得到一个值 而是通常将其表达为 5*1的矩阵比较好，最终运算会得到一个矩阵"},{"path":"/2023/12/18/17-xiang-liang-hua/","content":"17 向量化17.1 What is vectorization? 【举例】 17.2 Neural network programming guideline 17.3 Logistic regression derivatives 17.4 Vectorizing Logistic Regression 17.5 Vectorizing Logistic Regression 2 17.6 Implementing Logistic Regression"},{"path":"/2023/12/18/15-logistic-hui-gui-zhong-de-ti-du-xia-jiang-fa/","content":"15 logistic 回归中的梯度下降法15.1 Logistic regression recap 15.2 Logistic regression on $m$ examples"},{"path":"/2023/12/18/13-ji-suan-tu/","content":"13 计算图13.1 Computation Graph 13.2 Computing derivatives 可控"},{"path":"/2023/12/18/11-dao-shu/","content":"11 导数11.1 Intuition about derivatives 11.2 examples"},{"path":"/2023/12/18/10-ti-du-xia-jiang-fa/","content":"10 梯度下降法10.1 Gradient Descent"},{"path":"/2023/12/18/08-logistic-hui-gui/","content":"08 logistic 回归8.1 Logistic Regression 8.2 Logistic Regression cost function"},{"path":"/2023/12/17/07-er-fen-fen-lei/","content":"07 二分分类7.1 Binary Classification 7.2 Notation"},{"path":"/2023/12/17/09-softmax-hui-gui-sun-shi-han-shu-tu-pian-fen-lei-shu-ju-ji/","content":"09 Softmax 回归 + 损失函数 + 图片分类数据集 9.1Softmax 回归 回归 vs 分类 回归估计一个连续值 分类预测一个离散类别 kaggle 上的分类问题 将人类蛋白质显微镜图片分成 28 类 9.2损失函数9.3图片分类数据集9.4Softmax 回归从零开始实现9.5Softmax 回归简洁实现"},{"path":"/2023/12/12/08-xian-xing-hui-gui/","content":"08线性回归 8.1线性回归 如何在美国买房 看中一个房子，参观了解 估计一个价格，出价 房屋基本信息： 房屋实景： 房价预测 历史的房价走势： 当你以一定的价格购入本套房屋之后，过了一段时间，再次给出房价走势，就可以看出你的盈亏情况。 一个简化模型 假设1：影响房价的关键因素是卧室个数，卫生间个数和居住面积，记为：$x_1, x_2, x_3$ 假设2：成交价是关键因素的加权和 y = w_1x_1 + w_2x_2 + w_3x_3 + b权重和偏差的实际值在后面决定 线性模型 给定 n 维输入 $\\boldsymbol {x} = [x_1,x_2,…, x_n]^T$ 线性模型有一个 n 维权重和一个标量偏差 \\boldsymbol {w} = [w_1, w_2, ..., w_n]^T, b 输出是输入的加权和 y = w_1x_1 + w_2x_2 + ... + w_nx_n + b向量版本：$y = &lt;\\boldsymbol {w}, \\boldsymbol {x}&gt; + b$ 线性模型可以看作是单层的神经网络 神经网络源于神经科学 衡量预估质量 比较真实值和预估值，例如房屋售价和估价 假设 $y$ 是真实值，$\\hat {y}$ 是估计值，我们可以比较 l(y, \\hat{y}) = 1/2(y - \\hat{y})^2这个叫做平方损失 训练数据 收集一些数据点来决定参数值（权重和偏差），例如过去6个卖的房子 这被称之为训练数据 通常越多越好 假设我们有 n 个样本，记 \\boldsymbol {X} = \\left[ \\boldsymbol {x_1}, \\boldsymbol {x_2}, ..., \\boldsymbol {x_n} \\right]^T \\\\ \\boldsymbol {y} = \\left[ {y_1}, {y_2}, ..., {y_n} \\right]^T 参数学习 训练损失 l(\\boldsymbol {X}, \\boldsymbol {y}, \\boldsymbol {w}, b) = 1/2n \\sum_{i=1}^n (y_i - - b)^2 \\\\ = 1/2n ||\\boldsymbol {y} - \\boldsymbol {X}\\boldsymbol {w} - b||^2 最小化损失来学习参数 \\boldsymbol {w}^*, \\boldsymbol {b}^* = arg \\space min_{\\boldsymbol {w}, b} \\space l(\\boldsymbol {X}, \\boldsymbol {y}, \\boldsymbol {w}, b) 显式解 将偏差加入权重 $\\boldsymbol {X} \\longleftarrow [\\boldsymbol {X}, \\boldsymbol {1}] \\space \\boldsymbol {w} \\longleftarrow [\\boldsymbol {w}, b]^T$ l(\\boldsymbol {X}, \\boldsymbol {y}, \\boldsymbol {w} = 1/2n ||\\boldsymbol {y} - \\boldsymbol {X}\\boldsymbol {w}||^2 \\space ) \\space \\frac{\\partial}{\\partial \\boldsymbol {w}}l(\\boldsymbol {X}, \\boldsymbol {y}, \\boldsymbol {w}) \\\\ =1/n (\\boldsymbol {y} - \\boldsymbol {X}\\boldsymbol {w})^T\\boldsymbol {X} 损失是凸函数（意味着最优解就在梯度等于0的地方），所以满足最优解 \\frac{\\partial}{\\partial w} l(\\boldsymbol {X}, \\boldsymbol {y}, \\boldsymbol {w}) = 0 \\\\ 1/n(\\boldsymbol {y} - \\boldsymbol {X}\\boldsymbol {w})^T\\boldsymbol {X} = 0 \\\\ \\boldsymbol {W}^* = (\\boldsymbol {X}^T\\boldsymbol {X})^{-1}\\boldsymbol {X}\\boldsymbol {y} 总结 线性回归是对 n 维输入的加权，外加偏差 使用平方损失来衡量预测值于真实值的差异 线性回归有显示解 线性回归可以看作是单层神经网络 8.2基础优化算法 梯度下降（当一个模型没有显式解） 挑选一个初始值 $\\boldsymbol {w}_0$ 重复迭代参数 t = 1, 2, 3 \\boldsymbol {w}_t = \\boldsymbol {w}_{t-1} - \\eta \\frac{\\partial \\phi }{\\partial \\boldsymbol {w}_{t-1}} 沿着梯度方向将增加损失函数值 学习率：步长的超参数 分析这个例子，二次函数的等高线如上，首先挑选一个初始值 $\\boldsymbol {w}_0$ ，然后计算它此处的梯度，也就是当前点处函数增长最快的方向，那么负梯度，就是函数下降最快的方向，我们要求出那个最优解，也就是谷底的位置，就要沿着某条路线不断接近这个最优点，而每一处的梯度为我们指明了方向。 如何选择学习率 小批量随机梯度下降 在整个训练集上计算梯度太贵了 一个深度神经网络模型可能需要数分钟至数小时 我们可以随机采样 b 个样本 $i_1, i_2, …, i_b$ 来近似损失 \\frac{1}{b} \\sum_{i \\in I_b} l(\\boldsymbol {X}_i, y_i, \\boldsymbol {w}) b是批量大小，另一个重要的超参数 选择批量大小 不能太小：每次计算量太小，不适合并行最大利用计算资源 不能太大：内存消耗增加，浪费计算，例如：如果所有样本嗾使相同的 总结 梯度下降通过不断沿着繁体都方向更新参数求解 小批量随机梯度下降是深度学习的求解算法 两个重要的超参数是批量大小和学习率 8.3线性回归的从零开始实现 我们将从零开始实现整个方法，包括数据流水线、模型、损失函数和小批量随机梯度下降优化器 根据带有噪声的线性模型构造一个人造数据集，我们使用线性模型参数$\\boldsymbol {w} = [2, -3.4]^T$、$b = 4.2$ 和噪声$\\xi$生成数据集及其标签： \\boldsymbol {y} = \\boldsymbol {X}\\boldsymbol {w} + b + \\xi 参考 torch.normal()的用法 PyTorch疑难杂症（1）——torch.matmul()函数用法总结 Python numpy函数：reshape（） X 的产生：从均值为 0 ，方差为 1 的分布，也就是正态分布中进行采样，行数是 num_examples ， 列数是 true_w 的列数，也就是特征数量。 y 是 X 和 w 进行矩阵相乘后，加上偏置 b 后实现的，也就是先将 [1000, 2] 的矩阵和 [1, 2] 的矩阵进行矩阵的相乘（使用了广播机制）。 y 添加噪音的方式是噪音元素和 y 元素的元素级相加。 features中的每一行都包含一个二维数据样本，labels中的每一行都包含一位标签值（一个标量） 通过生成第二个特征features[:, 1]和labels的散点图， 可以直观观察到两者之间的线性关系。 在下面的代码中，我们[定义一个data_iter函数， 该函数接收批量大小、特征矩阵和标签向量作为输入，生成大小为batch_size的小批量]。 每个小批量包含一组特征和标签。 Python shuffle() 函数 python中yield的用法详解——最简单，最清晰的解释 定义初始化模型参数 定义模型 定义损失函数 因为需要计算损失函数的梯度，所以我们应该先定义损失函数。 这里我们使用 :numref:sec_linear_regression中描述的平方损失函数。 在实现中，我们需要将真实值y的形状转换为和预测值y_hat的形状相同。 定义优化算法 训练过程 比较真实参数和通过训练学到的参数来评估训练的成功程度 8.4线性回归的简洁实现 通过使用深度学习框架来简洁地实现 线性回归模型 生成数据集 调用框架中现有的 API 来读取数据 我们可以这样使用数据集，迭代方式是： 使用框架的预定义好的层 可以直接使用 PyTorch中的 nn模块，里面有神经网络的容器函数 nn.Sequential()，里面填入一个nn.Linear(2, 1) 等价于我们的线性函数，输入维度为 2 输出维度是 1。 计算均方误差使用的是 MSELoss类，也称为平方范数 实例化SGD实例 训练过程代码于我们从零开始的实现时所做的非常相似 比较误差 8.5QA"},{"path":"/2023/12/11/07-zi-dong-qiu-dao/","content":"07自动求导7.1自动求导 向量链式法则 标量链式法则 拓展到向量 【举例】 假设 计算 先分解 得到： 【举例】 假设 计算： 分解： 得到： 自动求导 自动求导计算一个函数在指定值上的导数 它有别于 符号求导 数值求导 计算图 将代码分解成操作子 将计算表示成一个无环图 显示构造（Tensorflow/Theano/MXNet） 隐式构造（PyTorch/MXNet） 相当于告诉机器我是如何一步一步地计算地： 自动求导地两种模式 链式法则 正向积累 反向积累、又称反向传递 （backforward） 反向积累 先看正向，正向就是自计算图的底端，从下往上开始计算 当正向算完以后，就可以反向积累了。 首先，先求 $z$ 关于 $b$ 的导数，算完的结果是 $2b$ ，显然这里的 $b$ 反向积累时，不知道它的值究竟是多少，怎么办呢？我们可以使用之前正向计算过程中留下的结果，也就是读取之前运行的结果 同理，我们可以计算 $z$ 关于 $a$ 的导数，这里的 $b$ 就可以使用上面的 $2b$ 中的 $b$ 了。 最后，我们计算 $z$ 关于 $w$ 的导数，其中的 $a$ 可以取自正向计算时得到的结果 反向累积小结 前向：执行图，存储中间结果 反向：从相反方向执行图 去除不需要的枝 复杂度 计算复杂度：O(n), n是操作子个数 通常正向和反向的代价类似 内存复杂度：O(n)，因为需要存储正向的所有中间结果 跟正向累积对比： O(n)计算复杂度用来计算一个变量的梯度 O(1)内存复杂度 7.2自动求导实现假设我们想对函数 $y = 2 \\boldsymbol {x}^T\\boldsymbol {x}$ 关于列向量 $\\boldsymbol {x}$ 求导 在我们计算 $y$ 关于 $\\boldsymbol {x}$ 的梯度之前，我们需要一个地方来存储梯度。 现在我们计算 $y$ 。 通过调用反向传播函数来自动计算 $y$ 关于 $\\boldsymbol {x}$ 每个分量的梯度。 现在让我们计算 $\\boldsymbol {x}$ 的另一个函数。 深度学习中，我们的目的不是计算微分矩阵，而是批量中每个样本单独计算的偏导数之和。 将某些计算移动到记录的计算图之外，用于固定网络中的某些参数 即使构建函数的计算图需要通过 Python 控制流（例如，条件、循环或任意函数调用）我们任然可以计算得到变量的梯度。 工作原理是：隐式计算流：首先看我们的函数，你会发现，整个函数的返回值总是取决于 b 的值或者是你输入的 a 的值。每一次计算的时候，PyTorch会在背后将计算图存储下来，然后倒着做一遍就可以正确答案了。同时计算更慢。 7.3QA"},{"path":"/2023/12/10/06-ju-zhen-ji-suan/","content":"06矩阵计算6.1矩阵计算 标量导数 导数是切线的斜率 亚导数 将导数拓展到不可微的函数 另一个例子 梯度 将导数拓展到向量 $\\frac{\\partial y}{\\partial x}$ 列向量的导数是行向量 【举例】 \\frac{\\partial x_1^2}{\\partial x} + 2x_2^2=[2x_1 + 4x_2]方向 $(2, 4)$ 与等高线正交 如何理解：前面的函数可以画作是等高线，可以看作是一座山，现取任意一点 $(x_1, x_2) = (1, 1)$ ，经过这个点做等高线的切线（可以看作是上面函数的导数），而这个切线的垂线方向恰好可以使用我们所谓的梯度函数 $[2x_1, 4x_2]$ 计算出来。 梯度和等高线是正交的，指向的是等高线变化最大的方向。也就是机器学习求解的核心思想。 【举例】 $\\frac{\\partial y}{\\partial x}$ $\\frac{\\partial y}{\\partial x}$是行向量，$\\frac{\\partial y}{\\partial x}$是列向量，这个被称为分子局部符号，反过来的版本就叫分母布局符号 $\\frac{\\partial y}{\\partial x}$ 【举例】 拓展到矩阵 6.2QA"},{"path":"/2023/12/08/05-xian-xing-dai-shu/","content":"05线性代数 5.1线性代数标量 简单操作 c = a + b \\\\ c = a * b \\\\ c = sina 长度 \\begin{equation} \\left| a \\right| = \\begin{cases} a & \\text{ $ if a > 0 $ } \\\\ -a & \\text{ $ otherwise $ } \\end{cases} \\end{equation} \\left| a + b\\right| = 0 for all a \\\\ ||a + b|| = 0 正交矩阵 所有行都相互正交 所有行都有单位长度 $u$ 可以写成 置换矩阵 置换矩阵是正交矩阵 特征向量和特征值 不被矩阵改变方向的向量$x$（但长度可能会改变） Ax = \\lambda x 对称矩阵总是可以找到特征向量 5.2线性代数实现标量由只有一个元素的张量表示 你可以将向量视为标量值组成的列表 通过张量的索引来访问任一元素 访问张量的长度 只有一个轴的张量，形状只有一个元素 通过指定两个分量 $m$ 和 $n$ 来创建一个形状 $m * n$ 的矩阵 矩阵的转置 对称矩阵（symmetric matrix）$A$ 等于其转置：$A = A^T$ 就像向量是标量的推广，矩阵是向量的推广一样，我们可以构建具有更多轴的数据结构 代表2个具有3 * 4结构的矩阵的组合 给定具有相同形状的任何两个张量，任何按元素二元运算的结果都将是相同形状的张量 两个矩阵的按元素的成绩称为 哈达玛积（Hadamard product）（数学符号）⊙ 计算其元素的和 表示任意形状张量的元素和 指定求和汇总张量的轴 axis=0 意味着对 2 这一维度进行求和，剩下的两个维度留下来了。 axis=1 意味着对 5 这一维度进行求和，剩下的两个维度留下来了。 也可以指定其中的多个维度进行求和，其余的维度留下来 下面解释一下是怎么按特定轴求和的 12345678910111213原始的tensor.size([2, 5, 4]), 一共是40个数tensor([[[ 0, 1, 3, 4], [ 5, 6, 7, 8], [ 9, 10, 11, 12], [13, 14, 15, 16], [17, 18, 19, 20]], [[21, 22, 23, 24], [24, 25, 26, 27], [28, 29, 30, 31], [32, 33, 34, 35], [36, 37, 38, 39]]])按 axis=0 求和，也就是 一个与求和相关的量是 平均值（mean 或 average） 计算总和或均值时保持轴数不变 通过广播将 $A$ 除以 sum_A 某个轴计算 $A$ 元素的累积总和 点积是相同位置的按元素乘积的和4 我们可以通过执行按元素乘法，然后进行求和来表示两个向量的点积 矩阵向量积 $Ax$ 是一个长度为 $m$ 的列向量，其 $i^{th}$ 元素是点积 $a_i^Tx$ 我们可以将矩阵-矩阵乘法 $AB$ 看作是简单地执行 $m$ 次矩阵-向量积，并将结果拼接在一起，形成一个 $n * m$ 的矩阵 $L_2$ 范数是向量元素平方和的平方根 ||x||_2 = \\sqrt{\\sum_{i-1}^n x_i^2} $L_1$ 范数，它表示为向量元素的绝对值之和： ||x||_1 = \\sum_{i=1}^n |x_i| 矩阵的 弗罗贝尼乌斯范数 （Frobenius norm） 是矩阵元素的平方和的平方根： ||X||_F = \\sqrt{\\sum_{i=1}^m \\sum_{j=1}^n x_{ij}^2} 5.3按特定轴求和 5.4QA"},{"title":"00绪论基本概念","path":"/2023/12/07/00-xu-lun-ji-ben-gai-nian/","content":"绪论基本概念 1.1引言问题 通过计算的手段，利用经验来改善系统自身的性能 有了数据 通过某种学习算法 得到模型 进行预测 1.2基本术语1.有了数据 数据集：100个西瓜🍉 样本：1个西瓜 特征向量： 样本空间； 色泽、大小、敲起来的振幅； 维度 属性：色泽 2.通过某种学习算法 学习 训练 3.得到模型 有监督学习： 分类 二分类 Y 正负 瓜农眼中：这个瓜该不该摘，这个瓜熟没熟，我要不要摘 多分类 Y大于2 市场上要买哪种瓜 黑美人 小地雷 特小凤 回归 Y = R 实数集 某段时间内西瓜的价格，啥时间卖西瓜最合适 无监督学习： 聚类 我们不知道要分几类，机器自己分 每个组称为“簇” cluster 4.进行预测 测试 测试样本 泛化能力 1.3假设空间 科学的推理手段 归纳：特殊到一般 狭义：从训练数据中得到概念 布尔概念：是或不是 假设就是各种情况 广义：从样本中学习 演绎：一般到特殊 1.4归纳偏好 同一个数据集训练出了不同的模型，如何选择模型 原则：奥卡姆剃刀原理：选择最简单的那个，也有其他理解 推了半天期望，由于假设不成立，我们跳过，P8出现了很多符号"},{"title":"词向量与语言模型","path":"/2023/12/06/ci-xiang-liang-yu-yu-yan-mo-xing/","content":"词向量与语言模型1.语言模型基础与词向量语言模型可以简单理解为一个句子s在所有句子中出现的概率分布P(s)。比如：一个语料库中有100个句子，『OK』这个句子出现了5次，那么$p(OK) = 5 \\%$。 那么，如何学习到这种概率分布呢？最简单的方法是建立一个无比庞大的与来哦库，该语料库中包含了人类成千上万年间可能交过的所有的花，那么我们不久可以计算出这句话的概率了吗？可惜这种方法不够现实。 自然的，能不能通过数学的方法进行表示呢？答案是可以的，因为S是一个序列$w_1, w_2, …, w_n$,那么$p(s)$可以展开为： p(s) = P(w_1, w_2, \\cdots, w_n) = P(w_1) \\cdot P(w_2|w_1) \\cdot P(w_3|w_1,w_2) \\cdots P(w_n|w_1, w_2, \\cdots, w_{n-1})那么现在的问题就变成了我们如何计算$P(w_1, w_2, …, w_n)$ 1.1统计方法 - n元模型回忆概率论： P(A|B) = \\frac{P(AB)}{P(B)}我们观察上式， 会发现，$P(w_1)$比较好算， $p(w_2|w_1) = \\frac{P(w_1w_2)}{P(w_1)}$也还行，但是$P(w_3|w_1,w_2)$就比较有难度了，随着n的增大，计算会越来越难，$P(w_n|w_1, w_2,…, w_n)$几乎根本不可能计算出来，怎么办？ 马尔可夫假设：假设任意一个词$wi$出现的概率只同它前面的n个词$w{i-1}, …, w_{i - n}$有关，因此，那么就有： 一元模型：P(S) = \\prod_{i=1}^{l} P(w_i) \\\\ 二元模型：P(s) = \\prod_{i=1}^{l} P(w_i|w_{i-1})缺点： 无法建模更远的关系，语料不足使得无法训练更高阶的语言模型。 无法建模出词之间的相似度。 训练语料里面有些n元组没有出现过，其对应的条件概率就是0， 导致计算一整句话的概率为0，解决这个问题有来个那种常用的方法：平滑法和回退法。 1.2深度学习方法 - 神经网络语言模型[1]首先，我们回到问题本身，我们为什么要计算$p(s)$，我们的目的是为了通过大规模语料库学习到语言内部的概率分布，那么有没有办法通过深度学习额方式来学习到这种概率分布呢？ 观察上图，假设有一组词序列，$w_1, w_2, …, w_t$ ，其中，$w_i \\in V$，$V$是所有单词的集合。我们输入的是一个词序列， 而我们的输出是一个概率值，表示根据context预测出下一个词是$i$的概率。用数学来表示，我们最终是要训练一个模型： P(s) = P(w_t|w_1^{t-1}) $wt$ 表示这个词序列中的第$t$个单词，$w{t-n+1}$表示输入长度为n的词序列中的第一个单词 $w_1^{t-1}$表示从第一个单词到$t-1$个的那次组成的子序列 因此，我们发现，该模型的每个样本其实计算的是：$p(wn|w_1, …, w{n-1})$ 1.3词向量 - 表示语言的方式前面，我们通过NNLM可以知道，通过语言模型的训练，模型可以学习到语言的概率分布，那么如何将学习到的信息应用到下游任务呢？这就是词向量产生的背景，这里简单介绍下Word2Vec[4]。 首先明确一点，词向量是语言模型的副产物。怎么理解呢？意思是说，词向量是语言模型训练完成之后产生的。 这里我们以Word2Vec的CBOW训练模型为例： h = \\frac{1}{C} W^T(w_1 + w_2 + \\cdots + x_c) \\\\ u = w'^{T} * h \\\\ P(w_j| context) = y_i = \\frac{exp({u_j})}{\\sum_{k \\in V} exp({u_k})}通过这样的训练完成后，把$W_{V \\times N}$ 保存下来，我们就得到了词向量。 2.预训练语言模型 - 用模型表示语言前面提到，我们的最终目的还是通过语言模型来获得某种语言的表示，但是我们看到，上面那种训练方式，似乎不太合适，那个最后$W_{V \\times N}$ 消失不见了，并且模型没有做深。 我们此处总结一些Word2Vec的弱点： 模型无法做深，词向量的表征能力有限，词向量的抽象程度不高。 词向量获得的是上下文无关的，难以解决歧义问题 OV词无法解决 2.1NLP特点在进入预训练语言模型之前，我们先来看看对于NLP来说，最重要的是什么。 首先是NLP的特点： 输入是一个维线性序列 输入是不定长的，这点对于模型处理起来会比较麻烦 单词位置与句子位置的相对位置非常重要，互换可能导致完全不同的意思 句子中的长距离特征对于理解语义也是非常关键的。 其次是，NLP中几大常见的任务： 序列标注：分词、词性标注、命名实体识别等。特点是句子中每个单词要求模型根据上下文都要给出一个分类类别。 分类任务：文本分类、情感分析。特点是不管文章多长，总体给出一个分类类别即可。 句子关系推断：QA、自然语言推理。特点是给定两个句子，模型判断出两个句子是否具备某种语义关系。 生成式任务：机器翻译、文本摘要。特点是输入文本内容后，需要自主生成另一个文本。 最后，我们来聊一聊三大基本单元：CNN、LSTM、Transformer。 首先，先简单回顾下Transformer的self-attention机制，该机制在预训练语言模型中起到了至关重要的作用。我们看到，对于Transformer来说，通过self-attention机制，词与词之间的关系一目了然，并且不会受到文本长度的限制。然后注意，在Attention is all you need这篇文章中，Transformer是Encoder-decoder架构的，这与后面的BERT所用的有所不同，后面的BERT所用的只是transformer_block。 于是我们总结一下这三个基本单元的优缺点： RNN 优点：天生的具有时序结构，十分适合解决NLP问题 缺点： 反向传播时所存在的优化困难的问题，及梯度消失，梯度爆炸问题，进而导致超长距离依赖的解决不佳。 并行能力，进而导致难以做深 CNN 优点： 可以并行，可以做的非常深 能够很好的捕捉 n-gram 特征 缺点： 无法解决长距离依赖问题 对于位置信息不敏感 Transformer 优点： self-attention天生的就解决了长距离依赖问题 可以并行，可以做的非常深 位置信息通过 position embedding 很好的补充了 缺点： 对于超长文本，会导致非常大的计算复杂度 位置信息依赖于 position embedding 2.2ELMO ELMO是通过L层的双向LSTM语言模型来学习上下文信息的，这就解决了上文提到的前两个问题，而针对OV词，ELMO采用了 char-level 来生成词向量进而进行训练。而对于ELMO的不同层而言，不同层的LSTM能够把把握不同粒度和层级的信息，比如浅层的LSTM把握的是单词特征，中层的LSTM把握句法特征，深层的LSTM把握语义特征。 前向语言模型：p(t_1,t_2,\\cdots,t_N) = \\prod_{k=1}^N p(t_k|t_1,t_2, \\cdots, t_{k-1}) \\\\ 后向语言模型：p(t_1,t_2,\\cdots,t_N) = \\prod_{k=1}^{N} p(t_k|t_{k+1},t_{k+2},\\cdots.t_{N})但是，ELMO的缺点也十分明显： LSTM特征提取能力远弱于Transformer,并行性较差 拼接方式双向融合特征能力偏弱 层数浅，只有两层 2.2BERT语言模型： P(s) = P(x_{mask}|context)我们先来看模型架构，BERT-base 采用12层的 Transformer，这里简单说明，BERT 的架构相当于 Transformer 的 Encoder-decoder 架构中的 Encoder。 然后，我们关注，输入的组成部分，输入包含三个部分： token embedding：词向量，第一个单词是CLS标志，可以用于之后的分类任务 Segment Embeddings：区别两种句子，因为预训练不光做LM还要做两个句子为输入的分类任务 Position Embeddings：和之前文章中的 Transformer不一样，不是三角函数而是学习出来的 最后，我们看下预训练训练任务部分。 首先是 Masked LM：随机遮蔽输入的 token 的15%，然后预测被遮住的 token。这样会带来一个问题，即训练和微调阶段的不一致性，因为训练阶段采用了 [MASK] 而 fine-tune 阶段并没有。为了减轻该问题， we do not always replace “masked” words with the actual [MASK] token. 具体做法为： 假如我们有一句话，my dog is hairy，被选中的词为hairy，数据生成器并不总是将hairy替换为 [MASK]，此时过程为： 80%情况：用 [MASK] 替换 hairy 10%情况：随机选一个词，如 apple 来替换 hairy 10%情况：不改变这句话 然后是NSP，即Next Sentence Prediction，选定一个句子A，B作为预训练样本，B有50%的可能是A的下一句，也有50%的可能是语料库的随机句子。 2.3GPT1.0其实GPT1.0要比BERT出来的早，但是吃了不会宣传的亏。首先来看语言模型： P(s) = P(w_i|w_{i-k}, \\cdots, w_{i-1}) \\\\ L_1(U) = \\sum_i log P(u_i| u_{i-k}, \\cdots, u_{i-1}; \\Theta); \\, \\, \\text{k为窗口大小}跟 bert 有很明显的差别，但是符合原来语言模型的定义。 其次，模型结构采用单向 Transformer, 这是由于语言模型决定的。 再次， embedding 不包含 NSP 这种 segment embedding。 说到这里，我们就说完了基础的三个预训练语言模型，接下来我们探讨下如何更好的使用预训练语言模型。 3.如何使用预训练语言模型3.1是否要进行微调[1]我们是直接采用训练好的向量还是用预训练语言模型进行微调呢？ 『冰』表示freeze， 『火』表示微调的结果。 实际上，对于大多数任务， BERT 进行微调的方式总是比提取向量再训练的方式能够获得更佳的效果。因此，再条件允许的情况下，推荐采用微调的方式。 3.2是否要进行再次预训练[2]答案是需要。 我们知道现在的预训练语料采用的都是百科，书籍等比较规范的数据，而实际业务中的数据千差万别，可以这么理解，预训练本身获得的是语料库中文本的分布，而如果预训练数据分布于业务数据分布偏差较大，会带来一些负面的影响。 因此，针对一些业务，如果数据与百科数据擦汗别非常大，先进性预训练，然后再进行微调是一种比较合适的方式。 我们这里简单介绍下[2]中的结论： 在目标领域的数据集上继续预训练 （DAPT）可以提升效果；目标领域与语言模型的原始预训练预训练语料越不相关，DAPT效果则提升越明显。 在具体任务的数据集上继续预训练 （TAPT）可以十分廉价的提升效果。 结合二者（先进行DAPT，再进行TAPT）可以进一步提升效果。 如果能够获取更多的、任务相关的五标注数据继续预训练（Curated-TAPT），效果则最佳。 3.3BERT向量 vs Glove向量接下来我们分析，BERT 相对于 Glove 向量，到底强在哪。首先是训练数据集规模的影响： 随着数据规模的扩大，Glove 向量的表现与 BERT 向量的表现差距越来越小，我们看到当训练数据足够多的时候，Glove 在一些任务撒谎给你的获得略差于 BERT的影响，但是在绝大多数情况下依旧比 BERT 向量差很多，这说明 BERT对于小数据集的优越性。 在简单任务上，随着数据量的增加，Glove 能达到 BERT 十分接近的效果。 然后是语言特征： the complexity of text structure: 句子结构的复杂性 Ambiguity in word usage: 单词的歧义性 Prevalence of unseen words: 未登录词出现的概率 上图我们可以得出以 BERT 为代表的 Contextual embeddings 在解决一些文本结构复杂度高和单词歧义性方面有显著的效果。 4.预训练语言模型 - 后时代首先，我们来分析一下这张图，从上到下： Contextual：谈论 静态 embedding 与 上下文 embedding，被做烂了，pass。 Architectures： 模型整体架构，这部分还有的探讨，可以参见上面 T5 的Model Architecture 部分。 目前业界还没有统一的标准说应该选择哪种架构，不过从 T5 的效果来看， Transformer+Encoder+Decoder 的效果是最好的，但参数量也上去了。其实就目前来看，研究的意义不是很大了，除非说能出现一个大的突破。 Task Types： 谈论了两件事： 语言模型的选择以及Contrastive Learning，其实这两个应该分开讨论。 Multi-Lingual： 从多国语言的角度出发，这方面不太懂，也不感兴趣，觉得用处不会太大。 Multi-Modal： 多模角度，我个人认为这对于工业界是十分有意义的。 Knowledge Enriched： 知识 + 预训练语言模型，我觉得这是一个很值得研究的方向，无论是在工业界和学术界。 Domain Specific： 特定领域 + 预训练语言模型，我觉得这方面很有搞头，毕竟很多专有领域跟公共领域还是很不同的，比如医学，生物，法学等。由于每看过相关文章，无法说上面的模型与 bert在同样语料上预训练后哪个效果好，但还是有一定参考价值的。 Language-Specific： 这块我觉得还是很有研究价值的，毕竟我们中文跟英文从各个方面来说差距还是蛮大的，如果能对语言有深入了解，感觉还有搞头。 Model Compression： 模型压缩，这个在工业界用处很大，十分建议研究，需求也很大，一些蒸馏方法所需要的资源门槛也比较低，如果有资源，有idea，建议入坑。 考虑到涉及到的内容太多，我这里抽取四个部分讨论，分别是： Architectures， Task Types， Knowledge Enriched 以及 language generation。 4.1AE vs ARAR 语言模型：自回归语言模型，指的是，依据前面（或后面）出现的 tokens 来预测当前时刻的 token， 代表有 ELMO， GPT 等。 forward: p(x) = \\prod_{t=1}^T p(x_t | x_{t})AE 语言模型：通过上下文信息来预测被 mask 的 token， 代表有 BERT , Word2Vec(CBOW) 。 p(x) = \\prod_{x\\in Mask} p(x|context)AR 语言模型： 缺点：它只能利用单向语义而不能同时利用上下文信息。 ELMO 通过双向都做AR 模型，然后进行拼接，但从结果来看，效果并不是太好。 优点： 对生成模型友好，天然符合生成式任务的生成过程。这也是为什么 GPT 能够编故事的原因。 AE 语言模型： 缺点： 由于训练中采用了 [MASK] 标记，导致预训练与微调阶段不一致的问题。 此外对于生成式问题， AE 模型也显得捉襟见肘，这也是目前 BERT 为数不多实现大的突破的领域。 优点： 能够很好的编码上下文语义信息， 在自然语言理解相关的下游任务上表现突出。 5.GPT系列5.1GPT 2.0GPT 2.0 验证了数据的重要性，即使单纯的从数据角度入手，效果就可以获得巨大的提升。GPT 2.0 采用800w 互联网网页数据，这样训练出来的语言模型，能够覆盖几乎所有领域的内容。 第二个意义在于，GPT 2.0 开始探索了预训练语言模型在 zero-shot 下的表现。这方面在GPT 3.0 中体现的淋漓尽致。 预训练数据与网络深度的重要性，目前也没有到极限。 GPT 2.0 的生成效果非常惊艳，至少语法，流畅度等方面是没有问题的，就是没有灵魂 zero-flot 也不是不可以 5.2GPT 3.0先来介绍一下几个概念： FT，fine-tuning：就是微调啦 FS，few-shot：允许输入数条范例和一则任务说明 One-shot：只允许输入一条范例和一则任务说明 Zero-shot：不允许输入任何范例，只允许输入一则任务说明 GPT 3.0 本质上是探索超大型预训练语言模型在 few-shot，one-shot，zero-shot 上的可能性，这是延续之前 GPT 2.0 的研究，整体上，GPT 3.0 在 zero-shot 下能获得相当不错的结果。 6.BERT系列6.1Robertaroberta 是bert 的一个完善版，相对于模型架构之类的都没有改变，改变的只是三个方面： 预训练数据： BERT 采用了 BOOKCORPUS 和英文维基百科， 总共16GB。而 RoBERTa 采用了 BOOKCORPUS + 英文维基百科+ CC-NEWS+OPENWEBTEXT+STORIES， 总共160GB。 Roberta 与 bert 都采用 512 个token 作为序列长度，但与 bert 不同的是， robert 不会随机掺杂一些短句，这意味着 roberta 采用的都是长句。 动态mask vs 静态 mask： 静态 mask：Bert 在准备训练数据时，每个样本只会进行一次随机 mask，每个 epoch 都重复使用，后续的每个训练步都采用相同的mask。 修改版静态mask： 在预处理时将数据集拷贝10次，每份数据采用不同的 mask。 动态mask：不在预处理时进行 mask，而是在每次向模型输入时动态生成mask 数据格式与NSP： Segment-pair + NSP：与 bert 一样。输入包含两个 segment，这两个 segment 可能会来自同一个文档或不同文档，两个segment 的token 数均小于 512，预训练任务包含 MLM 与 NSP。 Sentence+pair + NSP：输入包含两个 sentence，两个句子可能来自同一文档或不同文档，两个句子 token 数均少于 512。预训练任务包含 MLM 与 NSP。 Full-sentences：输入只有一部分，来自同一个文档或不同文档的连续句子，token总数不超过512。输入可能跨越文档边界，如果跨文档，则在上一个文档末尾添加文档边界token。不包含NSP任务。 Doc-sentences：输入只有一部分，输入来自同一个文档的连续句子，token总数不超过512。预训练不包含 NSP 任务。 通过四个对比实验我们发现： Segment-pair 较好于 sentence-pair，可能是因为 segment 能够学习到长距离依赖关系。 Doc-sentences 几乎在所有任务中表现最佳，这意味着 NSP 任务没有什么用 Doc-sentences 略好于 Full-sentences。 6.2T57.预训练语言模型与自然语言生成这里我们先来回顾一下BERT和GPT， 前面提到， BERT 本质上相当于 Transformer 中的 Encoder， 而GPT 相当于 Transformer 中的 Decoder。既然我们已经验证了 Transformer 在文本生成领域的成功，尤其是机器翻译领域， 那么当我们想用于生成问题的时候，很自然的想到有没有办法把二者结合起来呢？ MASS 就是基于这样的思想。 7.1MASSMASS 的思想很简单， 对于输入序列 x， mask 该句从 u 到 v 位置上的token，记为 $x^{\\u:v}$， 而对应的， 从 u 到 v 位置上的 token 片段记为 $x^{u:v}$ 。 k = v - u + 1 表示 mask 的窗口大小 ， 表示一句话中多少个 token 被 mask 。 对于 MASS 的语言模型来说， 其输入为 mask 后的序列 $x^{\\u:v}$ ， 输出为被 mask 后的序列 $x^{u:v}$。 为何 MASS 适合生成 首先， 通过 Seq2Seq 框架来预测被 mask 的tokens 使得 Encoder 去学习没有被 mask 的 token 的信息， 而Decoder 去学习如何从 Encoder 中提取有效的信息。 然后， 与预测离散的 tokens相比，Decoder 通过预测连续的 tokens， 其能够建立很好的语言生成能力。 最后， 通过输入与输出的 mask 匹配， 使得 Decoder 能够从Encoder 中提取到有意义的信息，而不是利用之前的信息。 MASS 总结来说有以下几点创新： 引入了 Seq2Seq 来训练预训练模型。 mask 掉的是一段连续的tokens而不是离散的 mask， 有助于模型生成语言的能力。 Encoder 中是 mask 掉的序列，而 Decoder 中是对应被mask的 tokens。 7.2UNILMUNILM 同样想融合bert与gpt ，然而走了与 MASS 完全不同的路子，它想通过多任务学习的方式来解决。UNILM 这篇文章，厉害在，同时使用多个预训练语言模型训练这个思想，在预训练任务中包含了三种语言模型： Bidirectional LM ： BERT 的 mask LM Unidirectional LM：GPT 的 语言模型，包括 left-to-right 到 right-to-left Seq2Seq LM： 句子间LM。输入两个句子，第一个句子采用双向LM方式，第二个采用单向LM 方式。 7.3BARTBART 与 MASS 的基本思想一致，都是受到 Transformer 在机器翻译领域的成功，尝试将 Transformer架构跟预训练结合起来。 但是与 MASS 不同的是，他们输入的数据格式有很大的差别，Decoder 也有较大的差别。与MASS 相比， BART 完全延续 Transformer 原来的架构方式。 训练数据： Token Masking 和BERT一样，随机选择token用[MASK] 代替。 Token Deletion 随机删除token，模型必须确定哪些位置缺少输入。 Text Filling 屏蔽一个文段，文段长度服从泊松分布（λ=3）。每个文段被一个[MASK]标记替换。如果文段长度为0，意味插入一个[MASK]标记（灵感来自Span-BERT）。 Sentence Permutation 以句号作为分割符，将一篇文章分成多个句子，并随机打乱。 Document Rotation 随机均匀地选择一个token，以这个token为中心，旋转文档，选中的这个token作为新的开头，此任务训练模型以识别文档的开头。 8.预训练语言模型融入知识8.1ERNIEERINE 的网络架构，语言模型等与 BERT 完全相同，与BERT 不同的主要有两点： 数据的mask NSP 任务 与 DLM 首先我们来看 mask 方式，ERNIE 的 mask 包括三部分： BERT 的 basic-level mask 预训练 Phrase-level 预训练 Entity-level 预训练 但是我们反过来看这篇文章，它融入知识了吗？ 我觉得没有，对于知识图谱来说，实体本身的含义很重要，但是实体的关系同样非常重要，而这篇文章并没有融入任何的关系信息。 8.2ERNIE（清华）这篇文章最核心的点在于，将BERT的信息与TransE 的信息进行融合 我们看到，上述整个模型可以整体分为两部分： T-Encoder： 与 Bert 的预训练过程完全相同，是一个多层的双向 Transformer encoder， 用来捕捉词汇和语法信息。 K-Encoder： 本文创新点，描述如何将知识图谱融入到预训练模型。 8.3K-BERTReference语言模型基础与词向量： [1] A Neural Probabilistic Language Model [2] Mikolov, T.(2013). Distributed Representations of Words and Phrases and their Compositionality. [3] Mikolov, T.(2013). Efficient Estimation of Word Representations in Vector Space. [4] Rong, X. (2014). word2vec Parameter Learning Explained. 预训练语言模型： [1] ELMO: Deep contextualized word representations [2] BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [3] GPT 1.0: Improving Language Understanding by Generative Pre-Training [4] GPT 2.0: Language Models are Unsupervised Multitask Learners [5] GPT 3.0: Language Models are Few-Shot Learners 应用预训练语言模型： [1] To tune or not to tune? adapting pretrained representations to diverse tasks. [2] Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks 预训练语言模型 - 后时代： [2] ERNIE - Enhanced Language Representation with Informative Entities [3] ERNIE - Enhanced Representation through Knowledge Integration [4] ERNIE 2.0 - A Continual Pre-training Framework for Language Understanding [5] MASS - Masked Sequence to Sequence Pre-training for Language Generation [6] UNILM - Unified Language Model Pre-training for Natural Language Understanding and Generation [7] XLNet - Generalized Autoregressive Pretraining for Language Understanding [8] RoBERTa - A Robustly Optimized BERT Pretraining Approach [9] TransformerXL: Attentive Language Models Beyond a Fixed-Length Context 如何预训练一个好的预训练语言模型： [1] Pre-trained Models for Natural Language Processing: A Survey [2] T5: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 1Bag of Tricks for Efficient Text Classification","tags":["NLP"],"categories":["深度学习与自然语言处理"]},{"path":"/2023/12/04/ci-xiang-liang/","content":"词向量 https://zhuanlan.zhihu.com/p/29364112 1.Word2VecWord2Vec是Google发布的一个工具， 用于训练词向量，其提供了两种语言模型来供选择， 且Google 基于大规模语料集上训练出了预训练词向量来供开发者或研究者使用。 一般情况下，我们是没有必要自己去训练词向量的，但如果要求特殊，且语料集庞大，自己训练也是可以的。 在Word2Vec中，实现了两个模型：CBOW 与 Skip-Gram。 1.1CBOW模型CBOW，全称Continuous Bag-of-Word，中文叫做连续词袋模型： 如上图是一个两层的神经网络，其实在训练语言模型的过程中考虑到效率等问题，常常采用浅层的神经网络来训练，并取第一层的参数如上图就是 $W_{V \\times N}$ 来作为最终的词向量矩阵（参考 语言模型：从n元模型到NNLM）。 CBOW模型的目的是预测 $P(wt| w{t-k}, \\cdots, w{t-1}, w{t+1}, \\cdots, w_{t+k}) $，我们先来走一遍CBOW的前向传播过程 。 1.1.1前向传播过程 输入层: 输入C个单词： $x{1k}, \\cdots, x{Ck} $，并且每个 $x$ 都是用 One-hot 编码表示，每一个 $x$ 的维度为 V（词表长度）。 输入层到隐层: 共享矩阵为 $W_{V \\times N}$ ，V表示词表长度，W的每一行表示的就是一个N维的向量（训练结束后，W的每一行就表示一个词的词向量）。在隐藏层中，我们的所有输入的词转化为对应词向量，然后取平均值，这样我们就得到了隐层输出值 ( 注意，隐层中无激活函数，也就是说这里是线性组合)。 其中，隐层输出 $h$ 是一个N维的向量 。 h = \\frac{1}{C} W^T(x_1 + x_2 + \\cdots + x_c) 隐层到输出层：隐层的输出为N维向量 $h$ ， 隐层到输出层的权重矩阵为 $W’_{N \\times V}$ 。然后，通过矩阵运算我们得到一个 $V \\times 1 $ 维向量 u = W'^{T} * h 其中，向量 $u$ 的第 $i$ 行表示词汇表中第 $i$ 个词的可能性，然后我们的目的就是取可能性最高的那个词。因此，在最后的输出层是一个softmax 层获取分数最高的词，那么就有我们的最终输出： P(w_j| context) =y_i = \\frac{exp({u_j})}{\\sum_{k \\in V} exp({u_k})}1.1.2 损失函数我们假定 $j^*$ 是真实单词在词汇表中的下标，那么根据极大似然法，则目标函数定义如下： E = -log \\, p(W_O |W_I) = -log \\, \\frac{exp({u_j})}{\\sum_{k \\in V} exp({u_k})} = log \\, \\sum_{k \\in V} exp(u_{k}) -u_j1.2 Skip-gram模型Skip-Gram的基本思想是：已知当前词 $wt$ 的前提下，预测其上下文 $w{t-i}, \\cdots , w_{t+i}$ ，模型如下图所示： 1.2.1前向传播过程 输入层： 输入的是一个单词，其表示形式为 One-hot ，我们将其表示为V维向量 $xk$ ，其中 $V$ 为词表大小。然后，通过词向量矩阵 $W{V \\times N}$ 我们得到一个N维向量 h = W^T * x_k = v^{T}_{w_I} 隐层： 而隐层中没有激活函数，也就是说输入=输出，因此隐藏的输出也是 $h$ 。 隐层到输出层： 首先，因为要输出C个单词，因此我们此时的输出有C个分布： $y_1, \\cdots y_C $，且每个分布都是独立的，我们需要单独计算， 其中 $y_i$ 表示窗口的第 $i$ 个单词的分布。 其次， 因为矩阵 $W’{N \\times V}$ 是共享的，因此我们得到的 $V \\times 1$ 维向量 $u$ 其实是相同的，也就是有 $u{c,j} = u_j$ ，这里 $u$ 的每一行同 CBOW 中一样，表示的也是评分。 最后，每个分布都经过一个 softmax 层，不同于 CBOW，我们此处产生的是第 $i$ 个单词的分布（共有C个单词），如下： P(w_{i,j}| context) =y_i = \\frac{exp({u_j})}{\\sum_{k \\in V} exp({u_k})} 1.2.2 损失函数假设 $j^*$ 是真实单词在词汇表中的下标，那么根据极大似然法，则目标函数定义如下： \\begin{split} E &= - log \\, p(w_1, w_2, \\cdots, w_C | w_I) \\\\ &= - log \\prod_{c=1}^C P(w_c|w_i) \\\\ &= - log \\prod_{c=1}^{C} \\frac{exp(u_{c, j})}{\\sum_{k=1}^{V} exp(u_{c,k}) } \\\\ &= - \\sum_{c=1}^C u_{j^*_c} + C \\cdot log \\sum_{k=1}^{V} exp(u_k) \\end{split}1.3模型复杂度本节中我们来分析一下模型训练时的复杂度，无论是在CBOW还是Skip-Gram模型中，都需要学习两个词向量矩阵： $W, W’$ 。 对于矩阵 $W$ ， 从前向传播中可以看到， 可以看到对于每一个样本(或mini-batch)，CBOW更新 $W$ 的 C 行（h与C个x相关）， 而Skip-Gram 更新W中的其中一行（h与1个x相关），这点训练量并不算大。 对于 $W’$ 而言， 无论是 CBOW 还是 Skip-Gram 模型，每个训练样本(mini-batch)都更新 $W’$ 的所有 $V \\times N$ 个元素。 在现实中，用于语言模型训练的数据集通常都很大，此外词表也是巨大的，这就导致对于 $W’$ 的更新所花费的计算成本是很大的，真的是验证了一个道理：穷逼没必要搞语言模型。 为了解决优化起来速度太慢的问题， Word2Vec 中提供了两种策略来对这方面进行优化。 1.4Hierarchical SoftmaxHS 基于哈夫曼树将计算量大的部分转化为一种二分类问题。 原先的模型中，模型再隐层之后通过 $W’$ 连接输出层，现在 HS 则去掉了 $W’$ , 隐层向量 h 直接与上图的二叉树的 root 节点相连， 图中的每一个分支都代表一个选择。 上图中白色的叶子节点表示词表中所有的$|V|$个词， 黑色节点表示非叶子节点， 每一个叶子节点（单词）都对应一条从 root 节点出发的路径，而问题就转化为了使得 $w=w_o$ 这条路径的概率最大， 即：$P(w=w+O|w_I)$ 最大。 用 $n(w,j)$ 表示从 root 到叶子节点 w 的路径上的第 j 个非叶子节点, 并且每个非叶子节点都对应一个向量$v_{n(w,j)}′$, 维度与h 相同, 然后使用一个sigmod函数: $σ(x)=\\frac{1}{1+exp(−x)}∈[0,1]$ ，结合向量的内积, 来判断该向左还是向右，那么第 n 个节点向左以及向右的概率分别为： P(n,left) = \\sigma(v_w' \\cdot h) \\\\ P(n, right) = 1 - \\sigma(v_w' \\cdot h)那么就有： P(w=w_O|W_I) = \\prod_{j=1}^{L(w)-1} P(\\sigma(I(n(w, j+1 == left) v_w' \\cdot h)) $I()$ ：指示函数，条件成立值为1， 反之为 -1 $L(w)$ ：表示整条路径的长度 这样我们就能够通过训练来更新每个非叶子节点的参数 $v_w’$了。举例来说，图上加黑的黑色路径： $(n(w_2,1),n(w_2,2),n(w_2,3),w2$， 对于一个训练样本，我们要使得 $P(w_O=w_2|w_I)$ 概率最大： P(w_2=w_O) = P(n(w_2, 1), left) \\cdot P(n(w_2, 2), left) \\cdot P(n(w_2, 3), right)且需要注意的时，再一个非叶子节点处， 向左向右的概率和为1， 因此一直分裂下去，最后的和肯定还是1， 因此可以得出： \\sum_{j=1}^V P(w_j = w_O) = 1损失函数同样为最大似然： E = -log P(w = w_O | w_I) = -\\sum_{j=1}^{L(w) -1} log \\sigma([I] v_j'^Th)通过 HS， 隐层到输出层的计算量从 $O(V)$ 降到了 $O(logV)$。 1.5Negative Sampling — 负采样在 Word2Vec 中， 对于输出层来说，我每一个输出节点都要预测词表中所有词在当前位置的概率，在动辄几万甚至几十万大的词表中，用softmax 计算真的十分困难。 但我们的目的不在于训练一个精准的语言模型，而只是为了训练得到语言模型的副产物-词向量，那么我们可不可以把输出压缩呢，将几万的输出压缩到几十程度，这计算量是成几何倍数的下降。 负采样的思路很简单，不直接让模型从整个词表中找最可能的词，而是直接给定这个词（正例）和几个随机采样的噪声词（负例），然后模型能够从这几个词中找到正确的词，就算达到目的了。 那么如何对负例进行采样呢？作者直接使用基于词频的权重分布来获得概率分布进行抽样： weight(w) = \\frac{count(w)^{0.75}}{\\sum_u count(w)^{0.75}}相比于直接使用频次作为权重， 取0.75幂的好处可以减弱不同频次差异过大带来的影响，使得小频次的单词被采样的概率变大。 此时的损失函数为： E = - log \\sigma(v_{w_O}' h) - \\sum_{w_j \\in W_{neg}} log \\sigma(-v_{w_j}'h)Glove []QuestionsReference Papers[1] Mikolov, T.(2013). Distributed Representations of Words and Phrases and their Compositionality. [2] Mikolov, T.(2013). Efficient Estimation of Word Representations in Vector Space. [3] Rong, X. (2014). word2vec Parameter Learning Explained. [4] GloVe: Global Vectors for Word Representation [5] Enriching Word Vectors with Subword Information [6] Bag of Tricks for Efficient Text Classification"},{"title":"16.1词嵌入（Word2Vec）","path":"/2023/12/02/16-1-ci-qian-ru-word2vec/","content":"1.什么是词嵌入（Word Embedding）自然语言是一套用来表达含义的复杂系统。在这套系统中，词是表意的基本单元。顾名思义，词向量是用来表示词的向量，也可以被认为是词的特征向量或表征。把词映射为实数域向量的技术也叫做词嵌入（word embedding）。近年来，词嵌入已逐渐称为自然语言处理的基础知识。 在NLP（自然语言处理）领域，文本表示是第一步，也是很重要的一步，通俗来说就是把人类的语言符号转换为机器能够进行计算的数字，因为普通的文本雨涵机器是看不懂的，必须通过转化来标注对应的文本。早期是基于规则的方法进行转换，而现代的方法是基于统计机器学习的方法。 数据决定了机器学习的上限,而算法只是尽可能逼近这个上限，在本文中数据指的就是文本表示，所以，弄懂文本表示的发展历程，对于NLP学习者来说是必不可少的。接下来开始我们的发展历程。文本表示分为离散表示和分布式表示： 2.离散表示2.1One-hot表示One-hot简称独热向量编码，也是特征工程中最常用的方法。其步骤如下： 构造文本分词后的字典，每个分词是一个比特值，比特值为0或者1。 每个分词的文本表示为该分词的比特位为1，其余位为0的矩阵表示。 例如：John likes to watch movies. Mary likes too例如：约翰喜欢看电影。玛丽也喜欢 John also likes to watch football games.约翰还喜欢看足球比赛。 以上两句可以构造一个词典，{“John”: 1, “likes”: 2, “to”: 3, “watch”: 4, “movies”: 5, “also”: 6, “football”: 7, “games”: 8, “Mary”: 9, “too”: 10} 以上两句可以构造一个字典，{“John”: 1, “likes”: 2, “to”: 3, “watch”: 4, “movies”: 5, “also”: 6, “football” : 7、“游戏”: 8、“玛丽”: 9、“也是”: 10} 每个词典索引对应着比特位。那么利用One-hot表示为： John: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] 约翰: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0] likes: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0] …….等等，以此类推。 随着语料库的增加，数据特征的维度会越来越大，产生一个维度很高，又很稀疏的矩阵。 这种表示方法的分词顺序和在句子中的顺序是无关的，不能保留词与词之间的关系信息。 2.2词袋模型词袋模型(Bag-of-words model)，像是句子或是文件这样的文字可以用一个袋子装着这些词的方式表现，这种表现方式不考虑文法以及词的顺序。 文档的向量表示可以直接将各词的词向量表示加和。例如： John likes to watch movies. Mary likes too约翰喜欢看电影。玛丽也喜欢 John also likes to watch football games.约翰还喜欢看足球比赛。 那么第一句的向量表示为：[1,2,1,1,1,0,0,0,1,1]，其中的2表示likes在该句中出现了2次，依次类推。 词袋模型同样有以下缺点： 词向量化后，词与词之间是有大小关系的，不一定词出现的越多，权重越大。 词与词之间是没有顺序关系的。 2.3TF-IDFTF-IDF（term frequency–inverse document frequency）是一种用于信息检索与数据挖掘的常用加权技术。TF意思是词频(Term Frequency)，IDF意思是逆文本频率指数(Inverse Document Frequency)。 字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。一个词语在一篇文章中出现次数越多, 同时在所有文档中出现次数越少, 越能够代表该文章。 分母之所以加1，是为了避免分母为0。 那么，，从这个公式可以看出，当w在文档中出现的次数增大时，而TF-IDF的值是减小的，所以也就体现了以上所说的了。 缺点：还是没有把词与词之间的关系顺序表达出来。 2.4n-gram模型n-gram模型为了保持词的顺序，做了一个滑窗的操作，这里的n表示的就是滑窗的大小，例如2-gram模型，也就是把2个词当做一组来处理，然后向后移动一个词的长度，再次组成另一组词，把这些生成一个字典，按照词袋模型的方式进行编码得到结果。改模型考虑了词的顺序。 例如： John likes to watch movies. Mary likes too约翰喜欢看电影。玛丽也喜欢 John also likes to watch football games.约翰还喜欢看足球比赛。 以上两句可以构造一个词典，{“John likes”: 1, “likes to”: 2, “to watch”: 3, “watch movies”: 4, “Mary likes”: 5, “likes too”: 6, “John also”: 7, “also likes”: 8, “watch football”: 9, “football games”: 10} 以上两句可以构造一个字典，{“约翰喜欢”: 1, “喜欢”: 2, “看”: 3, “看电影”: 4, “玛丽喜欢”: 5, “也喜欢”: 6 , “约翰也”: 7, “也喜欢”: 8, “看足球”: 9, “足球比赛”: 10} 那么第一句的向量表示为：[1, 1, 1, 1, 1, 1, 0, 0, 0, 0]，其中第一个1表示John likes在该句中出现了1次，依次类推。 缺点：随着n的大小增加，词表会成指数型膨胀，会越来越大。 2.5离散表示存在的问题由于存在以下的问题，对于一般的NLP问题，是可以使用离散表示文本信息来解决问题的，但对于要求精度较高的场景就不适合了。 无法衡量词向量之间的关系。 词表的维度随着语料库的增长而膨胀。 n-gram词序列随语料库增长呈指数型膨胀，更加快。 离散数据来表示文本会带来数据稀疏问题，导致丢失了信息，与我们生活中理解的信息是不一样的。 3.分布式表示科学家们为了提高模型的精度，又发明出了分布式的表示文本信息的方法，这就是这一节需要介绍的。 用一个词附近的其它词来表示该词，这是现代统计自然语言处理中最有创见的想法之一。当初科学家发明这种方法是基于人的语言表达，认为一个词是由这个词的周边词汇一起来构成精确的语义信息。就好比，物以类聚人以群分，如果你想了解一个人，可以通过他周围的人进行了解，因为周围人都有一些共同点才能聚集起来。 3.1共现矩阵共现矩阵顾名思义就是共同出现的意思，词文档的共现矩阵主要用于发现主题(topic)，用于主题模型，如LSA。 局域窗中的word-word共现矩阵可以挖掘语法和语义信息，例如： I like deep learning.我喜欢深度学习。 I like NLP. 我喜欢自然语言处理。 I enjoy flying 我喜欢飞行 有以上三句话，设置滑窗为2，可以得到一个词典：{“I like”,”like deep”,”deep learning”,”like NLP”,”I enjoy”,”enjoy flying”}。 我们可以得到一个共现矩阵(对称矩阵)： 中间的每个格子表示的是行和列组成的词组在词典中共同出现的次数，也就体现了共现的特性。 存在的问题： 向量维数随着词典大小线性增长。 存储整个词典的空间消耗非常大。 一些模型如文本分类模型会面临稀疏性问题。 模型会欠稳定，每新增一份语料进来，稳定性就会变化。 4.神经网络表示4.1NNLMNNLM (Neural Network Language model)，神经网络语言模型是03年提出来的，通过训练得到中间产物—词向量矩阵，这就是我们要得到的文本表示向量矩阵。 NNLM说的是定义一个前向窗口大小，其实和上面提到的窗口是一个意思。把这个窗口中最后一个词当做y，把之前的词当做输入x，通俗来说就是预测这个窗口中最后一个词出现概率的模型。 以下是NNLM的网络结构图： input层是一个前向词的输入，是经过one-hot编码的词向量表示形式，具有V*1的矩阵。 C矩阵是投影矩阵，也就是稠密词向量表示，在神经网络中是w参数矩阵，该矩阵的大小为D*V，正好与input层进行全连接(相乘)得到D1的矩阵，采用线性映射将one-hot表示投影到稠密D维表示。 4.2Word2Vec谷歌2013年提出的Word2Vec是目前最常用的词嵌入模型之一。Word2Vec实际是一种浅层的神经网络模型，它有两种网络结构，分别是CBOW（Continues Bag of Words）连续词袋和Skip-gram。Word2Vec和上面的NNLM很类似，但比NNLM简单。 CBOW CBOW获得中间词两边的的上下文，然后用周围的词去预测中间的词，把中间词当做y，把窗口中的其它词当做x输入，x输入是经过one-hot编码过的，然后通过一个隐层进行求和操作，最后通过激活函数softmax，可以计算出每个单词的生成概率，接下来的任务就是训练神经网络的权重，使得语料库中所有单词的整体生成概率最大化，而求得的权重矩阵就是文本表示词向量的结果。 Skip-gram： Skip-gram是通过当前词来预测窗口中上下文词出现的概率模型，把当前词当做x，把窗口中其它词当做y，依然是通过一个隐层接一个Softmax激活函数来预测其它词的概率。如下图所示： 优化方法： 层次Softmax：至此还没有结束，因为如果单单只是接一个softmax激活函数，计算量还是很大的，有多少词就会有多少维的权重矩阵，所以这里就提出层次Softmax(Hierarchical Softmax)，使用Huffman Tree来编码输出层的词典，相当于平铺到各个叶子节点上，瞬间把维度降低到了树的深度，可以看如下图所示。这课Tree把出现频率高的词放到靠近根节点的叶子节点处，每一次只要做二分类计算，计算路径上所有非叶子节点词向量的贡献即可。 哈夫曼树(Huffman Tree)：给定N个权值作为N个叶子结点，构造一棵二叉树，若该树的带权路径长度达到最小，称这样的二叉树为最优二叉树，也称为哈夫曼树(Huffman Tree)。哈夫曼树是带权路径长度最短的树，权值较大的结点离根较近。 负例采样(Negative Sampling)：这种优化方式做的事情是，在正确单词以外的负样本中进行采样，最终目的是为了减少负样本的数量，达到减少计算量效果。将词典中的每一个词对应一条线段，所有词组成了[0，1］间的剖分，如下图所示，然后每次随机生成一个[1, M-1]间的整数，看落在哪个词对应的剖分上就选择哪个词，最后会得到一个负样本集合。 Word2Vec存在的问题 对每个local context window单独训练，没有利用包 含在global co-currence矩阵中的统计信息。 对多义词无法很好的表示和处理，因为使用了唯一的词向量 4.3sense2vecword2vec模型的问题在于词语的多义性。比如duck这个单词常见的含义有水禽或者下蹲，但对于 word2vec 模型来说，它倾向于将所有概念做归一化平滑处理，得到一个最终的表现形式。 5.词嵌入为何不采用one-hot向量虽然one-hot词向量构造起来很容易，但通常并不是⼀个好选择。⼀个主要的原因是，one-hot词向量⽆法准确表达不同词之间的相似度，如我们常常使⽤的余弦相似度。由于任何两个不同词的one-hot向量的余弦相似度都为0，多个不同词之间的相似度难以通过one-hot向量准确地体现出来。 word2vec⼯具的提出正是为了解决上⾯这个问题。它将每个词表⽰成⼀个定⻓的向量，并使得这些向量能较好地表达不同词之间的相似和类⽐关系。 6.Word2Vec代码实现","tags":["词嵌入（Word2Vec）"],"categories":["自然语言处理实战教程"]},{"title":"Kaggle's_30_Days_Of_ML","path":"/2023/11/16/kaggle-s-30-days-of-ml/","content":"Kaggle’s 30 Days Of ML (Day-1): Getting Started With KaggleKaggle’s 30 Days Of ML (Day-9): First Machine Learning Model and Validation","tags":["kaggle比赛"],"categories":["kaggle比赛"]},{"title":"文献整理","path":"/2023/11/09/wen-xian-zheng-li/","content":"基于大语言模型的零样本信息抽取方法实现 Event Extraction by Answering (Almost) Natural Questions","tags":["文献整理"],"categories":["文献整理"]},{"title":"基于提示的零样本关系抽取方法探索","path":"/2023/11/06/ji-yu-ti-shi-de-ling-yang-ben-guan-xi-chou-qu-fang-fa-tan-suo/","content":"基于提示的零样本关系抽取方法探索Abstract零样本关系抽取是处理现实世界中缺乏标记数据的新兴关系的重要方法。然而，主流的两塔零样本方法通常依赖于预定义关系的大规模域内标记数据。在这项工作中，我们将零样本关系提取视为一种通过提示调整优化的语义匹配任务，当预定义关系的标记数据极其稀缺时，它仍然保持优异的泛化性能。为了最大限度地提高数据利用的效率，我们引入了一种提示调整技术来引出预训练语言模型（PLM）中现有的关系知识，而不是直接进行微调。此外，在训练过程中模型接触到的关系描述很少，我们认为这是两塔方法的性能瓶颈。为了突破瓶颈，我们在编码过程中直接对关系实例及其描述之间的语义交互进行建模。在两个学术数据集上的实验结果表明：（1）我们的方法在预定义关系的不同样本上大幅优于之前最先进的方法； （2）这种优势在资源匮乏的情况下会进一步放大。 Introduction近年来，人们对从发票、采购订单、纳税申报表等各种垂直领域的类似表单的文档中提取结构化信息越来越感兴趣。[Zhao、Wu 和 Wang 2019；林等人。 2020；于等人。 2019]。在本文中，我们重新审视关键信息提取（KIE）问题，即从给定文档中提取一组键的值[Huang et al. 2017]。 2019]。例如，在图 1 中，给定一组键（“电话”、“总计”）和左侧收据文档，KIE 任务旨在提取“电话”的值“03-55423228”和“电话”的值“50.60”全部的”。提取的结构化信息对于广泛的下游任务至关重要，例如知识库构建、问答、文档理解等[Liu and Croft 2002；吴森等人。 2018； Geva 和 Berant 2018]。","tags":["ACL2022"],"categories":["NLP顶会"]},{"title":"忘掉你想忘掉的东西：LLMs的高效忘却","path":"/2023/11/06/wang-diao-ni-xiang-wang-diao-de-dong-xi-llms-de-gao-xiao-wang-que/","content":"忘掉你想忘掉的东西：LLMs的高效忘却Abstract大型语言模型（LLM）在预训练和记忆各种文本数据方面取得了显着进展，然而，这个过程可能会遇到隐私问题和违反数据保护法规的问题。因此，从此类模型中轻松删除与个人用户相关的数据，同时在删除后不降低其预测质量的能力变得越来越重要。为了解决这些问题，在这项工作中，我们提出了一种有效的取消学习框架，通过将选择性师生目标学习的轻量级取消学习层引入到变压器中，可以有效地更新 LLM，而无需在数据删除后重新训练整个模型。此外，我们引入了一种融合机制来有效地结合不同的遗忘层，学习遗忘不同的数据集来处理一系列遗忘操作。分类和生成任务的实验证明了我们提出的方法与最先进的基线相比的有效性。 1 Introduction利用大型语言模型（LLM）已成为各种 NLP 应用的主导范式（Brown et al., 2020；Chowdhery et al., 2022a；Kojima et al., 2022；Ouyang et al., 2022；Brown et al., 2020；Radford et al., 2019；Lewkowycz et al., 2022；Qin et al., 2023；Touvron et al., 2023），因为法学硕士在预训练或大范围微调期间会记住大量知识文本数据（Brown 等人，2020；Radford 等人，2019；Hoffmann 等人，2022；Webson 和 Pavlick，2022；Min 等人，2022；Liang 等人，2022；Carlini 等人， 2022）。然而，这些数据可能包含敏感信息，例如姓名、电话号码、电子邮件地址和私人临床记录（Jang 等人，2022 年；Kurmanji 等人，2023 年；Kumar 等人，2022）。广泛的研究表明，法学硕士可以生成私人信息，例如《麻省理工学院技术评论》主编，包括他的家庭成员、工作地址和电话号码（Carlini 等人，2022）。最近，欧盟的《通用数据保护条例》（GDPR）和美国的《加州消费者隐私法案》（CCPA）也对被遗忘权提出了新的规定，要求应用程序支持在用户请求时删除用户生成的内容（Sekhari）等人，2021 年；库马尔等人，2022 年）。有鉴于此，有必要为法学硕士提供一种高效且有效的方法来忘记用户所请求的信息。 ​ 最近人们开始关注通过再训练和数据预处理来处理法学硕士的此类忘却请求（Bourtoule et al., 2021; Kumar et al., 2022），其中训练数据存储在不同的隔离切片和每个检查点中在每个切片上训练后保存。当收到删除请求时，相应的数据点将从切片中删除，并且直到该数据点的模型检查点将用于进一步重新训练模型。遗忘的影响通常通过已删除数据的模型错误来体现（模型无法预测已删除数据）（Kurmanji et al., 2023; Jang et al., 2022）。其他工作也探索了确保差分隐私（DP）的算法设计（Yu et al., 2021；Li et al., 2021；Anil et al., 2021）。然而，像 SISA (Bourtoule et al., 2021) 这样的机器去学习方法通常需要大量的存储空间 (Bourtoule et al., 2021)，而 DP 方法可能会导致模型性能收敛缓慢和显着恶化 (Nguyen等人，2022）。此外，两者都需要重新训练整个模型，考虑到当前法学硕士的模型规模，这是极其昂贵和耗时的。这些限制也使它们无法动态处理一系列忘记学习的请求，这些请求通常是现实场景中的需要（Jang et al., 2022; Nguyen et al., 2022）。 【我们EUL框架的整体流程。遗忘层被插入到前馈网络之后的变压器层中。在训练过程中，只有取消学习层会忘记所请求的数据，而原始的 LLM 保持不变。对于每个删除请求，首先学习一个取消学习层，然后通过我们设计的融合机制与其他取消学习层合并，形成满足一系列删除请求的融合取消学习变压器。】 ​ 为了填补这些空白，在这项工作中，我们提出了一种 LLM 的高效忘却方法（EUL），可以有效地忘却需要忘记的内容，而无需完全重新训练整个模型，同时保留模型的性能。具体来说，我们提出了一种轻量级方法来学习遗忘层，该方法通过选择性的师生公式（Kurmanji 等人，2023）在几次更新中插入变压器，而无需调整大型语言模型。此外，我们引入了一种融合机制，通过最小化回归目标，有效地将学习忘记不同数据集的不同未学习层的权重组合到单个统一的未学习层。这使得 EUL 能够有效地处理一系列删除操作。为了证明我们提出的 EUL 的有效性，我们在不同设置下的 IMDB（Maas 等人，2011）和 SAMSum（Gliwa 等人，2019）上进行了实验，与最先进的取消学习或模型编辑基线相比。总而言之，我们的主要贡献有三个： 我们引入了一种有效的忘却方法，通过选择性的师生公式以轻量级的方式消除所需数据的影响。 我们设计了一种融合机制，将学习忘记不同数据集的遗忘层合并到单个遗忘层中，以处理一系列删除操作。 我们在不同设置下使用不同规模的骨干模型进行分类和生成任务的实验，以说明EUL的有效性。 2 Related Work2.1 Large Language Models大型语言模型最近取得了广泛的进展（Brown et al., 2020; Radford et al., 2019; Smith et al., 2022; Rae et al., 2021; Chowdhery et al., 2022b; Touvron et al., 2023 ），特别是在扩大法学硕士方面，例如 LLAMA（Touvron 等人，2023）、Megatron-turing NLG（Smith 等人，2022）、Gopher（Rae 等人，2021）和 PaLM Chowdhery 等人。 （2022b）。其他工作也通过更长的训练（Hoffmann et al., 2022）、指令调整（Wang et al., 2022；Zhou et al., 2023）和人类反馈（Ouyang et al., 2022）在较小的模型上取得了更好的性能。 。然而，最近的研究表明，训练数据，例如姓名、电话号码、电子邮件地址，甚至银行帐号等个人身份信息（Carlini 等人，2021；Lee 等人，2021；Carlini 等人，2022） ; Jagielski et al., 2022），可以很容易地从 LLM 中提取，因为 LLM 会记住数十亿个参数的训练数据（Carlini et al., 2022）。我们的工作旨在通过允许从法学硕士中学习的参数中有效地消除所请求的或私有的数据来缓解此类问题。 2.2 Machine Unlearning for Privacy为了减轻法学硕士的隐私风险，引入了机器取消学习方法来消除用户要求删除的训练示例的贡献（Bourtoule 等人，2021；Chien 等人，2023），包括重新训练深度学习的精确取消学习删除后新数据集上的学习模型（Bourtoule et al., 2021）和近似遗忘（Izzo et al., 2021; Golatkar et al., 2020; Kurmanji et al., 2023; Jang et al., 2022），旨在修改训练模型的权重以生成一组新的权重，这些权重近似于重新训练的权重。遗忘的影响通常通过已删除数据的模型错误来体现（模型无法预测已删除数据）（Kurmanji et al., 2023; Jang et al., 2022）。另一条工作重点是差分隐私（DP），它确保训练数据中的用户信息无法被推断（Dwork，2008；Yu et al.，2021；Li et al.，2021；Anil et al.，2021；Abadi等人，2016）。然而，这两种方法都需要重新训练整个模型，这是极其昂贵和耗时的，特别是对于大型语言模型，甚至会影响任务性能（Anil et al., 2021）。因此，它们无法动态处理删除序列（Jang et al., 2022；Nguyen et al., 2022）。为了克服这些限制，我们引入了一种有效的忘却方法以及融合机制来高效、动态地忘却用户数据序列。 我们的工作也与模型编辑相关（Mitchell et al., 2021; Belinkov et al., 2017; Dai et al., 2021; Wang et al., 2020），而他们通常专注于根据几个给定的数据编辑模型输出有关世界的语言结构或事实，而不是忘记所需的数据。 3 Efficient Unlearning for LLMs本节介绍了我们为法学硕士（EUL）设计的高效遗忘方法，该方法可以高效、动态地处理一系列删除请求。整体流程如图1所示。形式上，对于在数据集D = {(x, y)}上训练的大型语言模型F(.)，其中x是文本数据，y是相应的标签，并且删除请求忘记 Df = {(xf , yf } )，我们的目标是学习满足以下条件的更新模型 F ′(.) (Kurmanji et al., 2023)： 其中 Dr = D − Df = {(xr, yr)} 指的是我们想要保留的数据，I(.) 是互信息。直观上，我们将用 F(.) 更新 F(.)，为我们想要保留的数据生成类似的输出，同时丢失有关对我们想要忘记的数据进行预测的所有信息。 3.1 Learning to Forget via Unlearning Layers由于当前法学硕士的规模和训练数据量通常很大，更新模型 F(.) 中的所有参数（例如，在 Dr i 上重新训练 F(.)）变得极其昂贵。受参数高效微调最新进展的启发（Houlsby et al., 2019; Chien et al., 2023），我们通过 F (f (.)) 对 F ′(.) 进行建模，其中 f (.; W ) 是与 F (.) 相比，W 的参数数量显着减少。我们只会更新 f(.) 来满足取消学习的请求。 ​ 为了有效地实现等式 1 中的忘却目标，我们最小化了选择性的师生目标，其中学生模型 F ′(.) = F (f (.)) 被学习以遵循 Dr 上的教师模型 F (.)，同时不服从F (.) 在 Df 上： 其中 α 是一个超参数，用于平衡忘记 xf 和保留 xr 之间的权衡。直观上，在训练过程中，f(.) 倾向于最小化更新模型的输出和原始模型在要保留的数据上的输出之间的 KL 散度，同时最大化它们在要忘记的数据上的输出之间的 KL 散度。 ​ 为了保持任务性能，我们针对保留数据上的任务损失优化 f(.)： 其中 l(.) 是与任务相关的损失，例如，对于分类任务，交叉熵损失 - log P (F (f (xr)))。 ​ 此外，我们还否定了法学硕士中使用的原始训练目标（例如，掩码语言建模目标（Raffel et al., 2020）），以忘记与数据相关的知识，以便忘记预先训练的参数并确保遗忘数据中的信息不能轻易地从 F(.) 中提取： 其中l(.)是预训练F(.)时使用的语言模型损失，例如，屏蔽语言模型损失，− log P (^ x|x − ^ x)（^ x是随机屏蔽的标记）。在我们的实验中，我们使用 T5 模型（Raffel et al., 2020）。因此，我们在这个损失项的输入开头添加了一个额外的“预测屏蔽词”。 ​ 我们的最终培训目标如下： 其中 λ 和 γ 是超参数。在实践中，遵循 Kurmanji 等人。 （2023），我们交替更新要忘记的数据和要保留的数据，以更稳定地优化 LEUL 中的最小-最大项。具体来说，我们迭代地对要保留的数据执行一个纪元更新，然后对要忘记的数据执行一个纪元更新。 3.2 Fusing Unlearning Layers为了动态处理一系列遗忘请求并导出一个可以忘记所有请求数据的统一模型，我们引入了一种融合机制，可以合并不同的遗忘层 fi(.; Wi)，这些层学会了忘记 Df i = (Xf i ,Yf i ) 将上一节中的单个 f m(.; Wm) 转换为单个 f m(.; Wm)。也就是说，我们希望 Df i 上的 f m(.) 输出接近 fi(.)： 这是一个线性回归问题，有一个封闭式解： 具体来说，为了导出合并的遗忘层 f m 的权重 Wm，我们将使用遗忘数据 Xf i T Xf i 的 LLM 中的遗忘层之前的隐藏表示的预先计算的内积矩阵，然后根据公式 7 计算 Wm。 ​ 融合机制确保了效率和隐私，因为它可以在没有任何额外训练的情况下执行，并且只需要存储要忘记的数据表示的内积矩阵而不是数据本身。 Conclusion在这项工作中，我们提出了 EUL，这是一种针对LLMs的有效忘却方法，可以通过选择性的师生目标通过学习忘却层来高效且有效地忘却用户请求的数据。我们进一步引入了一种融合机制，可以将不同的遗忘层合并到一个统一的层中，以动态地遗忘一系列数据。对不同设置（不同数据集、不同模型大小、不同遗忘集大小）的实验证明了我们提出的 EUL 方法与最先进的基线相比的有效性。","tags":["EMNLP2023"],"categories":["NLP顶会"]},{"title":"并发编程","path":"/2023/11/03/bing-fa-bian-cheng/","content":"3.7 sleep和yieldsleep 1.调用sleep会让当前线程从Running进入Timed Waiting状态（阻塞） 2.其他线程可以使用interupt方法打断正在睡眠的线程，这时sleep方法会抛出InterruptedException 3.睡眠结束后的线程未必会立刻得到执行 4.建议用TimeUnit的sleep代替Thread的sleep来获得更好的可读性 yield 1.调用yield会让当前线程从Running进入Runnable就绪状态，然后调度执行其它线程 2.具体的实现依赖于操作系统的任务调度器 线程优先级 线程优先级会提示（hint）调度器优先调度该线程，但它仅仅是一个提示，调度器可以忽略它 如果CPU比较忙，那么优先级高的线程会获得更多的时间片，但CPU空闲时，优先级几乎没作用 不使用yield 1234567891011121314151617181920212223public class Test9 &#123; public static void main(String[] args) &#123; Runnable task1 = () -&gt; &#123; int count = 0; for (;;) &#123; System.out.prinln(&quot;----&gt;1&quot; + count ++ ); &#125; &#125;; Runnable task2 = () -&gt; &#123; int count = 0; for (;;) &#123; // Thread.yield(); System.out.println(&quot; ------&gt;2 &quot; + count ++ ); &#125; &#125;; Thread t1 = new Thread(task1, &quot;t1&quot;); Thread t2 = new Thread(task2, &quot;t2&quot;); // t1.setPriority(Thread.MIN_PRIORITY); // t2.setPriority(Thread.MAX_PRIORITY); t1.start(); t2.start(); &#125;&#125; 未开启yield两者打印次数基本差不多 开启后 开启优先级 案例-防止CPU占用100%sleep实现 在没有利用CPU进行计算时，不要让while(true)空转浪费CPU，这时可以使用yield或sleep来让出cpu的使用权给其他程序 1234567while(true) &#123; try &#123; Thread.sleep(50); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125;&#125; 可以使用wait或条件变量达到类似的效果 不同的是，后两种都需要加锁，并且需要响应的唤醒操作，一般适用于要进行同步的场景 sleep适用于无需锁同步的场景 不使用sleep，将会占满单核虚拟机资源 使用后","tags":["并发编程"],"categories":["软件开发"]},{"title":"JVM","path":"/2023/11/03/jvm/","content":"1. JVM与Java体系结构 1.1Java虚拟机字节码： 1.任何能在jvm平台上执行的字节码格式都是一样的，称为jvm字节码 2.不同的编译器，可以编译出相同的字节码文件，字节码文件也可以在不同的JVM上运行 3.Java虚拟机与Java语言并没有必然的联系，它只与特定的二进制文件——Class文件格式所关联，Class文件中包含了Java虚拟机指令集（或称为字节码、Bytecodes）和符号表，还有其他辅助信息 虚拟机： 1.所谓虚拟机（Virtual Machine），就是一台虚拟的计算机。他是一款软件，用来执行一系列不腻计算机指令。大体上，虚拟机可以分为系统虚拟机和程序虚拟机。Visual Box、VMware属于系统虚拟机，完全是对物理计算机的仿真，提供了一个可运行完整操作系统的软件平台；程序虚拟机就是Java虚拟机，它专门为执行单个计算机程序而设计 2.无论是系统虚拟机还是程序虚拟机，在上面运行的软件都被限制与虚拟机提供的自愿者中 作用 1.Java虚拟机就是二进制字节码的运行环境，负责装载字节码到其内部，解释/编译为对应平台上的机器指令执行。每一条Java指令，Java虚拟机规范中都有详细的定义，如怎么取操作数，怎么处理操作数，处理结果放在哪里。 特点 1.一次编译，到处运行 2.自动内存管理 3.自动垃圾回收功能 1.2JVM位置 JVM是运行在操作系统之上的，它与硬件没有直接的交互。 1.3Google的Android系统结构 1.4JVM整体结构 1.HotSpot VM是目前市面上高性能虚拟机的代表作之一 2.它采用解释器与即时编译器并存的架构 1.5Java代码的执行流程","tags":["JVM"],"categories":["软件开发"]},{"title":"设计模式","path":"/2023/11/02/she-ji-mo-shi/","content":"1，设计模式1.1 软件设计模式产生背景1.2 软件设计模式概念软件设计模式（Software Design Pattern），是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。它描述了在软件设计过程中一些不断重复发生的问题，以及该问题的解决方案。也就是说，它是解决特定问题的一系列套路，是前辈的代码设计经验的总结，具有一定的普遍性，可以反复被使用的 1.3 学习设计模式的必要性设计模式的本质是面向对象上街原则的实际运用，是对类的封装性、继承性、多态性以及类的关联关系和组合关系的充分理解。 正确使用设计模式优点： 提高思维能力、编程能力和设计能力 使得程序上街更加标准化、代码百年之更加工程化、使得软件开发效率大大提高，从而缩短软件的开发周期 使得设计的diamond可重用性高、可读性强、可靠性高、灵活性好、可维护性强。 1.4 设计模式分类 创建型模式 用于描述“怎样创建对象”，它的主要特点是“将对象的创建与使用分离”，GoF书中提供了单例、原型、工厂方法、抽象工厂、建造者等5种创建型模式。 结构型模式 用于描述如何将类或对象按某种布局组成更大的结构，GoF书中提供了代理、适配器、桥接、装饰、外观、享元、组合等7种结构型模式。 行为性模式 用于描述类或对象之间怎样相互协作共同完成单个对象无法单独完成的任务，以及怎样分配职责。GoF书中提供了模板方法、策略、命令、职责链、状态、观察者、中介者、迭代器、访问者、备忘录、解释器等11种行为型模式。 2，UML图统一建模语言（Unified Modeling Language，UML）是用来设计软件的可视化建模语言。特点是简单、统一、图形化、能表达软件设计种的动态与静态信息。 2.1类图概述类图（Class digram）是显示了模型的静态结构，特别是模型中存在的类、类的内部结构以及它们与其他类的关系等。类图不显示暂时性的信息，类图是面向对象建模的主要组成部分。 2.2类图的作用 在软件工程中，类图是一种惊天的结构图，描述了系统的类的集合，类的属性和类之间的关系，简化了人们对系统的理解。 类图是系统分析和设计阶段的重要产物，是系统编码和测hi是的额重要模型。 2.3类图表示法2.3.1类的表示方法 在UML类图中，类使用半酣类名、属性（field）和方法(method)且带有分割线的矩形来表示，比如下图表示一个Employee类，它包含name，age和address这3个属性，以及work()方法 属性/方法名称前加的加号和减号表示了这个属性/方法的可见性，UML类图中表示可见性的符号有三种： +表示public -表示private 表示protected 属性的完整表示方法是：可见性 名称：类型 [ = 缺省值] 方法的完整表示方式是：可见性 名称（参数列表）[ : 返回类型] 注意： 1.括号中的内容表示可选 2.也有将类型放在变量名前面，返回值类型放在方法名前面 例子： Demo定义了三个方法： method()方法：修饰符为public，没有参数，没有返回值 method1()方法：修饰符为private，没有参数，返回值类型为String method2()方法：修饰符为protexted，接收两个参数，第一个参数为int，第二个参数为String，返回值类型是int 2.3.2 类与类之间关系的表示方式2.3.2.1关联关系关联关系是对象之间的一种引用关系，用于表示一类对象与另一类对象之间的联系，如老师和学生、师傅和徒弟、丈夫和妻子等。关联关系是类与类直接按最常用的一种关系，非为一般关联关系、聚合关系和组合关系 关联又可以分为单向关联、双向该你了，自关联 1，单向关联 在UML类图中单向关联用一个带箭头的实线表示。上图表示每个顾客都有一个地址，这通过让Customer类持有一个类型为Address的成员变量类实现。 2，双向关联 所谓的双向关联就是双方各自持有对方类型的成员变量。 在UML类图中，双向关联用一个不带箭头的直线表示。上图中子啊Customer类中维护一个List，表示一个顾客可以购买多个商品；在Product类中维护一个Customer类型的成员变量表示这个产品被哪个顾客所购买。 3，自关联 自关联在UML类图中用一个带有箭头且指向自身的线表示。上图的意思就是Node类包含类型为Node的成员变量，也就是“自己包含自己”。 2.3.2.2 聚合关系聚合关系是关联关系的一种，是强关联关系，是部分和整体之间的关系。 聚合关系也是通过成员对象来实现的，其中成员对象是整体对象的一部分，但是成员对象可以脱离整体对象而独立存在。例如：学校和老师的关系，学校包含老师，但是如果学校停办了，老师依然存在。 在UML类图中，聚合关系可以用带空心菱形的实现表示，菱形指向整体。下图是大学和教师的关系图： 2.3.2.3 组合关系组合表示类之间的整体与部分的关系，但它是一种更加强烈的聚合关系。 在组合关系中，整体对象可以控制部分对象的生命周期，一旦整体对象不存在，部分对象也将不存在，部分对象不能脱离整体对象而存在。例如：头和嘴的关系，没有了头，嘴也就不存在了。 在UML类图中，组合关系用带实心菱形的实线来表示，菱形指向整体，下图所示是头和嘴的关系图： 2.3.2.4 依赖关系依赖关系是一种使用关系，它是对象之间耦合程度最弱的一种关联方式，是临时性的关联。在代码中，某个类的方法通过局部变量、方法的参数或者对静态方法的调用来访问另一个类（被依赖类）中的某些方法来完成一些职责。 在UML类图中，依赖关系使用带箭头的虚线来表示，箭头从使用类指向被依赖的类。下图所示是司机和汽车的关系图，司机驾驶汽车： 2.3.2.5 继承关系继承关系是对象之间耦合度最大的一种关系，表示一般与特殊的关系，是父类与子类之间的关系，是一种继承关系。 在UML类图中，泛化关系用带空心三角箭头的实线来表示，箭头从子类指向父类。在代码实现时，使用面向对象的继承机制来实现泛化关系。例如：Student类和Teacher类都是Person类的子类。 2.3.2.6 实现关系实现关系是接口与实现类之间的关系。在这种关系中，类实现了接口，类中国的操作实现接口中所声明的所有的抽象操作。 在UML类图中，实现关系使用带空心三角箭头的虚线来表示，箭头从实现类指向接口。例如：汽车和船实现了交通工具。 3，软件设计原则在软件开发中，为了提高软件系统的可维护性和可复用性，增加软件的可扩展性和灵活性，程序员要尽量根据6条原则来开发程序，从而提高软件开发效率、节约软件开发成本和维护成本。 3.1 开闭原则对扩展开放，对修改关闭。在程序需要进行扩展的时候，不能去修改原有的代码，不能去修改原有的代码，实现一个热插拔的效果。简言之，是为了使程序的扩展性好，易于维护和升级。 想要达到这样的效果，我们就需要接口和抽象类。 因为抽象灵活性好，适应性广，只要抽象的合理，可以基本保持软件架构的稳定。而软件中易变的细节可以从抽象派生出来的实现类来进行扩展，当软件需要发生变化时，只需要根据要求重新派生一个实现类来扩展就可以了。 下面哦才能够搜狗输入法的皮肤为例介绍开闭原则的应用。 例子：搜狗输入法的皮肤设计 分析：搜狗输入法的皮肤时输入法背景图片、窗口颜色和声音等元素的组合。用户可以根据自己的喜爱更换自己的输入法的皮肤，也可以从网上下载新的皮肤。这些皮肤有共同的特点，可以为其定义一个抽象类（AbstractSkin），而每个具体的皮肤（DefaultSpecificSkin和HiSpecificSkin）是其子类。用户窗体可以根据需要选择或增加新的主题，而不需要修改原代码，所以它是满足开闭原则的。 1234567891011121314package com.cup.principles.demo1;/*** @version v1.0* @ClassName: AbstractSkin* @Description: 抽象类* @Author: humbleyl*/public abstract class AbstractSkin &#123; // 显示方法 public abstract void display();&#125; 123456789101112131415package com.cup.principles.demo1;/*** @version v1.0* @ClassName: DefaultSkin* @Description: 默认皮肤类* @Author: humbleyl*/public class DefaultSkin extends AbstractSkin&#123; public void display() &#123; System.out.println(&quot;默认皮肤&quot;); &#125;&#125; 123456789101112131415package com.cup.priciples.demo1;/*** @version v1.0* @ClassName: HiSkin* @Description: Hi皮肤* @Author: humbleyl*/public class HiSkin extends AbstractSkin &#123; public void display() &#123; System.out.printlin(&quot;Hi皮肤&quot;); &#125;&#125; 123456789101112131415161718192021package com.cup.priciples.demo1;/*** @version v1.0* @ClassName: SougouInput* @Description: 搜狗输入法* @Author: humbleyl*/public class SougouInput &#123; private AbstractSkin skin; public void setSkin(AbstractSkin skin) &#123; this.skin = skin; &#125; public void display() &#123; skin.display(); &#125;&#125; 12345678910111213141516171819202122package com.cup.priciples.demo1;/*** @version v1.0* @ClassName: SougouInput* @Description: 搜狗输入法* @Author: humbleyl*/public class Client &#123; public static void main(String[] args) &#123; // 创建搜狗输入法对象 SougouInput input = new SougouInput(); // 创建皮肤对象 DefaultSkin skin = new DefaultSkin(); // 将皮肤设置到输入法中去 input.setSkin(skin); // 显示皮肤 input.display(); &#125;&#125; 3.2 里氏代换原则里氏代换原则是面向对象设计的而基本原则之一。 里氏代换原则：任何基类可以出现的地方，子类一定可以出现。通俗理解：子类可以扩展父类的功能，但不能该百年父类原有的功能。换句话说：子类继承父类时，除添加新的方法完成新增功能外，尽量不要重写父类的方法。 如果通过重写父类的方法来完成新的功能，这样写起来虽然简单，但是整个继承体系的可复用性会比较差，特别是运用堕胎比较频繁时，出现运行出错的概率会非常大。 里氏代换原则经典例子 例子：正方形不是长方形。 在数学里面，正方形毫无疑问时长方形，他是一个长宽相等的长方形。所以，我们开发的一个于几何图像相关的软件系统，就可以顺理成章的让正方形继承长方形。 123456789101112131415161718192021222324252627282930package com.cup.principles.demo2.before;/*** @version v1.0* @ClassName: Rectangle* @Description: 长方形类* @Author: humbleyl*/public class Rectangle &#123; private double length; private double width; public double getLength() &#123; return length; &#125; public void setLength(double length) &#123; this.length = length; &#125; public double getWidth() &#123; return width; &#125; public void setWidth(double width) &#123; this.width = width; &#125;&#125; 1234567891011121314151617181920212223package com.principles.demo2.before;/*** @version v1.0* @ClassName: Square* @Description: 正方形类* @Author: humbleyl*/public class Square extends Rectangle &#123; @Override public void setLength(double length) &#123; super.setLength(length); super.setWidth(length); &#125; @Override public void setWidth(double width) &#123; super.setLength(width); super.setWidth(width); &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637package com.cup.principles.demo2.before;/*** @version v1.0* @ClassName: RectangleDemo* @Description: 测试类* @Author: humbleyl*/public class RectangleDemo &#123; public static void main(String[] args) &#123; // 创建长方形对象 Rectangle r = new Rectangle(); // 设置长和宽 r.setWidth(20); r.setWidth(10); // 调用resize方法进行扩宽操作 resize(r); // 打印结果 printLengthAngWidth(r); &#125; // 扩宽方法 public static void resize(Rectangle rectangle) &#123; // 判断宽如果比长小，则进行扩宽操作 while(rectangle.getWidth() &lt;= rectangle.getLength()) &#123; rectang;e.setWidth(rectangle.getWidth() + 1); &#125; &#125; // 打印长和宽 public static void printLengthAngWidth (Rectangle rectangle) &#123; System.out.println(rectangle.getWidth()); System.out.println(rectangle.getLength()); &#125;&#125; 如果进行下面的操作 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.cup.principles.demo2.before;/*** @version v1.0* @ClassName: RectangleDemo* @Description: 测试类* @Author: humbleyl*/public class RectangleDemo &#123; public static void main(String[] args) &#123; // 创建长方形对象 Rectangle r = new Rectangle(); // 设置长和宽 r.setWidth(20); r.setWidth(10); // 调用resize方法进行扩宽操作 resize(r); // 打印结果 printLengthAngWidth(r); System.out.prinln(&quot;===================&quot;); // 创建正方形对象 Square s = new Square(); // 设置长和宽 s.setLenth(10); // 调用resize方法进行扩宽操作 resize(s); // 打印结果 printLengthAngWidth(s); &#125; // 扩宽方法 public static void resize(Rectangle rectangle) &#123; // 判断宽如果比长小，则进行扩宽操作 while(rectangle.getWidth() &lt;= rectangle.getLength()) &#123; rectang;e.setWidth(rectangle.getWidth() + 1); &#125; &#125; // 打印长和宽 public static void printLengthAngWidth (Rectangle rectangle) &#123; System.out.println(rectangle.getWidth()); System.out.println(rectangle.getLength()); &#125;&#125; 卡死了，程序继续在运行 我们运行一下这段代码就会发现，加入我们把一个普通长方形作为参数传入resize方法，就会看到长方形阔度逐渐增长的效果，当宽度大于长度，代码就会停止，这种行为的结果符合我们的预期；假如我们再把一个正方形作为参数传入resize方法后，就会看到正方形的宽度和长度不断增长，代码会一致运行下去，直至系统产生溢出错误。所以，普通的长方形是适合这段代码的，正方形不适合。 我们得出结论：在resize方法中，Rectangle类型的参数是不能被Square类型的参数是不能被Square类型的参数所替代，如果进行了替换就得不到预期结果。因此，Square类和Rectangle类之间的继承关系违反了里氏代换原则，它们之间的继承关系不成立，正方形不是长方形。 如何改进呢？此时我们需要重新设计他们之间的关系，抽象出来一个四边形接口（Quardrilateral），让Rectangle类和Square类实现Quadrilateral接口 123456789101112131415package com.cup.principles.demo2.after;/*** @version v1.0* @ClassName: Quardrilateral* @Description: 四边形接口* @Author: humbleyl*/public interface Quardrilateral &#123; // 获取长 double getLength(); // 获取宽 double getWidth();&#125; 1234567891011121314151617181920212223242526272829package com.cup.principles.demo2.after;/*** @version v1.0* @ClassName: Square* @Description: 正方形* @Author: humbleyl*/public class Square implements Quadrilateral &#123; private double side; public double getSide() &#123; return side; &#125; public void setSide(double side) &#123; this.side = side; &#125; public double getLength() &#123; return side; &#125; public double getWidth() &#123; return side; &#125;&#125; 123456789101112131415161718192021222324252627282930package com.cup.principles.demo2.after;/*** @version v1.0* @ClassName: Rectangle* @Description: 长方形* @Author: humbleyl*/public class Rectangle implements Quadrilateral &#123; private double length; private double width; public double setLength(double length) &#123; this.length = length; &#125; public void setWidth(double width) &#123; this.width = width; &#125; public double getLength() &#123; return length; &#125; public double getWidth() &#123; return width; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334package com.cup.pinciples.demo2.after;/*** @version v1.0* @ClassName: RectangleDemo* @Description: 测试类* @Author: humbleyl*/public class RectangleDemo &#123; public static void main(Sting[] args) &#123; // 创建长方形对象 Rectangle r = new Rectangle(); r.setLength(20); r.setWidth(10); // 调用方法进行扩宽操作 resise(r); printLengthAndWidth(r); &#125; // 扩宽的方法 public static void resize(Rectangle rectangle) &#123; // 判断宽如果比长小，则进行扩宽操作 while(rectangle.getWidth() &lt;= rectangle.getLength()) &#123; rectang;e.setWidth(rectangle.getWidth() + 1); &#125; // 打印长和宽 public static void printLengthAndWidth(Quadrilateral quadrilateral) &#123; System.out.println(quadrilateral.getLength()); System.out.println(quadrilateral.getWidth()); &#125;&#125; 3.3依赖倒转原则高层模块不应该依赖于底层模块，两者都应该依赖其抽象；抽象不应该依赖于细节，细节应该依赖于抽象。简单说就是要求对抽象进行编程，不要对实现进行编程，这样就降低了客户与是西安模块间的耦合。 下面看一个例子来理解依赖倒转原则 【例】组装电脑 现在要组装一台电脑，需要配件CPU，硬盘，内存条。只有这些配置都有了，计算机才能正常的与性能。选择CPU有很多选择，如Inter，AMD等，硬盘可以选择希捷，西部数据等，内存条可以选择金士顿，海盗船等。 类图如下： 12345678910111213141516171819202122package com.cup.principles.demo3.before;/*** @version v1.0* @ClassName: XiJieHardDisk* @Description: 希捷硬盘* @Author: humbleyl*/public class XiJieHardDisk &#123; // 存储数据方法 public void save(String data) &#123; System.out.printlin(&quot;使用希捷硬盘存储数据为&quot; + data); &#125; // 获取数据的方法 public String get() &#123; System.out.println(&quot;使用希捷硬盘获取数据为&quot;); return &quot;数据&quot;; &#125;&#125; 123456789101112131415package com.cup.principles.demo3.before;/*** @version v1.0* @ClassName: Intel cpu* @Description: Intel cpu* @Author: humbleyl*/public class IntelCpu &#123; public void run() &#123; System.out.println(&quot;使用Intel处理器&quot;); &#125;&#125; 123456789101112131415package com.cup.principles.demo3.before;/*** @version v1.0* @ClassName: KingstonMemory* @Description: 金士顿内存条* @Author: humbleyl*/public class KingstonMemory &#123; public void save() &#123; System.out.println(&quot;使用金士顿内存条&quot;); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.cup.principles.demo3.before;/*** @version v1.0* @ClassName: Computer* @Description: 计算机类* @Author: humbleyl*/public class Computer &#123; private XiJieHardDisk hardDisk; private IntelCpu cpu; private KingstonMemory memory; public XiJieHardDisk getHardDisk() &#123; return hardDisk; &#125; public void setHardDisk(XiJieHardDisk hardDisk) &#123; this.hardDisk = hardDisk; &#125; public IntelCpu getCpu() &#123; retrun cpu; &#125; public void setCpu(IntelCpu cpu) &#123; this.cpu = cpu; &#125; public KingstonMemory getMemory() &#123; return memory; &#125; public void setMemory(KingstonMemory memory) &#123; this.memory = memory; &#125; public void run() &#123; System.out.println(&quot;开始运行计算机&quot;); // 开机先从硬盘加载数据 String data = hardDisk.get(); System.out.println(&quot;从硬盘上获取的数据是：&quot; + data); // 运行cpu cpu.run(); // 内存条保存 memory.save(); &#125;&#125; 123456789101112131415161718192021package com.cpu.principles.demo3.before;public class ComputerDemo &#123; public static void main(String[] args) &#123; // 创建组件对象 XiJieHardDisk hardDisk = new XiJieHardDisk(); IntelCpu cpu = new IntelCpu(); KingstonMemory memory = new KingstonMemory(); // 创建计算机对象 Computer c = new Computer(); // 组装计算机 c.setCpu(cpu); c.setHardDisk(hardDisk); c.setMemory(memory); // 运行计算机 c.run(); &#125;&#125; 上面代码可以看到已经组装了一台电脑，但是似乎组装的电脑的cpu只能是Intel，内存条只能是金士顿的，硬盘只能是希捷的，这对用户肯定是不友好的额，用户有了机箱后肯定是按照自己的喜好，选择自己喜欢的额配件。 根据依赖倒转原则进行改进 代码我们只需要修改Computer类，让Computer类依赖抽象（各个配件的接口），而不是依赖于各个组件具体的实现类。 类图如下： 1234567891011121314151617package com.cup.principles.demo3.after;/*** @version v1.0* @ClassName: HardDisk* @Description: 硬盘接口* @Author: humbleyl*/public interface HardDisk &#123; // 存储数据 public void save(String data); // 获取数据 public String get();&#125; 12345678910111213141516171819202122package com.cuo.principes.demo3.after;/*** @version v1.0* @ClassName: XiJieHardDisk* @Description: 希捷硬盘* @Author: humbleyl*/public class XiJieHardDisk implements HardDisk &#123; // 存储数据方法 public void save(String data) &#123; System.out.printlin(&quot;使用希捷硬盘存储数据为&quot; + data); &#125; // 获取数据的方法 public String get() &#123; System.out.println(&quot;使用希捷硬盘获取数据为&quot;); return &quot;数据&quot;; &#125;&#125; 1234567891011121314package com.cup.principles.demo3.after;/*** @version v1.0* @ClassName: Cpu* @Description: cpu接口* @Author: humbleyl*/public interface Cpu &#123; // 运行cpu public void run();&#125; 123456789101112131415package com.cup.principles.demo3.before;/*** @version v1.0* @ClassName: Intel cpu* @Description: Intel cpu* @Author: humbleyl*/public class IntelCpu implements Cpu&#123; public void run() &#123; System.out.println(&quot;使用Intel处理器&quot;); &#125;&#125; 1234567891011121314package com.cup.principles.demo3.after;/*** @version v1.0* @ClassName: Memory* @Description: 内存条接口* @Author: humbleyl*/public interface Memory &#123; // 存储数据 public void save();&#125; 123456789101112131415package com.cup.principles.demo3.before;/*** @version v1.0* @ClassName: KingstonMemory* @Description: 金士顿内存条* @Author: humbleyl*/public class KingstonMemory implements Memory&#123; public void save() &#123; System.out.println(&quot;使用金士顿内存条&quot;); &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950package com.cup.principles.demo3.before;/*** @version v1.0* @ClassName: Computer* @Description: 计算机类* @Author: humbleyl*/public class Computer &#123; private HardDisk hardDisk; private Cpu cpu; private Memory memory; public HardDisk getHardDisk() &#123; return hardDisk; &#125; public void setHardDisk(HardDisk hardDisk) &#123; this.hardDisk = hardDisk; &#125; public Cpu getCpu() &#123; retrun cpu; &#125; public void setCpu(Cpu cpu) &#123; this.cpu = cpu; &#125; public Memory getMemory() &#123; return memory; &#125; public void setMemory(Memory memory) &#123; this.memory = memory; &#125; public void run() &#123; System.out.println(&quot;开始运行计算机&quot;); // 开机先从硬盘加载数据 String data = hardDisk.get(); System.out.println(&quot;从硬盘上获取的数据是：&quot; + data); // 运行cpu cpu.run(); // 内存条保存 memory.save(); &#125;&#125; 123456789101112131415161718192021222324252627package com.cpu.principles.demo3.before;/*** @version v1.0* @ClassName: ComputerDemo* @Description: 测试类* @Author: humbleyl*/public class ComputerDemo &#123; public static void main(String[] args) &#123; // 创建组件对象 HardDisk hardDisk = new XiJieHardDisk(); Cpu cpu = new IntelCpu(); Memory memory = new KingstonMemory(); // 创建计算机对象 Computer c = new Computer(); // 组装计算机 c.setCpu(cpu); c.setHardDisk(hardDisk); c.setMemory(memory); // 运行计算机 c.run(); &#125;&#125; 3.4 接口隔离原则客户端不应该被迫依赖于它不使用的方法；一个类对另一个类的依赖应该建立在最小的接口上。 下面看一个例子来理解接口隔离原则 【例】安全门案例 我们需要创建一个Hi品牌的安全门，该安全门具有防火、防水、防盗的功能，可以将防火，防水，防盗功能提取成一个接口，形成一套规范。类图如下： 1234567891011121314151617181920package com.cup.principles.demo4.before;/*** @version v1.0* @ClassName: SafetyDoor* @Description: 安全门接口* @Author: humbleyl*/public interface SafetyDoor &#123; // 防盗 void antiTheft(); // 防火 void fireProof(); // 防水 void waterProof();&#125; 1234567891011121314151617181920212223package com.cup.principles.demo4.before;/*** @version v1.0* @ClassName: CupkSafetyDoor* @Description: Cupk品牌安全门* @Author: humbleyl*/public class CupkSafetyDoor implements SafetyDoor &#123; public void antiTheft() &#123; System.out.println(&quot;防盗&quot;); &#125; public void fireProof() &#123; System.out.println(&quot;防火&quot;); &#125; public void waterTheft() &#123; System.out.println(&quot;防水&quot;); &#125;&#125; 1234567891011121314151617package com.cup.principles.demo4.before;/*** @version v1.0* @ClassName: Client* @Description: 测试类* @Author: humbleyl*/public class Client &#123; public static void main(String[] args) &#123; CupkSafetyDoor door = new CupkSafetyDoor(); door.antiTheft(); door.fireProof(); door.waterProof(); &#125;&#125; 上面的设计我们发现了它存在的问题，黑马品牌的安全门具有防盗、防水、防火的功能。现在如果我们还需要再创建一个cupk品牌的安全门，而该安全门只具有防盗、防水功能呢？很显然如果是实现SafetyDoor接口就违背了接口隔离原则，那么我们如何进行修改呢？看下面类图： 123456789101112package com.cup.principles.demo4.after;/*** @version v1.0* @ClassName: AntiTheft* @Description: 防盗接口* @Author: humbleyl*/public interface AntiTheft &#123; void antiTheft();&#125; 123456789101112package com.cup.principles.demo4.after;/*** @version v1.0* @ClassName: FireProof* @Description: 防火接口* @Author: humbleyl*/public interface FireProof &#123; void fireProof();&#125; 123456789101112package com.cup.principles.demo4.after;/*** @version v1.0* @ClassName: WaterProof* @Description: 防水接口* @Author: humbleyl*/public interface WaterProof &#123; void waterProof();&#125; 12345678910111213141516171819202122package com.cup.principles.demo4.after;/*** @version v1.0* @ClassName: CupkSafetyDoor* @Description: Cupk品牌安全门* @Author: humbleyl*/public class CupkSafetyDoor implements AntiTheft, FireProof, WaterProof &#123; public void antiTheft() &#123; System.out.println(&quot;防盗&quot;); &#125; public void fireProof() &#123; System.out.println(&quot;防火&quot;); &#125; public void waterProof() &#123; System.out.println(&quot;防水&quot;); &#125;&#125; 1234567891011121314151617181920212223242526package com.cup.principles.demo4.after;/*** @version v1.0* @ClassName: Client* @Description: 测试类* @Author: humbleyl*/public class Client &#123; public static void main(String[] args) &#123; // 创建Cpuk品牌安全门 CupkSafetyDoor door = new CupkSafetyDoor(); // 调用功能 door.antiTheft(); door.fireProof(); door.waterProof(); System.out.println(&quot;==========&quot;); // 创建Cupker安全门对象 CupkerSafetyDoor door1 = new CupkerSafetyDoor(); // 调用功能 door1.antiTheft(); door1.fireProof(); &#125;&#125; 123456789101112131415161718package com.cup.principles.demo4.after;/*** @version v1.0* @ClassName: CupkerSafetyDoor* @Description: Cupker品牌安全门* @Author: humbleyl*/public class CupkerSafetyDoor implements AntiTheft, FireProof &#123; public void antiTheft() &#123; System.out.println(&quot;防盗&quot;); &#125; public void fireProof() &#123; System.out.println(&quot;防火&quot;); &#125;&#125; 3.5 迪米特法则迪米特法则又叫最少知识原则。 只和你的直接朋友交谈，不跟“陌生人”说话（Talk only to your immediate friends and not to strangers）。 其含义是：如果两个软件实体无需直接通信，那么就不应当发生直接的相互调用，可以通过第三方转发该调用。其目的是降低类之间的耦合度，提高模块的相对独立性。 迪米特法则中的“朋友”是指：当前对象本身、当前对象的成员对象、当前对象所创建的对象、当前对象的方法参数等，这些对象同当前对象存在关联、聚合或组合关系，可以直接访问这些对象的方法。 下面看一个例子来理解接口隔离原则 【例】明星与经纪人的关系实例 明星由于全身心投入艺术，所以许多日常事务由经纪人来负责处理，如和粉丝的见面会，和媒体公司的业务洽谈等。这里的经纪人是明星的朋友，而粉丝和米欸天公司是陌生人，所以适合使用迪米特法则。 类图如下：","tags":["设计模式"],"categories":["软件开发"]},{"title":"Java基础","path":"/2023/11/02/java-ji-chu/","content":"1.多行注释 单行注释：// 注释文字 多行注释： / 注释文字 / 细节：（1）被注释的文字，不会被JVM解释执行；（2）多行注释里面不允许多行嵌套注释 文档注释：注释内容可以被JDK提供的工具javadoc所解析，生成一套以网页文件形式体现的该程序的说明文档，一般写在类前面 1234567891011/*** @author 院龙* @version 1.0*/public class Comment02 &#123; // 编写一个main方法 public static void main(String[] args) &#123; &#125;&#125; 12// 生成javadoc文档javadoc -d 文件夹名 -xx -yy Demo3.java 2.Java代码规范1.类、方法的注释，要以javadoc方式来写 2.非Java Doc的注释，往往是给代码的维护者看的，着重告诉读者为什么这样写，如何修改，注意什么问题等 3.使用tab操作，实现缩进，默认整体向右边移动，用shift+tab整体向左移 4.运算符和 = 两边习惯性各加一个空格 5.源文件使用utf-8编码 6.行宽度不要超过80字符 7.代码编写次行和行尾风格 7. 面向对象编程（基础）7.1 类与对象123456789101112131415161718192021public class Object01 &#123; private String name; private int age; private String color; // 编写一个main方法 public static void mian(String[] args) &#123; /* 张老太养了两只猫猫：一只小白，今年3岁，白色 还有一只小花，今年100岁，花色。程序： 当用户输入小猫的名字时，就显示该猫的名字，年龄，颜色。 如果输入小猫名称有误，则显示 张老太没有这只猫 */ firstCat = new Object01(); secondCat = new Object02(); &#125;&#125; 类和对象的区别和联系 1.类是抽象的，概念的，代表一类事物，即它是数据类型 2.对象是具体的，实际的，代表一个具体事物，即它是实例 3.类是对象的模板，对象是类的一个个体，对应一个实例 对象在内存中的样子 注意事项 1.属性定义语法同变量：访问修饰符 属性类型 属性名 ​ 简而言之：控制属性的访问范围 ​ 有四种访问修饰符 public protected 默认 private 2.属性定义类型可以为任意类型，包含基本类型（int age）或引类型（String[] args） 3.属性如果不赋值，有默认值，规则和数组一致。 ​ int 0, short 0, byte 0, long 0, float 0.0, double 0.0, char \\u0000, boolean false, String null 如何创建对象 1.先声明再创建 12Cat cat; // 声明对象catcat = new Cat(); // 创建对象cat 2.直接创建 1Cat cat = new Cat(); 类和对象的内存分配机制 一个思考题，下面代码 12345Person p1 = new Person();p1.age = 10;p1.name = &quot;小明&quot;;Person p2 = p1;System.out.println(p2.age); Java内存结构分析 1.栈：一般存放基本数据类型（局部变量） 2.堆：存放对象（Cat cat, 数组等） 3.方法区：常量池（常量，比如字符串），类加载信息 4.示意图 [ Cat (name, age, price) ] 【例】 12345678910Person p = new Person();p.name = &quot;Jack&quot;;p.age = 10;/** 1. 先加载Person类信息（属性和方法信息，只会加载一次）* 2. 在堆中分配空间，进行默认初始化（有特定规则）* 3. 把地址赋给p, p就指向对象* 4. 进行指定初始化，比如p.name = &quot;Jack&quot;; p.age = 10;*/ 【例】 7.2 overload7.3 可变参数7.4 作用域7.5 this","tags":["Java基础"],"categories":["软件开发"]},{"title":"情境学习创建任务向量","path":"/2023/11/02/qing-jing-xue-xi-chuang-jian-ren-wu-xiang-liang/","content":"情境学习创建任务向量Abstract大型语言模型 (LLM) 中的上下文学习 (ICL) 已成为一种强大的新学习范式。然而，其基本机制仍不清楚。特别是，将其映射到“标准”机器学习框架具有挑战性，在该框架中，人们使用训练集 S 在某个假设类中找到最佳拟合函数 f (x)。在这里，我们在这个问题上取得了进展，表明 ICL 学习的函数通常具有非常简单的结构：它们对应于变压器 LLM，其唯一输入是查询 x 和从训练集计算出的单个“任务向量”。因此，ICL 可以看作是将 S 压缩为单个任务向量 θ(S)，然后使用该任务向量来调制变压器以产生输出。我们通过一系列模型和任务的综合实验来支持上述主张。 1 Introduction大型语言模型在过去几年中得到了显着改进。这些模型的一个显着特性是它们可以从很少的演示中学习新规则。例如，可以使用输入“Apple → Red、Lime → Green、Corn →”提示模型并生成输出“Yellow”。因此，该模型仅基于两个示例就学习了映射，并且可以将其正确应用于新示例。这种能力被称为 InContext Learning (ICL)，已被广泛使用，产生了令人印象深刻的实证结果（Brown 等人，2020；Liu 等人，2023；Dong 等人，2022）。 ​ 鉴于这一成功，人们自然会问 ICL 背后的根本机制是什么。即，模型内部如何使用演示 S 和查询 x 来产生所需的输出？在这里，我们利用统计学习理论中的假设类概念（Shalev-Shwartz 和 Ben-David，2014）。在学习理论公式中，人们通常考虑假设类 H，其中 H 的每个元素都是函数 h(x; θ)，对输入 x 进行运算，并由参数向量 θ 指定。例如，如果 x ∈ Rd，则类 H 可以是线性分类器的集合，由系数向量 θ 定义为 h(x; θ) = θ · x。学习算法寻找一个能很好地拟合训练集的元素 h ∈ H。这称为经验风险最小化。 ​ 目前尚不清楚 ICL 是否以这种方式运行， 因为预测是通过 T ([S, x]) 执行的，其中 T 通常是自回归变压器和[S, x] 是 S 和 x 中标记的串联 *。因此，在一般情况下，它可以是对 S 和 x 进行运算以产生输出的任意函数**。这可以包括“非参数”方法，例如最近邻法。最近的工作已经开始探索这个问题。例如，研究表明，当从头开始训练 Transformer 以在上下文中执行线性回归时，新兴的学习算法类似于随机梯度下降（Akyürek 等人，2022 年；von Oswald 等人，2022 年）。然而，对于执行更复杂的自然语言任务的法LLMs来说，根本不清楚假设空间可能是什么。 ​ 在这项工作中，我们表明，在广泛的任务中，LLMs中的 ICL 可以被视为在非常自然的假设空间上工作。我们认为，给定训练集 S，变压器将其映射到“任务向量”θ(S)，该向量本质上表示 S.2 中描述的映射/规则。即，给定变压器 T 和向量 θ，我们可以构造实现该任务的新函数 f (x; θ)。函数 f 与应用于 x 的原始变换器非常相似，但没有演示，而是通过 θ 进行调制（见图 2）。 ​ 我们的观点也与软提示有关（Lester et al., 2021），因为这两种方法都会针对特定任务调节变压器的功能。然而，在 ICL 中，任务向量是在前向传递中计算的，而不是进行微调的。 ​ 我们的贡献包括提出基于假设类的 ICL 机械观点，并进行实验来验证我们对一系列公开可用的LLMs和各种任务的观点。我们的结果进一步加深了对 ICL 的理解，并可能对LLMs有效适应执行特定任务具有实际意义。 2 A Hypothesis Class View of ICL受学习理论的假设类视图的启发，我们的目标是了解 ICL 是否将演示集 S 映射到查询 x 上的函数以及这种映射是如何发生的。具体来说，我们试图看看 ICL 是否将 S 转换为 θ——某个假设空间内函数的“参数”。我们的实证研究结果表明这种观点是适用的，揭示了 ICL 运行的假设空间的结构。 2.1Theoretical Framework我们使用 T 表示仅解码器变压器 LLM，S 表示用作 ICL 输入的演示集（即训练示例），x 表示要求 ICL 提供输出的查询。我们使用 T ([S, x]) 表示 ICL 对 S 和 x 串联的输出。 ​ 为了证明 ICL 在假设空间内运行，我们的目标是证明其基本机制可以分为两部分： 一种“学习算法”（用 A 表示），将 S 映射到“任务向量”θ，独立于查询 x。鉴于注意力层可以访问 S 和 x，这种独立性并非微不足道。 一个“规则应用”（用f表示），它基于θ ≡ A(S) 将查询x 映射到输出，而不直接依赖于S。同样，这种独立性并非微不足道。 因此，我们考虑以下从一组演示和查询到预测输出的映射：T ([S, x]) = f (x; A(S))。如果我们可以将 LLM 的前向传递分解为上述两个部分，我们可以将 ICL 视为在以下假设类上运行： H = {f (·; θ) | θ}。在下一节中，我们提出这样一个类的实现。 2.2 A proposed Hypothesis Class上述框架有多种可能的实现，对应于 A 和 f 的不同选择。接下来我们描述我们关注的实现，这自然是从变压器架构中得出的。我们考虑如图 1 所示的 ICL 设置，其中输入以查询 x（即 Corn）结尾，后跟“→”符号。如上所述，我们将学习视为由两个步骤组成：根据训练样本 S 计算参数向量 θ，并将该参数向量定义的规则应用于查询 x。变压器执行此操作的一个简单方法可能是让 → 表示的前 L 层计算 θ，然后让其余层将 θ 和 x 作为输入并产生输出。参见图 1。回想一下，变压器在任何层都可以访问 S 和 x，这对我们的观点提出了挑战。 ​ 在以下部分中，我们将解决这一挑战并提出验证我们观点的实验。也就是说，我们证明我们可以在执行 ICL 的 LLM 的前向传播中分离出我们提出的 A 和 f。我们还表明 θ 向量是可解释的并且对应于学习任务。 3 Validity of the Hypothesis Class View我们首先证明，将前向传播分成两个不同的分量 A 和 f（在第 2.2 节中定义）可以保持 ICL 的高精度。 3.1 Separating A and f我们在常规前向传递中面临一些挑战：首先，对应于 A 的初始 L 层，更新 → 的表示以创建 θ，可以处理查询 x。因此，它们可能依赖于 x，从而产生 θ 对 x 的不必要的依赖。其次，与 f 相对应的其余层可以直接访问 S，而不是仅使用 x 和 θ。 ​ 我们提出以下过程来解决这些挑战：为了解决第一个问题，我们引入一个“虚拟查询”x′并使用该查询计算→的表示。我们使用前 L 层之后的 → 表示（使用 x′ 计算）作为向量 θ（如图 2 左侧所示）。另一种方法是阻止对 x 的关注，但这会导致性能不佳。为了解决在不允许直接依赖于 S 的情况下计算 f (x, θ) 的第二个问题，我们仅对 x 和 →,3 执行变换器的前向传递，并对我们之前在第 L 层提取的 θ 进行“修补”。 →（图2右侧）.4 3.2 8 Conclusions通过LLMs对 ICL 的探索，我们揭示了 ICL 学习机制的新视角。我们揭示了一个简单而优雅的结构：ICL 通过将给定的训练集压缩为单个任务向量来发挥作用，然后引导变压器根据给定的查询生成适当的输出。我们的工作为了解LLMs如何进行 ICL 奠定了基础。根据我们的发现，未来的工作可以集中于理解任务向量是如何构建的以及如何使用它来计算输出。","tags":["EMNLP2023"],"categories":["NLP顶会"]},{"title":"多语言模型中事实知识的跨语言一致性","path":"/2023/11/02/duo-yu-yan-mo-xing-zhong-shi-shi-zhi-shi-de-kua-yu-yan-yi-zhi-xing/","content":"多语言模型中事实知识的跨语言一致性Abstract多语言大规模预训练语言模型（PLM）已被证明可以存储大量的事实知识，但观察到语言之间存在很大差异。为了确保具有不同语言背景的用户从同一模型获得一致的反馈，我们研究了各种多语言PLM中事实知识的跨语言一致性（CLC）。为此，我们提出了一个基于排名的一致性（RankC）指标，以独立于准确性来评估跨语言的知识一致性。使用这个指标，我们在模型级别和语言对级别对CLC的决定因素进行了深入分析。在其他结果中，我们发现增加模型大小会导致大多数语言中更高的事实探测准确性，但不会提高跨语言的一致性。最后，我们进行了关于CLC的案例研究，当通过模型编辑在PLM中插入新的事实关联时。英语插入的一小部分事实样本的结果揭示了一个清晰的模式，即新知识仅转移到英语具有高 RankC 分数的语言。 Introduction 大规模预训练语言模型 （PLM） 在事实知识发挥重要作用的任务中展示了强大的能力（Roberts 等人，2020 年;秦等人，2022 年）。虽然以前大多数关于探索 PLM 中事实知识的工作都集中在英语上（Davison 等人，2019 年;布拉维等人，2020 年;申等人，2020;布朗等人，2020 年;阿尔甘米等人，2021 年;Peng 等人，2022 年），一些值得注意的研究已将评估扩展到许多其他语言（Jiang 等人，2020 年;卡斯纳等人，2021 年;尹等人，2022 年）。这些研究结果表明事实知识在多大程度上跨语言泛化，揭示了现代 NLP 技术中语言不平等的另一个方面（Hupkes 等人，2022 年）。 然而，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;评估跨语言的事实知识并非易事。确保结果的可比性要求以所有语言查询一组“普遍”事实，但该集合的选择可能偏向于在维基数据等流行知识库中代表性更高的特定世界区域.2相反，在世界其他地区更相关的事实（例如， 关于某一特定区域的地点或重要人物的信息）不太可能出现在基准中，这使得难以解释这种评估的结果&lt;/span&gt;。 在这项工作中，我们采取了不同的立场：我们没有衡量PLM在每种语言中编码的事实知识量，而是关注其跨语言的一致性。如图 1 所示，多语言 BLOOM-3b 模型当以英语、西班牙语和越南语查询时，输出始终正确完成第一个提示，但不是匈牙利语和希腊语。该模型还以英语、西班牙语和越南语（但不是匈牙利语和希腊语）对第二个查询输出一致但错误的答案，这表明前三种语言在模型中共享相关的知识表示。 跨语言一致性 （CLC） 的研究很重要，至少有两个原因：首先，对事实的真正了解意味着无论给定的表面形式如何，都要对其含义进行编码（Ohmer 等人，2023 年）。因此，如果模型知道北京市是中国的首都，那么当用不同的语言询问相同的问题时，它应该返回相同的答案。从实际的角度来看，CLC 对于确保用户在不同语言与同一模型交互时具有相似的体验至关重要。其次，研究CLC对于了解在多语言PLM中以一种语言获得的知识是否以及如何隐含地转移到另一种语言非常重要。除了科学相关性外，这对将外部知识纳入多语言PLM具有实际意义。事实上，虽然多产的工作线侧重于模型编辑，作为以各种数据和计算效率的方式在 PLM 中插入新事实关联的一种方式（De Cao 等人，2021 年;侯等人，2022;Meng 等人，2022 年），据我们所知，还没有人研究过这如何影响直接应用编辑的语言以外的语言中的事实知识。 我们对多语言PLM中的事实知识CLC进行了首次深入研究，并做出了以下贡献：（i）我们提出了一种新的基于排名的一致性（RankC）指标，该指标独立于准确性评估知识的一致性。（ii） 我们过滤现有的不平衡数据集（Jiang 等人，2020 年;Kassner 等人，2021 年）形成多并行 CLC 基准，平衡多语言模型分析 （BMLAMA），该基准将相同的一组提示翻译成所有语言。（iii）我们将新指标应用于BMLAMA，以评估各种仅编码器，仅解码器和编码器解码器PLM中的CLC，包括XLM-RoBERTa-large，mT5-large和BLOOM系列。我们分析了许多与CLC相关的语言属性，并为事实知识如何在语言之间渗透提供了新的见解。最后（iv）我们使用基于神经元可解释性的最先进的模型编辑技术（Meng 等人，2022 年）提供案例研究，提供初步证据，证明 CLC 可以预测插入语言 X 的事实是否会转移到语言 Y 中。 2.Related Work探索 PLM 中的事实知识 自 LAMA 首次提出以来（Petroni 等人，2019 年），基于提示的探测已成为评估 PLM 中事实知识的主要技术（Davison 等人，2019 年;布拉维等人，2020 年;申等人，2020;布朗等人，2020 年;阿尔甘米等人，2021 年;彭等人，2022 年）。给定元组（主体、关系、对象）中表示的知识，通过将主题填充到特定于关系的模板中来形成查询 q，该模板被馈送到 PLM 中。如果预测与对象一致，则认为模型具有此知识。例如，给定一组候选城市名称，当查询“中华人民共和国的首都是_”时，如果PLM在所有候选城市中正确答案“北京”的概率最高，则认为PLM捕获了这条知识。 事实知识的多语言探索 除了大量关注英语的著作外，一些著名的研究通过将英语提示-对象对翻译成多种语言来多语言探索事实知识。X-FACTR（Jiang 等人，2020 年）和 MLAMA（Kassner 等人，2021 年）表明，由于其培训语料库的大小，不同语言的知识量之间存在很大差异。除了英语和少数其他高资源欧洲语言外，总体上报告的探测准确性非常低（即&lt;10%）。另一项相关工作， GeoMLAMA（Yin 等人，2022 年）专门探测了在不同地区可能有所不同的常识性知识，导致相当令人惊讶的发现，即探索某个国家（例如中国）知识的最佳语言通常不是给定国家的母语（例如中文）。所有这些研究的主要重点是评估每种语言编码的事实知识的数量，而不是了解这些知识如何在语言之间渗透。 自洽性 自洽性是指 PLM 对同一查询的保留含义的释义输出相同答案的能力。英语PLM的自洽性在不同任务中都受到了关注（Li等人，2019;米切尔等人，2022 年;王等人，2023 年）。Fierro和Søgaard（2022）通过将自洽性的研究扩展到多语言PLM，方法是在每种语言中单独测量自洽性。他们的结果显示，所有语言的自洽性都很差。 跨语言一致性 据我们所知，我们是第一个对多语言PLM中事实知识的跨语言一致性进行系统分析的公司，即PLM对不同语言提出的相同问题返回相同答案的程度。作为探索研究的一部分，Jiang等人（2020）计算了mBERT中两种语言之间重叠的正确预测的比例（参见第3.1节）。他们报告的总体比率较低，在最相似的对（英语 - 荷兰语）中只有34%的峰值，但没有进一步调查决定一致性的因素。此外，他们将这种分析限制在一个（仅编码器）模型，同时我们还检查了编码器-解码器和一系列仅解码器模型（参见第5.1节）。另一个区别是，我们对一致性采取了更全面的观点，即不正确但跨语言引用同一实体的预测也应被视为一致。&lt;/span&gt;有趣的是，Ohmer 等人（2023 年）的并行工作建议使用模型预测的跨语言一致性作为评估其对特定单词形式之外的含义的理解的一种手段。他们在两个语言理解任务（释义识别和自然语言推理）中展示了他们的方法。尽管范围不同，但他们使用英语、德语和中文翻译对 ChatGPT 的评估表明，模型响应的一致性有限，这与我们的事实调查结果一致（参见第 5 节），并进一步表明这个问题在非常大规模的上一代 PLM 中仍然存在 3.Measuring Cross-Lingual Consistentcy任务定义 每种语言l ∈ L有一组定义为 Ql 的查询（即提示）。对于每个查询 qi ∈ Ql，有Ni对应候选项，例如，查询“史蒂夫乔布斯为 __ 工作”有 10 个候选者：苹果、任天堂、谷歌、WWE、亚历山大、德国、雅虎、柏林、BBC、Microsoft。每个查询都会馈送到 PLM，返回的概率用于计算每个候选单词的排名分数。分数计算取决于模型的类型（仅编码器、编码器解码器或仅解码器）以及候选单词分割为子单词的方式（请参阅附录 B 中的详细信息）。按排名分数排序后，Qi 的候选集表示为 {ci1， . . . ， cNi i }，其中 ci1 的预测概率最高，cNi i 的预测概率最低。请注意，现有的用于知识探测的多语言数据集（X-FACTR（Jiang 等人，2020 年）和 MLAMA（Kassner 等人，2021 年））在不同语言中具有不同数量的查询，这对于衡量一致性是有问题的。 3.1Prioions Work:Correct Predictions Overlap 基于每个 qi 和 q′ i 的预测 ci1 和 c′1 i（即排序候选列表的第一个元素），Jiang 等人 （2020） 计算正确预测的平均重叠率如下： 其中 1（·) 是指示函数，oi 和 o′i 分别是 qi 和 q′ i 的正确答案。 由于他们的基准测试包含不同语言的不同数量的查询，因此它们通过丢弃 l 或 l′ 中不可用的样本来过滤每个语言对 （l， l′） 的查询集： 由于筛选是分别对每个语言对完成的，因此这会导致不同的查询集，这限制了它们的结果在具有非常不同的筛选集的语言对之间的可比性。 3.2This Work:RankC Metric为了确保不同语言对之间的可比性，我们要求基准测试中的所有查询及其相应的候选查询都翻译成所有语言。因此，对于任何语言对 （l， l′），查询集的长度始终相等 |Ql|= |Ql′|，第 i 个查询 Ni = N ′ i 的候选项数也是如此。基于这些假设，我们提出了一种新的基于排名的一致性（RankC）指标，以有效地评估PLM中知识的跨语言一致性，而与准确性无关。我们不只是关注正确的预测，而是将所有候选的排名纳入考虑。RankC的灵感来自信息检索的K（MAP\\@K）指标的平均平均精度（Schutze等人，2008）。与原版MAP\\@K不同，在 RankC K 中因查询而异。qi 的值 K 等于 Ni，即其候选者的数量。给定语言 l 和 l′，两种语言之间的一致性分数定义为所有翻译查询对 （qi， q′ i） ∈ （Ql， Ql′） 的一致性平均值： 每个查询对的一致性是通过加权平均 P @j 函数计算的，该函数输出具有前 j 个最高概率的候选函数之间的重叠比率3： 每个 P @j的权重 wj 定义如下。 基于排名的权重 直观地说，排名较高的候选人应该对一致性分数产生更大的影响。为了实现这一目标，RankC 对所有 P @js采用加权平均值，其中 j 较小的 P @j被赋予较高的权重 wj，以强调具有高概率的候选人的影响。但是，预测概率不能直接使用，因为它们对于 qi 和 q′ i 的候选者是不同的。为了解决这个问题，我们引入了基于softmax的归一化权重，而不是值j： 其中 Ni 是查询 qi 和 q′ i.4 的候选数量 结合等式 3、4 和 5，RankC 指标变为： 附录D给出了RankC计算示例，以及高/低RankC的解释 我们在同一数据集上对RankC与以前使用的指标（COverlap，参见公式1)进行了实证比较。附录F中的结果表明，&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;几乎所有具有高COVERLAP分数的语言对也获得了较高的RankC分数。此外，RankC揭示了一些新的高一致性对，由于探测精度低，它们的COverlap评分较低。&lt;/span&gt; 4.Experimental Setup数据集 如第 3.2 节所述，RankC 要求将查询及其候选语言翻译成所有评估语言。因此，我们从 X-FACTR（Jiang 等人，2020 年）和 MLAMA（Kassner 等人，2021 年）中提取所有满足此标准的查询。我们将生成的多并行数据集称为平衡多语言模型分析（BMLAMA），并以两个版本发布：BMLAMA-17，包括17种语言的6.7k查询（接近X-FACTR，包括23种语言），BMLAMA-53包括53种语言的3k查询（与MLAMA相同）。详细统计数据如表1所示。 模型 多语言知识探索的先前工作（Jiang等人，2020;Kassner 等人，2021 年）专注于仅编码器的 PLM，例如 mBERT（Devlin 等人，2019 年）或 XLM-RoBERTa（Liu 等人，2019 年）。然而，由于纯解码器 PLM 已成为当前 NLP 时代的主流，我们的实验还包括仅解码器的 BLOOM 系列（560m、1.1b、1.7b、3b 参数）（Scao 等人，2022 年）和编码器-解码器 mT5large （1.2b）（Xue 等人，2021 年），此外还包括仅编码器的 XLM-RoBERTa-large（354m）。 5.Main Consistency Result在查看一致性之前，我们在图 2 中展示了 BMLAMA-17.5 上三个 PLM 的实际探测精度结果，我们首先注意到，仅编码器 XLM-RoBERTa-large 和编码器解码器 mT5-large 模型在平均探测精度方面优于整个仅解码器的 BLOOM 系列。三种型号的跨语言趋势相似，但是，BLOOM以远高于所有其他语言的英语准确性脱颖而出。关于模型大小（BLOOM 系列，绿条），我们发现增加参数数量会导致事实探测精度的轻微但一致的提高，这与以前的工作一致（Petroni 等人，2019 年）。 我们的XLM-RoBERTa-large结果与Jiang等人（2020）在XFACTR上报告的结果一致，证明了我们的多并行数据集BMLAMA的可靠性。 5.1Consistency in Different PLMs图 3 显示了三种 PLM 的 RankC 结果。第一个观察结果是，所有模型的平均一致性6都相低，BLOOM3b（25%）最低。这一阴性结果与Jiang等人（2020）在mBERT上观察到的正确预测的低重叠率一致。 &lt;span style=&quot;background-color: #ff666680&quot;&gt;我们现在放大了不同语言对之间的比较，这是通过新的RankC指标和平衡数据集BMLAMA实现的。在这里，我们发现欧洲语言英语，法语，荷兰语，西班牙语和加泰罗尼亚语在mT5-large和XLM-RoBERTa-large方面共享了相当多的知识。类似的模式适用于BLOOM-3b，但荷兰语除外，这是意料之中的，因为该语言未包含在此模型的训练语料库中。此外，越南语和土耳其语在所有PLM中都与上述欧洲语言实现了显着的一致性。这些语言的一个共同特点是它们都使用相同的脚本（拉丁语）。另一个值得注意的高一致性对是俄语和乌克兰语，使用相同脚本（西里尔文）并且也密切相关的两种语言。这些观察表明，各种语言属性会影响多语言知识的CLC。我们将在第 6.1 节中检查许多此类属性。&lt;/span&gt; 5.2Effect of Model Size如上所述（图 2 中的绿条）和之前的工作（Petroni 等人，2019 年）所观察到的，当其他因素固定时，检索正确知识的能力会随着模型大小的增长而增长。我们问CLC是否也是如此。然而，图4中的BLOOM结果显示，从我们系列中最小的模型移动到最大的模型时，平均RankC（+2%）只有很小的变化，即参数增加了5倍。虽然这种模式不能安全地推广到其他模型，但它确实表明，在非常大规模的PLM中，跨语言一致性可能仍然是一个问题8。 6.Typological Similarity类型学特征已被证明可用于模拟语言之间的细粒度相似性，并指导各种多语言 NLP 任务的迁移学习技术（Ponti 等人，2019 年;尤斯图恩等人，2022 年）。这些特征是否也能解释在多语言PLM中观察到的事实知识一致性的一些差异？例如，我们可能期望具有相似语法和词序或具有相关词汇的语言共享更高的语言程度。在多语言模型中。我们可能还期望在同一世界地区使用的语言更有可能在训练数据中遇到相同实体和事件的提及。 为了回答这个问题，我们从lang2vec（Littell等人，2017）获得了四种类型的&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;类型相似性（句法，遗传，地理和语音）&lt;/span&gt;，这是一个开源库，提供基于各种类型学数据库的预先计算的相似性.9接下来，我们计算RankC分数与BMLAMA中所有语言对的类型相似性之间的皮尔逊相关系数（Cohen等人，2009）。 表2显示了BMLAMA-17和较小但多语言的BMLAMA-53.10的结果 对于BMLAMA-17，&lt;span style=&quot;background-color: #ff666680&quot;&gt;我们发现RankC与遗传相似性具有中等相关性，与地理相似性具有弱相关性，但与句法相似性没有显着相关性。正如预期的那样，没有观察到与语音相似性的相关性。更全面的数据集BMLAMA-53上的相关性结果相似，除了句法相似性获得弱正相关。&lt;/span&gt;有点令人惊讶的是，在这个更大的数据集中，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;遗传和地理上的相似性使它们的相关性略有下降，这可能是由于低资源语言的类型向量中存在噪声。&lt;/span&gt; 遗传相关语言的一个重要特征是它们往往有很多单词共同或具有共同祖先。&lt;span style=&quot;background-color: #ff666680&quot;&gt;因此，RankC与遗传相似性的中等相关性，加上与句法和地理相似性的弱相关性，表明词汇重叠可能是CLC比具有相似的语法和词序或在附近地区使用更重要的因素。&lt;/span&gt; 6.2Subword Vocabulary Overlap基于上述观察结果，我们研究了词汇重叠的粗略测量是否也可以很好地预测CLC。具体来说，我们提取了我们评估语言中严格平行语料库的词汇表，并测量它们的成对重叠： 我们考虑两个语料库：BMLAMA 本身和 Flores-200（Costa-jussà 等人，2022 年)。前者预计非常相关，但由此产生的相关性可能不太可推广，因为它是衡量一致性本身的同一语料库。相比之下，后者是一组混合域的 2k 个句子，从英语翻译成 200 种语言，用于机器翻译评估。因为我们对不同语言使用完全相同的单词表示的程度感兴趣，所以我们在测量词汇重叠之前用模型的分词器对语料库进行分割，这使得这个指标模型依赖于。 如表2（右）所示，BMLAMA上的皮尔逊相关分数证明，&lt;span style=&quot;background-color: #ff666680&quot;&gt;子词词汇重叠对PLM中知识的跨语言一致性有显著的强烈影响，掩盖了遗传的影响&lt;/span&gt; 相似。这表明事实知识可能主要以相当肤浅的方式（通过共享使用一些子词嵌入）渗透到语言之间，相反，即使语言相关，在没有这种锚点的情况下，它也可能受到阻碍。例如，BLOOM-3b中一致性最高的对是乌克兰语-俄语，它们位于语言树中（遗传相似性：0.8），并且总体上共享大量子词词汇（词汇重叠：0.76）。然而，在查询大卫·卡梅伦的工作地点时，BLOOM-3b预测的是俄语查询（“伦敦”）中的正确答案，但乌克兰语（“莫斯科”）中的错误答案。这表明正确的知识没有从俄语转移到乌克兰语，因为这两个查询之间的子词重叠有限（0.17）。当在Flores上测量词汇重叠时（表2的最后一列），相关性较低，但仍然显着为正，表明我们的发现不仅限于我们的基准。跨语言知识一致性与词汇重叠之间的相关性如图5所示。CLC对浅词汇重叠的强烈依赖部分解释了为什么增加模型大小没有积极的影响（参见第5.2节)。我们推测，较大的子单词词汇实际上可能导致较低的一致性，因为在任何两种语言之间共享部分单词的机会会降低。我们将对这一假设的进一步调查留给未来的工作。 7.Case Study: Cross-Lingual Consistency and Knowledge Incorporation之前的工作（Jiang et al., 2020；Kassner et al., 2021；Artetxe et al., 2022）和我们的探索结果表明，低资源语言的知识量是有限的。简单地在更大的非英语语料库上训练新的 PLM 非常耗时，而且大多数大学和其他研究机构都无法承担其成本（Ding 等人，2022）。一个有前景的解决方案是通过微调方法整合外部知识（Hu et al., 2022）或以非常有针对性的方式直接编辑 PLM 的权重（De Cao et al., 2021；Meng et al., 2022）。为了使该过程在多语言场景中可行并避免意外影响，重要的是要了解以一种语言插入知识是否以及如何影响 PLM 中的其他语言，包括最易受影响和最不易受影响的语言。在本节中，我们将针对这个问题及其与 CLC 的相互作用进行第一个案例研究。 Rank-One 模型编辑（ROME）由Meng 等人提出。 (2022)，这种基于神经元可解释性的最先进的模型编辑技术在特异性和泛化方面都优于其他几种编辑技术。简而言之，该技术直接修改 PLM 早期前馈层中的权重，其中事实关联已通过因果干预找到。 反事实知识 遵循孟等人。 （2022），我们考虑将反事实知识插入 PLM 的任务，例如事实上错误的“史蒂夫·乔布斯曾为微软工作”。由于在预训练期间从未观察到此类事实关联，因此这种方法避免了插入模型已认为可能的事实的风险。 案例研究 我们研究了 BLOOM-3b，因为 ROME 目前仅适用于仅解码器模型。选择英语作为插入事实的源语言。作为目标语言，我们选择两种与英语具有高度一致性（RankC）的语言（西班牙语和越南语）和两种RankC 较低（匈牙利语和希腊语）。这些语言在脚本和与英语的相关性方面也各不相同。通过确保 PLM 在编辑之前选择最有可能的最初正确答案来挑选六个查询。我们还确保，对于每个编辑的知识，主题和客体实体在所有语言中都是相同的标记。这消除了这样的担忧：例如，西班牙语和越南语仅仅因为所评估的查询中主语和宾语标记的词汇共现而获得与英语一致的预测。对于评估，我们遵循孟等人的设置。 （2022）并将候选集缩小为两个单词——一个正确，一个错误。后者是ROME的编辑目标。根据每个查询，PLM 计算正确和错误答案的 logit 值，分别为 logitC 和 logitW。这些 logits 在不同语言之间差异很大。为了关注原始事实和编辑事实之间的关系，我们按照之前的工作（Sarti et al., 2023）将 logits 标准化为 表 3 显示了三个查询的结果。一个非常清晰的模式出现了：&lt;span style=&quot;background-color: #ff666680&quot;&gt;当一个事实被插入到英语中时，它会一致地传播到高 CLC 语言（即西班牙语和越南语）。相反，低 CLC 语言（匈牙利语和希腊语)受到的影响要小得多，即使在模型编辑后，仍然会输出更高的正确答案概率。&lt;/span&gt;附录 J 中给出的其余三个查询显示了相同的模式。 尽管我们的研究规模较小，但结果表明，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;CLC 不仅是 PLM 中现有知识的副产品，而且还代表了在将新知识融入其他语言时对语言扰动的敏感性。&lt;/span&gt;我们认为这是增强多语言场景中模型编辑优势的一个有前途的方向。 8.Conclusion我们分析了多语言大型 PLM 中事实知识的跨语言一致性 (CLC)。我们提出了一个新的指标 RankC，用于独立于准确性来量化一致性，并将其应用于跨语言平衡的事实知识基准。我们的综合分析表明，(i) 不同 PLM 的平均 CLC 较低，并且不受模型大小的明显影响； (ii) PLM 内不同语言对的 CLC 与遗传相似性显着相关，但与词汇重叠的相关性明显更强； (iii) 通过模型编辑插入到语言 X 中的新事实更有可能传播到具有 X 的 CLC 分数较高的语言。 Limitations由于 GPU 资源的限制，我们无法测试大于 BLOOM-7.1b 的模型。鼓励在未来的工作中将我们的分析扩展到更大规模的模型，看看是否得出相同的结论。然而，图4的结果表明，随着模型规模的增加，平均CLC增长极其缓慢。 BMLAMA 中包含的事实虽然被认为具有普遍性，但可能与西方世界更相关，这可能会在评估中引入偏见。我们从 BMLAMA 所建立的基准中继承了这个问题。解决这个问题并非易事，特别是在比较工作中，需要探究跨语言的确切事实集，并且应该在未来的工作中予以关注。","tags":["EMNLP2023"],"categories":["NLP顶会"]},{"title":"我们可以通过情景学习来编辑事实知识吗？","path":"/2023/11/02/wo-men-ke-yi-tong-guo-qing-jing-xue-xi-lai-bian-ji-shi-shi-zhi-shi-ma/","content":"我们可以通过情景学习来编辑事实知识吗？Abstract之前的研究表明，像 GPT 这样的大型语言模型 (LLM) 在其参数中存储了大量事实知识。然而，存储的知识可能是错误的或过时的。传统的知识编辑方法通过对包含特定知识的文本进行微调来完善LLMs。然而，随着LLMs规模的不断扩大，这些基于梯度的方法带来了巨大的计算成本。模型即服务的趋势也使得修改黑盒 LM 中的知识变得不可能。受到上下文学习（ICL）这种基于演示上下文而无需参数更新的新范式的启发，我们探索 ICL 是否可以编辑事实知识。为了回答这个问题，我们对 ICL 策略进行了全面的实证研究。实验表明，与 GPT-J (6B) 上基于梯度的方法相比，上下文知识编辑 (IKE) 在没有任何梯度和参数更新的情况下实现了有竞争力的成功率，但副作用要少得多，包括减少对相似但不相关事实的过度编辑以及更少的对先前存储的知识的遗忘。我们还将该方法应用于具有数十或数百个参数的大型 LM，例如 OPT-175B，这显示了我们方法的可扩展性。该代码可在“https:// github.com/PKUnlp-icler/IKE.” 1 Introduction预训练语言模型 (LM) 为 NLP 研究树立了新范式，并席卷了所有现有的 NLP 基准。由于取得了令人鼓舞的成果，研究人员为 LM 赋予了满足现实世界需求的新技能，例如使用网络浏览器（Nakano 等人，2021）、编码（Chen 等人，2021）、玩策略游戏（FAIR 等人） al.，2022）和对话人才（OpenAI，2022、2023）。然而，语言模型的广泛应用也引发了人们对其生成虚假内容的陷阱的日益关注（Elazar et al., 2021；Cao 等人，2021a）、过时（Dhingra 等人，2022）、有偏见（Sheng 等人，2019；Zhao 等人，2021）和攻击性（Gehman 等人，2020）。为了缓解这一缺陷，旨在修改语言学习者所学到的知识的知识编辑（图 1）引起了越来越多的关注（Mitchell 等人，2022a；Meng 等人，2022a)。知识编辑的目标有两个：概括性和特异性。前者需要泛化到描述相同知识的各种提示，后者则不需要干扰其他不相关的知识。 以往的知识编辑方法主要采用&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;基于梯度的方法来修改特定的模型参数以获得所需的模型行为&lt;/span&gt;（Mitchell等，2021；Meng等，2022a），例如在选举后更新总统。然而，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;目标知识神经元的识别通常需要计算开销很大的梯度估计&lt;/span&gt;（Dai et al., 2022）。此外，&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;更新的参数本身会导致超出所需版本的副作用，例如忘记以前学到的事实或对不相关事实进行过度编辑&lt;/span&gt;。先前的研究表明，当大规模 LM (LLM) 作为黑盒服务部署时（Sun 等人，2022），&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;对其参数的微小修改可能会极大地影响其最终用户的行为&lt;/span&gt;。因此，传统方法仍然受到编辑 LLM arXiv:2305.12740v1 \\[cs.CL] 202 年 5 月 22 日的困扰，因为这些限制阻碍了可扩展性和通用性。 最近，情境学习（ICL）（Brown et al., 2020）已成为指导LLMs执行复杂任务的新范式。在 &lt;span style=&quot;background-color: #2ea8e580&quot;&gt;ICL 中，任务描述和演示示例以自然语言表示以形成上下文，并且以上下文为条件的 LM 预测根据预定义规则转换为答案&lt;/span&gt;（Brown 等人，2020）。通过这种方式，大型 LM 无需对参数进行任何修改即可适应各种下游任务，使其自然适合大型 LM 上的知识编辑。首先，它通过避免修改参数来减少计算开销，并消除参数更新带来的副作用的风险。最重要的是，&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;ICL 为人类提供了一种可解释的方式来校准 LM 行为&lt;/span&gt;。尽管有这些优点，ICL 是否适用于知识编辑仍不清楚。 在本文中，我们研究了 ICL 为LLMs进行知识编辑的潜力。我们专注于两个目标：（1）确保泛化，以便大型语言模型可以泛化到多个文本表面以获取更新的知识；（2）通过对目标知识事实进行准确修改，同时保留其他不相关事实，确保特异性。为了同时实现这些目标，我们设计了演示格式和组织策略，以构建合适的上下文学习演示，以指导LLMs的知识编辑。我们定义了三种类型的演示格式化模板，包括&lt;span style=&quot;background-color: #ff666680&quot;&gt;（i）复制，旨在将新事实注入语言模型； (ii) 更新，提高注入知识事实的泛化能力； (iii) 保留，指导语言模型保留不相关的知识事实。此外，为了充分利用 ICL 进行知识编辑的潜力，我们从训练语料库中检索相关知识事实作为演示输入。&lt;/span&gt; GPT-J（6B）知识编辑基准的实验结果表明，所提出的上下文学习知识编辑（IKE）在强基线下实现了整体可比的知识编辑性能。例如，IKE 的编辑成功率绝对优于 MEND（Mitchell 等人，2021）10％，并且在特异性方面比 ROME 获得了 30 分的增益（Meng 等人，2022a）。由于没有参数修改，IKE适用于OPT-175B等LLM，并表现出更好的记忆能力，即编辑后，近50%的知识事实保留了较高的概率。进一步的分析表明，&lt;span style=&quot;background-color: #ff666680&quot;&gt;演示选择和保留演示有助于特异性，而更新演示则提高泛化能力&lt;/span&gt;。最后，我们讨论了IKE在实际场景中应用时可能遇到的潜在挑战，并提供了相应的讨论。总的来说，这项研究的贡献有四个方面： 据我们所知，这项工作代表了对 ICL 编辑 LM 知识潜力的首次系统探索。 我们对ICL 策略进行全面的实证研究，并分析这些策略如何影响最终性能。 通过设计适当的演示格式和组织策略，IKE 可以以更少的计算开销和副作用实现相当的成功率。 我们研究将IKE 应用到现实场景的可行性并讨论潜在的挑战。 2 Related Work知识编辑方法 最近关于知识编辑的研究大多是基于炒作网络或基于归因的。基于炒作网络的方法训练超网络以获得某些编辑的梯度变化。例如，曹等人。 （2021b）使用超网络来预测测试时的参数变化，这会改变事实，同时保留不相关的事实。 MEND（Mitchell 等人，2022a）学会了将原始微调梯度转换为梯度的低秩分解。米切尔等人。 （2022b）使用编辑记忆检索器和反事实模型来生成，而不更新基本模型的参数。基于归因的方法定位神经网络中某些知识的神经元激活，仅更新相关参数。戴等人。 （2022）使用基于梯度的归因评估了不同神经元对特定知识的贡献，并通过用缩放的嵌入向量替换多层感知器（MLP）权重矩阵中的列来更新或删除事实。孟等人。 (2022a)定位了表达事实知识的单层，并通过在MLP模块中编写新的键值对来编辑这些事实知识。 知识编辑基准 一些知识编辑基准通常用于评估编辑方法的有效性和特异性。对于 BERT 风格的模型，通常采用事实检查数据集 FEVER (Thorne et al., 2018) 和问答数据集 zsRE (Levy et al., 2017)。在 FEVER 中，每个 x 是一个声明，每个 y 表示相应声明的有效性。在 zsRE 中，每个 x 都是关于事实的问题，每个 y 都是答案，而 xloc 询问与 x 无关的事实。对于 GPT 风格的模型，Mitchell 等人。 (2022a) 引入了维基文本编辑数据集，该数据集要求模型完成带有编辑延续的段落，同时每个标记的分布与不相关的段落 xloc 应保持不变。在我们的实验中，我们使用了一个更具挑战性的 QA 数据集，称为 COUNTERFACT（Meng 等人，2022a）。在 COUNTERFACT 中，问题 x 的编辑答案 y 有时可能与现实世界反事实，并且不相关的超出范围的样本 xloc 比 zsRE 中的困难得多，这使得模型更难预测所需的答案。此外，预先训练的LLMs很难捕获这些所需的事实，从而避免了LLMs在编辑之前了解这些知识的影响。 情境学习 情境学习 (ICL) 是一种免训练范例，可从输入情境中串联的演示中学习。给定相关示例和查询，模型通过类比学习来做出预测（Brown 等人，2020；Liu 等人，2022）。现有的知识编辑方法需要重新计算梯度或者以廉价的方式计算并执行这样的知识编辑。斯等人。 （2022）首次探讨了情境学习是否可以更新LLMs的知识，并表明结合各种演示可以提高知识编辑的成功率。然而，他们只关注GPT-3，而没有深入探索知识编辑的潜在能力和副作用。 3 Task Formulation知识编辑的目标是通过最大化概率 PM(y*|x*) 来将新事实 (x*, y*) 注入到 LMM 中。 x*是探究M中事实知识的提示（例如，美国总统是），y*将是编辑目标乔·拜登。知识编辑还需要概括性和特异性： 泛化：对于编辑中的提示x的范围 Dx*（即与新事实相关的提示），x ∈ Dx* 的预测也应该更新为 y*。例如，预测问题：谁是美国总统？答：将更新为乔·拜登。 特异性：对于提示x 超出编辑范围，x / ε Dx*，x 的预测应该是它原来的预测yo。例如，俄罗斯总统的预测应该保留。 4 Method: IKE4.1 In-Context Learning情境学习（ICL）是由 Brown 等人提出的。 （2020）用于小样本学习。对于大型语言模型 M，ICL 的目标是根据 k 个演示 C = {(x1, y1),… 来预测输入 x 的 ˆ y ∈ Y，而无需进行任何参数更新。 。 。 ，（xk，yk）}。语言模型 M 预测给定 x 的 y ∈ Y 的概率：PM(y | x, C)。更具体地说，ICL 使用模板 T 将输入和标签转换为自然语言文本。以情感分析为例，输入 xi 和标签 yi 的上下文演示将转换为句子：xi。情感：yi，那么语言模型 M 将在给定 T (x1, y1), 的情况下预测 y ∈ Y。 。 。 ，T（xk，yk），T（x，）。 4.2 In-Context Knowledge Editing当我们将目标事实 f = (x*, y*) 注入 LM 时，我们将构造 k 个演示 C = {c1,…。 。 。 ，ck}。知识编辑的目标是当提示x在目标提示x*的编辑范围内时最大化P(y*|x,f,C)，并且最小化P(y*|x,f,C)之间的距离。 | x, f, C) 和 P (y | x) 当 x / ∈ Dx* （特异性目标）时。 LM 应确定探测提示 x 是否在 x* 的编辑范围内，即 Dx*。为了通过 ICL 实现这些目标，适当的演示输入至关重要。我们进一步将以f为目标的知识编辑演示构建分解为两个子问题： （i）如何设计每个演示的格式； (ii) 如何选择上下文演示并对其进行排名（Dong 等人，2023）。 4.2.1 Demonstration Formating每个演示 ci 都包含一个新事实 fi = (xi*, y* i )、一个探测提示 xi 及其预测 yi。上下文演示应该教会 LM 复制、更新和保留针对不同提示的预测： 复制：要将新事实注入 LM，第一步是教他们将目标提示的预测复制到新事实中。在复制演示中，xi = xi* 且 yi = y* i。 更新：知识编辑不仅仅是教语言模型重复新事实。为了知识编辑的泛化，编辑范围内提示的预测也应该更新。在更新演示中，xi ∈ Dx* i 且 yi = y* i。 保留：出于知识编辑的特殊性，语言模型应在超出范围的提示中保留其原始预测。在保留演示中，xi / ∈ Dx* i 和 yi 应该是它的原始答案 yo i。 IKE 的模板 T 将 f 、 x 和 y 转换为自然语言：T (f, x, y) = New Fact: f 。提示：xy。详细信息列于§A。 4.2.2 Demonstration Organization当我们在 LM 中编辑知识事实 f 时，我们构建 k 个演示 C = {c1,… 。 。 , ck} 来自训练语料库。哪些演示适合上下文编辑？我们关注刘等人。 （2022）使用无监督检索器来选择 k 个最近邻居。更具体地说，我们使用预训练的句子编码器 E 对新事实 f 的提示 x* 及其原始答案 yo 和目标预测 y* 进行编码。这 训练语料库中的记录将以相同的方式进行编码，并根据余弦相似度检索 k-NN 事实。上下文演示的排名还取决于余弦相似度：cos(c0, f ) &lt; cos(c1, f ) &lt; 。 。 。 &lt; cos(ck, f )，其中 c1, . 。 。 , ck 从左到右放置在上下文中。 4.3 Discussion:Gradient-based methods and gradient-free methods之前的参数更新方法会调整LM M的参数θ。它们根据梯度∇θ − log PM(y*|x*)计算Δθ，将基础模型Mθ更新为编辑后的M′θ+Δθ。然后将通过 PM′(y | x) 评估编辑方法。相反，上下文学习通过为新事实 f = (x*, y*) 构建演示 C 来修改 M 中的知识事实，然后通过 PM(y | x, f, C) 来评估编辑方法。将 PM(y | x, f, C) 与 PM′(y | x) 进行比较，可以发现： (i) ICL 不需要对目标事实进行梯度估计，并且在知识编辑后保持原始 LM M 不变。这大大减少了计算开销，从而使编辑适用于具有万亿级参数的LM，并消除了修改参数的副作用。 (ii) 演示 C 以自然文本表示，比显着参数更新 Δθ 更容易解释。它提供了一个人类可理解的界面来校准模型行为。我们在表 1 中重点介绍了这两种方法的特点。 5 Experiment在本节中，我们通过实验来回答以下研究问题： 与基于梯度的方法相比，IKE 的性能如何？ 演示设计策略如何影响IKE 的性能 LM 的规模如何影响IKE 的性能，IKE 能否扩展到具有数百或数千亿参数的大型语言模型？ 知识编辑有哪些副作用？与其他参数更新方法相比，IKE 产生的副作用是多还是少？ 我们首先介绍实验设置，包括比较基线方法、评估基准和不同尺度的语言模型，用于知识编辑（第 5.1 节）。然后我们分析了§5.2中的主要知识编辑结果以及情境学习知识编辑的影响因素（§5.3）。 5.1 Experimental Setting我们的目标是评估上下文知识编辑与参数更新方法相比的性能。我们还对不同大小的语言模型进行了实验，以探索上下文知识编辑的扩展能力。 5.1.1 基线遵循之前的知识编辑方法，我们还选择 GPT-J (6B) 作为我们的主要评估骨干。比较的基线包括： FT 在描述编辑事实的文本上微调基本模型，而无需通过应用 Adam 提前停止来训练新的模型编辑器。 MEND MEND（Mitchell 等人，2022a）通过使用预训练的超网络将权重矩阵分解为rank-1 形式来转换更新事实的微调梯度。 ROME ROME（Meng et al., 2022a）学习定位一组特定 MLP 模块的事实检索，并通过直接在 MLP 模块中写入新的键值对来更新知识。 PROMPT 探索上下文演示如何影响 IKE 的性能。我们直接使用新事实作为上下文，通过 P(y|x, f ) 来探测 LM，其中 f = (x*, y*)。实施细节见§A 5.1.2 Evaluation Setup模型 为了探索 LM 的规模将如何影响上下文知识编辑的有效性，我们在五个类似 GPT 的自回归转换器语言模型上评估了上下文知识编辑，其规模范围从 1.5B 到 175B 参数： GPT- 2 XL (1.5B)（Radford 等人，2019），GPT-2 的 15 亿参数版本。 GPT-NEO (2.7B)（Gao 等人，2021），EleutherAI 发布的类 GPT-2 因果语言模型的 27 亿参数版本。它是在专门为 LLM 训练设计的 Pile 数据集上进行训练的。 GPT-J (6B)（Wang 和 Komatsuzaki，2021），一种在具有 60 亿个参数的 Pile 上训练的自回归文本生成模型。 GPT-NEOX (20B)（Black 等人，2022），一个在 Pile 上训练的 200 亿参数自回归语言模型。 OPT (175B)（Zhang 等人，2022），开放式预训练 Transformer，由 MetaAI 创建，具有 1750 亿个参数。 基准 我们主要评估 COUNTERFACT 的基线（Meng et al., 2022a），这是一个具有挑战性的基准，适用于具有困难编辑目标和难以区分编辑范围的类 GPT 因果语言模型。它包含 21, 919 条不同关系和实体的记录。每条记录的目标是将知识三元组（s*，r*，oc）更改为（s*，r*，o*），其中s*和r*由目标提示x*描述。该记录还包含释义提示 P P 作为范围内提示和邻域提示 P N ，即与目标三元组共享同一对象的知识三元组（s′，r*，oc）作为范围外提示。我们关注孟等人。 (2022a) 使用前 2000 条记录作为测试集，其余记录分为训练集。 COUNTERFACT 的详细信息在 §B 中列出。 指标 知识编辑的性能从三个方面来衡量（有效性、泛化性和特异性）。 功效 通过功效得分 (ES, E[I[P(o*) &gt; P(oc)]]) 和功效幅度 (EM, E[P(o*) − P( oc）]）。 泛化 通过释义衡量释义提示的译后编辑准确性分数 (PS) 和释义幅度 (PM)。 PS和PM的定义与ES和EM类似。 特异性 通过邻域得分 (NS, E[I[P(oc) &gt; P(o*)]]) 和邻域量级 (NM, E[P(oc) − P(o*)]) 来衡量邻域提示的准确性，因为邻域提示 (s′, r*, oc) 与目标提示共享相同的原始对象，并且这些事实不应被编辑。 我们也关注孟等人。 (2022a) 将 ES、PS、NS 的调和平均值报告为分数 (S) 5.2 Main Results表2的顶行显示了不同方法的知识编辑结果。我们的研究结果是：（i）所有方法在功效方面都表现良好，正如它们接近的 ES 分数所示。然而，在普遍性和特殊性方面存在显着差异。例如，FT 获得了较高的 ES (99.9) 和 PS (96.4) 分数，但在特异性方面表现不佳。这凸显了知识编辑中平衡泛化和特殊性的挑战。 (ii) 在基线方法中，ROME 在所有三个指标方面总体表现最好，但计算开销较高。由于这一限制，它不适用于诸如 OPT175B 等更迫切需要知识编辑的大型 LM。 (iii) 所提出的方法 IKE 在特异性方面表现出色，但在有效性和泛化方面也表现良好。例如，IKE 在 GPTJ 上获得了与 ROME 相当的总分（89.6 比 91.5），同时不需要任何参数对 LM 的修改。这种计算优势使得在 OPT-175B 等大型 LM 上执行知识编辑成为可能，其中 IKE 比 PROMPT 明显提高了 36.0 个点。这些结果证明了 IKE 在知识编辑方面的有效性、效率和可扩展性。 5.3 Analysis在这一部分中，我们讨论不同演示策略的效果、跨尺度模型的 IKE 可扩展性以及知识编辑引入的副作用。 5.3.1Ablation on Demonstration演示次数 演示次数是 ICL 性能的影响因素之一 (Brown et al., 2020)。我们研究了演示数量如何影响第二阶段的 IKE 性能 表 3 中的块。在没有任何演示的情况下，PROMPT 因其低 NS（37.9）而表现出过度泛化，表明它只是学习复制预测。给定一些演示（4 或 8)，IKE 在有效性和泛化性方面比 PROMPT 表现更差，因为它开始区分提示是否在编辑范围内。随着演示次数的增加，IKE逐渐学会平衡通用性和特殊性，实现更好的权衡。 演示组织 先前的研究（Liu et al., 2022; Rubin et al., 2022; Lu et al., 2022）表明，包括演示选择和演示排序（Dong et al., 2023）在内的演示组织对于 ICL 也至关重要。我们的建议遵循刘等人的简单无监督方法。 （2022），根据输入提示和演示之间的余弦相似度从训练语料库中检索和排序演示。在表 3 第三块中的两项消融研究中，我们发现删除选择程序（即随机选择）会导致 NS 分数从 77.0 明显下降到 45.0，这表明正确提示选择的重要性。然而，随机排序带来的性能差异可以忽略不计。我们推测这是因为所选的提示与目标事实高度相关，并且基于 Transformer 的 LM 中的注意力机制可以很好地处理长程依赖性。我们将进一步的改进作为未来的工作。 演示格式 我们进一步检查演示类型的影响，包括复制、更新和保留。如表 3 中的第四个块所示，删除复制演示会导致性能轻微下降，因为即使没有复制演示，LM 也可以轻松复制演示中的内容。相反，更新演示在教导 LM 修改其知识方面发挥着重要作用，删除更新演示后泛化得分要差得多。此外，删除保留演示会导致特异性急剧下降（通过 NM 分数衡量），从 35.2 降至 -47.6。这表明保留演示对于帮助 LM 识别超出范围的事实并维持对这些提示的原始预测至关重要。 5.3.2IKE Benefits from Model Scaling我们进一步评估了 COUNTERFACT 上的 IKE，针对不同尺度的五种类似 GPT 的因果语言模型。正如之前的实验表明，所有方法都表现出很高的知识编辑功效，因此我们重点关注大型语言模型的泛化性和特异性，因为这些指标的定义是为了衡量可能对最终用户造成巨大影响的副作用。如表 4 所示，我们发现 IKE 的性能与 LM 的规模正相关，并且最大的 OPT-175B 实现了最强的泛化和特异性结果。这是令人鼓舞的，因为 IKE 的性能可以随着 LM 规模的增加而增强，使其可插入未来更强大的 LM 主干。 5.3.3Resilience to Over-Editing过度编辑是知识编辑的常见副作用，指在编辑目标事实时对超出范围的事实产生影响。尽管 COUNTERFACT 已经包含由 (s′, r*, oc) 组成的范围外提示，它们与编辑目标共享相同的关系 r 和原始对象 oc： (s*, r*, oc) → (s*, r*, o*），我们采用Dong等人提出的对比知识评估（CKA）对过度编辑进行更全面的评估。 （2022）。具体来说，对于一个三元组（s，r，o），CKA将r替换为其他相似但不相关的关系r′，并比较PM（o | s，r）和PM（o | s，r′）来评估M是否知道事实（s，r，o）。受此启发，我们将(s*,r’,o*)视为相似但不相关的提示，并考虑P(o*|s*,r’)的变化，发现P(o*|s*,r’ ）在注入（s*，r*，o*）后也会增加。为了进一步探索不同方法中的过度编辑，我们考虑 CKA 分数 P(o*|s*, r*)/Er′∈RP(o*|s*, r′)。 CKA评估结果如表5所示。如果CKA得分小于预定义阈值α，则正确事实的困惑度为输给了对比虚假事实的困惑，结果证明这是一次编辑失败。尽管所有基线在编辑功效方面都表现良好，但在更严格的对比评估下它们往往过于概括。 ROME 的平均 CKA 得分最低，错误率最高，这表明它识别与目标提示共享同一主题的范围外提示的能力较差。 IKE 对过度编辑的影响较小。 5.3.4Maintenance for Original Knowledge我们得出的结论是，先前存储在语言模型中的事实知识将在知识编辑过程中被删除或遗忘。我们在表6中考虑编辑前后P(oc|s*, r)的变化。结果表明，所有编辑方法都会导致P(oc|s*, r*)的下降。罗马几乎忘记了所有最初的事实。如果我们想要纠正 LM 的预测，就必须擦除原来的事实知识。然而，如果我们想更新语言模型的预测，例如更新美国总统是从唐纳德·特朗普到乔·拜登的预测（时间感知关系），那么旧知识 2017 年，美国总统是唐纳德·特朗普不应该被忘记。 为了评估编辑中这种时间感知知识的遗忘，我们基于 TEMPLAMA (Dhingra et al., 2022) 构建了一个小型基准，以进一步表明 IKE 比 §C 中的其他基准可以导致更少的知识遗忘。 6 Discussions在之前的实验中，我们遵循孟等人之前研究的设置。 （2022a）并主要评估编辑单个事实以进行公平比较的方法。我们的结果表明 IKE 可以获得更好的泛化性和特异性，副作用更少，并且不需要修改参数。尽管如此，为了探讨可行性在将 IKE 应用到现实场景中时，有几个重要问题尚未得到充分探索：(1) IKE 能否扩展以容纳更多的编辑事实？考虑到语言模型的输入长度有限，在上下文中包含大量的编辑事实可能是不可行的。 (2) IKE 能否适应处理不同格式和域的事实和提示？在IKE中，事实和提示的域和格式保持一致。然而，在现实世界中，事实和提示有多种形式。米切尔等人。 (2022b)提出了一种基于检索的方法来编辑多个知识事实。类似地，具有外部存储器来存储事实编辑的 IKE 可以检索正确的事实编辑来构建给定提示的上下文，从而避免永远在上下文中预先添加所有事实编辑。为了验证 IKE 对不同形式的事实或提示的泛化，我们用维基百科中的中性数据替换了事实，或者用生成提示替换了提示，提示 LM 生成与新对象相关的文本。详细讨论可以在§D 中找到。 7 Conclusion在这项工作中，我们研究了上下文学习在大规模语言模型上进行知识编辑的潜力。具体来说，我们设计了提示LM的演示策略，包括三种类型的演示格式和基于检索的演示组织。我们表明，所提出的方法 IKE 在不需要任何参数修改的情况下实现了竞争性知识编辑功效，并保持了良好的泛化和特异性性能。进一步的分析证明了它对于大型 LM 的可扩展性、对过度编辑问题的弹性以及通过多轮编辑维护时间感知知识事实的能力。我们的结果证明 ICL 在 LM 知识编辑方面具有巨大潜力。","tags":["EMNLP2023"],"categories":["NLP顶会"]},{"title":"我们可以编辑多模态大型语言模型吗？","path":"/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/","content":"我们可以编辑多模态大型语言模型吗？Abstract在本文中，我们重点关注编辑多模态大型语言模型（MLLM）。与编辑单模态LLMs相比，多模态模型编辑更具挑战性，需要在编辑过程中进行更高水平的审查和仔细考虑。为了促进这一领域的研究，我们构建了一个名为 MMEdit 的新基准，用于编辑多模式LLMs并建立一套创新的评估指标。我们进行了涉及各种模型编辑基线的综合实验，并分析了编辑不同组件对多模式LLMs的影响。根据经验，我们注意到以前的基线可以在一定程度上实现多模态 LLM 的编辑，但效果仍然差强人意，这表明这项任务的潜在难度。我们希望我们的工作能够为 NLP 社区提供见解1。 1 Introduction随着大型语言模型 (LLM) 的广泛部署（Zhao 等人，2023），在不产生大量再培训成本的情况下保持其知识准确和最新的必要性变得越来越重要（Sinitsin 等人，2020）。先前的研究引入了知识编辑方法，旨在逐步向语言模型注入一组新的事实（Mitchell 等人，2022a；Han 等人，2023；Hartvigsen 人，2022；Zhong 等人，2023；Gandikota等人，2023；姚等人，2023）。 与单模态模型编辑不同，多模态LLMs的编辑任务因其固有的多样性和复杂性而面临相当大的挑战。具体来说，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;多模式模型的错误输出可能源于各种模式的协同效应。输出不正确可能不仅仅源于LLMs，类似于误读或误识别等人为错误&lt;/span&gt;（例如，色盲影响图像中的颜色识别）。如图1所示，在编辑之前，模型将物体错误地识别为“梯子”而不是正确的“障碍物”，从而导致错误的预测。编辑后，模型准确识别了“障碍”。请注意，多模态LLMs（Yin et al., 2023）的效用正在增加，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;但缺乏相应的数据集资源和用于编辑多模态大语言模型的基准。&lt;/span&gt; 为了促进这一领域的研究，我们第一步构建了一个多模态模型编辑基准：&lt;span style=&quot;background-color: #ff666680&quot;&gt;称为 MMEdit，它包含两个子任务：编辑 VQA 和编辑图像标题。&lt;/span&gt;具体来说，我们&lt;span style=&quot;background-color: #ff666680&quot;&gt;遵循单模态模型编辑方法（Mitchell et al., 2022a；Cao et al., 2021；Mitchell et al., 2022b）来构建数据集，这扩展了之前的评估原则，即 Reliability2、Locality3 和通用性4，多模式设置。对于可靠性评估，我们从严格的数据收集开始，收集表现不佳的多模态模型数据来创建专用的可靠性编辑数据集（§3.2.1）。对于局部性评估，我们将其分为文本局部性和多模态局部性，以评估多模态 LLM 的稳定性（第 3.2.2 节）。对于通用性评估，与局部性类似，我们将其分为文本通用性和多模态通用性，并利用 ChatGLM (Du et al., 2022) 和稳定扩散 (Rombach et al., 2022) 生成重新措辞的文本以及重新措辞的图像进行评估（第 3.2.3 节）。我们评估了 MMEdit 上的几种知识编辑方法。&lt;/span&gt;根据经验，我们注意到&lt;span style=&quot;background-color: #5fb23680&quot;&gt;当前的编辑方法对于编辑多模态语言模型中的文本模型有效，但对于编辑视觉模块则不那么有效&lt;/span&gt;。例如，在编辑BLIP-2模型的语言模块时，MEND的可靠性可以达到92.6%，但在编辑视觉模块时只能达到14.1%，表明该任务的潜在难度和机遇。总的来说，我们的主要贡献如下： 我们迈出了第一步，研究编辑多模态LLMs，将模型编辑扩展到多模态设置。 我们提出了MMEdit，一个新的基准，用于评估多模态模型编辑方法的可靠性、局部性和通用性。 我们使用各种基线进行实验，证明虽然当前的方法可以在一定程度上帮助多模式编辑，但结果仍然达不到完全满意的程度。我们将公开代码和数据集以用于未来的研究目的。 2 Related Work2.1 Multimodal Language Models 多模态学习 (MML)（Xu 等人，2022a；Yin 等人，2023）提供了一种构建 AI 模型的整体方法，该模型可以从各种数据模态中提取和关联信息。由于其社会意义，MML 在研究界站稳了脚跟，在过去十年中巩固了自己作为一个重要研究领域的地位。视&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;觉语言预训练是MML的重要分支之一，旨在学习在各种视觉和语言任务上具有改进性能的多模态基础模型&lt;/span&gt;。 Vision Transformer (ViT)（Dosovitskiy et al., 2021）&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;是一项开创性的工作，贡献了端到端将 Transformers 的编码器应用于图像的解决方案。&lt;/span&gt; CLIP（Radford et al., 2021）&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;提出了一种方法，使用多模态预训练将分类转换为检索任务，使预训练模型能够解决零样本识别问题&lt;/span&gt;。最近，LLaMA (Touvron et al., 2023)、BLOOM (Scao et al., 2022) 和 ChatGPT (OpenAI, 2022) 等 LLM 的进步得到了扩大训练数据和增加参数的支持，最近取得了重大成功。这些模型展示了令人印象深刻的语言理解、生成和知识推理能力，增强了它们理解自然语言和生成高质量、基于上下文的文本的能力。大型语言模型的发展刺激了自回归语言模型作为视觉语言任务中的解码器的广泛使用。&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;利用跨模态迁移，这种方法可以实现语言和多模态领域之间的知识共享&lt;/span&gt;（Gao et al., 2023; Liu et al., 2023; Li et al., 2023a; Ye et al., 2023; Zhu et al., 2023；Li 等人，2023b；Zhang 等人，2023）。 2.2 Model Editing LLMs（Zhao et al., 2023）主要从训练语料库中获取知识。然而，数据集的质量并不总是得到保证，可能会将有害或不正确的信息集成到模型中（Hernandez 等人，2023）。&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;一种解决方案是使用更新的知识重新训练模型，尽管这可能成本高昂且难以实施&lt;/span&gt;。或者，&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;可以考虑使用一些更新的事实进行微调，但它存在过度拟合和灾难性遗忘的风险&lt;/span&gt;（Zhai et al., 2023）。为了解决这些问题，（Sinitsin et al., 2020）提出了模型编辑，旨在高效、准确地改变模型中存储的事实知识。这种方法应用于各个领域（Mao et al., 2023; Onoe et al., 2023; Xu et al., 2022b; Wang et al., 2023a; Li et al., 2023c），并且研究数量不断增加调查编辑的影响（Ilharco 等人，2023；Gupta 等人，2023；Hase 等人，2023；Cohen 等人，2023；Wu 等人，2023；Wang 等人，2023b；Gandikota 等人等人，2023；Li 等人，2023d）。目前，模型编辑方法主要有三种类型：&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;1）元学习方法，2）定位然后编辑方法，3）上下文知识编辑方法。&lt;/span&gt; 元学习方法。 MEND（Mitchell 等人，2022a）和知识编辑器 (KE)（Cao 等人，2021）提出了涉及外部编辑器的方法，能够学习用于知识更新的最佳参数集 θ，同时施加约束以保持模型稳定性。 CaliNET (Dong et al., 2022) 和 T-Patcher (Huang et al., 2023) 从 (Dai et al., 2022) 中汲取灵感，将额外的可训练参数引入到预训练语言模型的前馈模块中。 SERAC（Mitchell 等人，2022b）利用显式记忆来存储编辑，并学习对它们进行推理，以根据需要调整基本模型的预测。 定位然后编辑方法。 ROME（Meng et al., 2022a）提出了采用因果中介分析来识别编辑区域的方法。 ROME 发现记忆的事实关联可以精确定位到 GPT 模型中的特定位置。然而，ROME 的一个显着限制是它一次只能编辑一个事实。为了解决这个问题，Meng 等人。 (2022b)提出了一种称为MEMIT的新方法，它是之前工作ROME的继承者，它对单层的MLP权重进行rankone修改，以将内存直接写入模型中。 上下文知识编辑方法。 InContext Learning (ICL)（Brown et al., 2020）表示一种免训练范式，其中知识是从输入上下文中直接串联的演示中获取的。最近出现了一种新颖的编辑范式，它利用LLMs理解上下文的能力（Zheng et al., 2023），从而实现基于上下文的模型编辑，指导模型的生成过程，并提供高效、轻量级的模型方法编辑。迄今为止的模型编辑方法主要迎合单模态场景，在多模态编辑方面留下了空白。据我们所知，我们是第一个研究LLMs多模式模型编辑的人，并为促进该领域的研究提供了新的基准。 3 Editing Multimodal LLMs我们在图 2 中说明了多模态编辑的建议任务。我们将介绍任务定义（§3.1）、（§3.2）中的数据集构建细节、多模态模型（§3.3）以及我们在实验。 3.1 Task Definition假设我们有一个由 θ 参数化的多模态 LLM f （由两部分组成，由 θvision 和 θtext 参数化的 fvision 和 ftext ），将输入 ie 和 xe 映射到 yo 的预测，其中 ie 指的是编辑图像输入，xe 指的是编辑文本提示输入和 yo 表示为原始输出。我们将 M 表示为特定指标的符号表示，下标表示特定度量，上标表示变化编辑数据。我们准备第 3.2.1 节中所述的编辑数据集，其表示为 Dedit。受到姚等人的启发。 （2023），我们引入了一系列多模式模型编辑指标。 可靠性。需要编辑可靠性才能将预测从 yo 更改为 ye。直观上，我们需要的是更新后的 θe，其中 f（即 xe;θe）= ye。为了衡量可靠性，我们使用编辑准确性，如下所述： 其中θe指的是编辑后的参数。 局部性。为了保持模型的稳定性，必须最大限度地减少编辑对模型更广泛的知识库造成的意外副作用。为了实现这一目标，我们引入了两个指标：M Text loc (T-Locality) 和 MImage loc (M-Locality)，这两个指标都是为了在编辑过程中保持模型的稳定性。鉴于多模态语言模型中的知识是从LLMs继承的，保护这些知识至关重要。考虑到这一目标，我们搁置了模型的视觉辨别模块，而是采用基本的问答数据集 Dloc-t，如第 3.2.2 节中所述。我们定义问题为x，答案为y，如下： 视觉编码器在多模态语言模型中发挥着关键作用，将将图像转换为矢量表示，以便与自然语言文本共同编码。 因此，我们必须考虑对该模块进行任何修改的潜在后果。我们构建表示为 Dloc-v 的数据集用于测试 MImage loc ，并计算如下： 其中（iv，xv，yv）是范围外数据，θe表示编辑数据更新的参数（即xe，ye)。 泛化性。在整个编辑过程中，仅仅修改个别错误的输入是不够的。修订后的模型还应保留泛化能力，并始终为等效输入（例如改写的句子）生成一致的输出，如图 3 所示。虽然以前的单模态模型编辑任务仅需要考虑改写的文本，但多模态场景需要泛化以及图像。为了解决这个问题，我们引入了两个泛化考虑因素：MText gen (TGenerality) 和 MImage gen (M-Generality)，其表示如下： 其中 ir 表示重新表述的图像，xr 指重新表述的文本提示，N (x) 表示 x 范围内的对象。 3.2 Datasets我们构建的数据集MMEdit主要包含两个子任务：编辑VQA（E-VQA）和编辑图像标题（E-IC）。 3.2.1 Reliability Dataset Construction为了对我们的实验进行基准测试，我们选择了两个常见的多模态任务：视觉问答（VQA）（Antol et al., 2015）和图像 字幕（Herdade 等人，2019）。 VQA 旨在设计一种算法，不仅可以理解图像中的视觉内容，还可以理解用于查询该图像的自然语言，并随后生成这些查询的精确答案。图像字幕是设计能够理解图像视觉内容的算法，随后用自然语言生成连贯且精确的图像描述。在本研究中，我们选择 BLIP-2 OPT。我们的基础编辑数据源自两个评估数据集的次优条目，即 VQAv2（Goyal 等人，2017）和 COCO Caption（Chen 等人，2015)。 除了基本的编辑数据之外，利用其他数据也至关重要。这些数据不仅有助于编辑过程，还验证更改的有效性，评估模型编辑的稳定性和通用性。 3.2.2 Locality Dataset Construction我们必须仔细考虑多模态模型中编辑对语言功能的影响，类似于我们如何评估手术后个体大脑的各个认知区域。 文本局部性数据集。为了评估语言模型的稳定性，我们利用之前在 MEND 中使用的 NQ 数据集（Kwiatkowski 等人，2019）作为模型内 LLM 组件稳定性的基准。我们专门使用模型的输出预编辑和后期编辑来构建 KL 散点图，从而促进对模型编辑的约束。此外，我们还计算了保持 top-1 状态的实例比例，进一步量化了模型的稳定性。 多模态局部性数据集。同样，验证编辑对视觉模块的影响也至关重要。因此，我们在多模态领域使用简单的数据集 OK-VQA（Marino 等人，2019），作为多模态视觉模块局部性的度量。我们再次在编辑过程之前和之后使用 logits 更新 KL 离散度约束。 3.2.3 Generality Dataset Construction我们在多模态模型中提出了两种形式的通用性。共性数据集构建的整体流程如图4所示。 文本泛化数据集。值得注意的是，LLMs表现出强大的会话能力和强大的解决问题的能力，这使我们能够制定任务指令，从而指导模型生成类似的文本输入。对于 E-VQA 任务，我们利用 ChatGLM（Du et al., 2022；Zeng et al., 2022）生成类似的查询。然而，对于E-IC任务，由于提示的简洁和相对直接，模型生成的输出质量并不令人满意。因此，我们采用手动编写的包含20条提示的模板来随机替换原来的提示。 视觉泛化数据集。近年来，扩散模型（Ho et al., 2020）在图像生成领域取得了巨大成功。超越最初最先进的模型：生成对抗网络（GAN）模型（Goodfellow 等人，2014）。扩散模型在许多图像生成任务中表现出色，并在各个应用领域中表现出了值得称赞的性能。稳定扩散（Rombach 等人，2022）是一种潜在的文本到图像扩散模型，能够根据给定的文本输入生成逼真的图像。我们利用稳定扩散 2.1 来生成重新解释的图像。该数据集利用 COCO 数据集的标题描述来评估模型的图像泛化能力。 3.3 Multimodal Language ModelsBLIP-2 OPT。 BLIP-2（Li et al., 2023b）是一种通用且高效的预训练策略，可从现成的冻结预训练图像编码器和冻结大型语言模型引导视觉语言预训练。该模型利用轻量级查询转换器来弥合视觉模态和文本模态之间的差距，并在各种视觉语言任务上实现最先进的性能。我们选择 BLIP-2 OPT 作为基本编辑模型，它在视觉模块中利用 ViT-L，并选择无监督训练的 OPT 模型用于基于解码器的 LLM。迷你GPT-4。 MiniGPT-4（Zhu et al., 2023）是一种类似于 BLIP-2 的有效视觉语言模型，利用冷冻视觉编码器与冷冻 Vicuna（Chiang et al., 2023）相结合。据报道，基于 LLaMA 构建的 Vicuna 根据 GPT-4 的评估标准达到了 ChatGPT 90% 的性能。 MiniGPT-4 添加了一个投影层，以使编码的视觉特征与 Vicuna 语言模型保持一致。 MiniGPT-4 采用与 BLIP-2 相同的预训练视觉组件，由 EVA-CLIP（Sun 等人，2023）的 Vit-G/14 和 Q-Former 组成。 3.4 BaselinesFinetune。微调已成为一种广泛采用的策略，用于使预训练的语言模型适应特定任务或领域（Cortes 等人，2015）。在我们的探索中，我们深入研究了两种不同的微调方法：一种专注于语言模型的最后一层。以BLIP-2 OPT模型为例，我们对OPT模型的第31个解码器层进行微调。另一个目标是多模态语言模型中的视觉块，具体来说，我们微调 Q-former 模型以过度拟合编辑数据集。 MEND。具有梯度分解的模型编辑器网络（Mitchell 等人，2022a）使用单个输入输出对对语言模型进行高效的本地编辑。本质上，MEND 学习转换微调 LLM 的梯度，它利用梯度的低秩分解。 KE。 KE（Cao et al., 2021）是一种可以编辑语言模型中错误知识而无需重新训练整个模型的方法。 KE 利用具有约束优化的超网络（双向 LSTM），用于预测推理过程中的权重更新。 SERAC。 SERAC（Mitchell 等人，2022b）引入了一种基于内存的模型编辑方法，该方法利用显式内存系统来缓存编辑。该内存随后用于在推理过程中调整基本模型的输出。该系统利用一个小型辅助范围分类器和反事实模型。范围分类器的作用是确定输入是否在内存缓存的范围内。如果在此范围内找到输入，则会将其与最相关的缓存项结合起来，并输入到反事实模型中进行预测。 In-Context Knowledge Editing.。上下文知识编辑（IKE）（Zheng et al., 2023）构造 k 个演示 C = {c1, . 。 。 , ck }，遵循 Liu 等人中概述的方法。 （2022）。该方法采用基于余弦相似度的无监督检索器，在将事实 f = (x*, y*) 注入语言模型之前从训练集中获取演示。 x* 是探索模型中事实知识的提示（例如，美国总统是），y* 将是编辑目标乔·拜登。上下文演示的排名也取决于余弦相似度：cos(c1, f ) &lt; cos(c2, f ) &lt; · · · &lt; cos(ck, f )。其中 c1, . 。 。 , ck 在上下文中从左到右顺序排列。演示 C 可以被视为外部增强的知识库，主要设计用于指导 LM 内的生成。其最终目标是当提示符x落在目标提示符x*的编辑范围内时，最大化P(y|x,f,C)。 4 Experiments4.1 Results在这一部分中，我们对 MMEdit 上的多种编辑方法进行了比较分析。这些比较的结果如表2所示。之后，我们深入研究了实验结果的三个指标，包括可靠性、局部性和通用性三个方面。此外，我们通过文本和视觉方式分析局部性和通用性，并在图 6 中提供了几个编辑案例。 可靠性。从结果来看，所有模型编辑方法在可靠性方面均优于基本方法。特别是，利用外部存储器进行编辑的 IKE 和 SERAC 方法在多模态语言中表现出了值得称赞的性能楷模。我们观察到微调方法的性能比模型编辑方法差。请注意，仅微调 LLM 或模态融合模块的参数并不能充分捕获多模态数据的特征。我们分析原因为如下：用于微调的数据与原始模型有较大差异，例如Q-former和OPT模型，需要有效协作。简单地微调这些模块之一可能无法准确捕获特定于任务的特征。另一方面，微调所有模块会产生大量的资源开销。此外，根据我们的实验结果，我们观察到微调可能会导致原始模型发生重大变化，通常会导致其他知识的丢失，在多模式数据集中尤其明显。 局部性。几种传统的编辑方法仍然适用于多模式编辑，这对于有效修改模型内的知识并纠正其输出很有价值。然而，IKE和SERAC尽管在可靠性方面表现出色，但由于缺乏对M-Locality的约束而在M-Locality上表现不佳，这表明尽管这些基于外部存储器的编辑技术无疑成功地修复了输出，但它们在稳定内部模型中的知识还有改进的空间。对于T-Locality，大多数模型编辑方法都获得了良好的性能，而IKE再次表现不佳。根本原因是其他三种方法对T-Locality施加了约束，而IKE作为InContext Learning方法缺乏鲁棒的约束机制，导致性能不佳。 泛化。我们在 E-VQA 中与 MiniGPT-4 进行了各种方法的文本和图像泛化能力的比较探索。请注意，KE 往往表现出较低程度的图像泛化，这主要是由于其在训练阶段对 M 局部性的固有考虑。因此，与基于记忆的方法相比，元学习方法的图像泛化效率往往较低。另一方面，基于内存的方法所表现出的卓越图像泛化能力是以牺牲 M-Locality 为代价实现的，导致 M-Locality 水平显着降低。通过对各种编辑方法的评估，我们经常发现图像泛化性能往往不如文本泛化性能强大。 4.2 Editing Different Component我们进一步分析了编辑多模态模型不同区域的变化。与编辑单模态模型相比，由于多模态模型的复杂性和多样性，我们可以尝试编辑更多模块并分析它们对视觉和文本知识的影响。结果如图 7 所示。对于 BLIP-2 OPT 模型，我们研究了在 VQA 数据集上编辑 Q-former 和 OPT 的区别。关于MiniGPT4模型，我们主要关注llama_proj和Vicuna模型在编辑最后几层的区别。选择的分析编辑方法有 MEND、KE 和 FT，这使我们能够指定编辑区域。 结果表明，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;编辑视觉模块比编辑语言模块更具挑战性（另请参阅图 6 中的失败编辑）。我们认为这种困难可能归因于模型的架构&lt;/span&gt;。&lt;span style=&quot;background-color: #ff666680&quot;&gt;编辑LLM的最后一层可以直接修改输出，而修改视觉模块只影响LLM的输入，对模型的影响相对较小&lt;/span&gt;。具体来说，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;各种模态驻留在不同的空间中，这意味着事实知识可以存储在模型内的单独参数中&lt;/span&gt;。考虑到LLMs拥有大量参数，这一点对于多模态模型变得更加重要。因此，编辑语言模型可以显着提高性能。值得注意的是，模型中的视觉模块在图像理解中起着至关重要的作用，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;因此表明未来的工作需要同时考虑来自不同模式的信息。&lt;/span&gt; 5 Conclusion在本文中，我们介绍了多模式模型编辑，以及新的基准 MMEdit。根据经验，我们分析了各种模型编辑基线的有效性，并探索它们对不同组件（例如视觉和文本）的影响。 6 Limitations楷模。我们只编辑几个基本的多模式LLMs，留下许多其他的。此外，由于资源限制，我们编辑的多模态LLM的参数数量低于10B，我们无法编辑具有更多参数的LLM，例如65B LLaMA Adapter V2（Gao et al., 2023） 。 高效的视觉编辑。在本文中，我们的分析主要集中于比较不同模式模块中现有编辑方法的不同效果。然而，结果并不令人满意。展望未来，我们的主要目标是探索如何跨其他模式高效、准确地编辑信息。这包括研究技术，例如通过查明多模态模型内的知识并识别需要修改的内容来在不同模态之间进行共同编辑。","tags":["EMNLP2023"],"categories":["NLP顶会"]},{"title":"知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现","path":"/2023/11/02/zhi-shi-shen-jing-yuan-zhong-xin-zhi-lu-yu-yan-wu-guan-zhi-shi-shen-jing-yuan-he-jian-bing-zhi-shi-shen-jing-yuan-de-fa-xian/","content":"知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现Abstruct预训练语言模型 (PLM) 包含大量事实知识，但这些知识如何存储在参数中仍不清楚。本文深入研究了理解事实知识如何存储在多语言 PLM 中的复杂任务，并介绍了适应架构的多语言集成梯度方法，与现有方法相比，该方法成功地更精确地定位了知识神经元，并且在各种架构和语言中更加通用。此外，我们对知识神经元进行了深入的探索，得到了以下两个重要发现：（1）语言无关的知识神经元的发现，它以超越语言的形式存储事实知识。我们设计了跨语言知识编辑实验，证明 PLM 可以基于语言无关的神经元完成这项任务； （2）退化知识神经元的发现，这是一种新型神经元，表明不同的知识神经元可以存储相同的事实。其功能重叠的特性赋予 PLM 强大的事实知识掌握能力。我们设计了事实检查实验，证明退化知识神经元可以帮助 PLM 检测错误事实。实验证实了这些发现，揭示了多语言 PLM 中事实知识存储的机制，并为该领域提供了宝贵的见解。源代码将公开以供进一步研究。 1 Introduction预训练语言模型 (PLM)（Devlin 等人，2018 年；Radford 等人，2019 年；Shliazhko 等人，2022 年；OpenAI 2023；Touvron 等人，2023 年）因其卓越的性能而彻底改变了自然语言处理领域涵盖广泛的任务。这些模型在维基百科等广泛的语料库上进行训练，被广泛认为封装了大量事实知识（Petroni 等人，2019b；Jiang 等人，2020），但知识如何存储在参数中仍不清楚（Kandpal 等人） .2023）。研究知识存储机制将有助于更深入地理解和掌握 PLM 中的知识（Zhen 等人，2022 年；Zhao 等人，2023 年）。在本文中，我们对知识定位任务（Hase et al. 2023; Andreas 2022）进行了深入研究，该任务旨在确定模型参数中特定事实知识的存储位置，其中此类参数被称为知识神经元（Dai 等人，2022）。 最近，一些已建立的方法致力于阐明 PLM 中的知识存储机制。一种策略是基于梯度的方法（Ancona et al. 2019），它通过使用积分梯度计算每个神经元的归因得分来评估每个神经元的贡献。另一种是因果启发方法，它采用跟踪算法来跟踪模型层之间的因果影响（Cao 等人，2023）。尽管在知识本地化任务中取得了成功，这些方法仍然面临两个主要挑战：（1）缺乏针对不同 PLM 架构的通用方法：观察到事实知识出现在各种 PLM 架构中，包括自动编码模型（例如， BERT）（Devlin 等人，2018 年）和自回归模型（例如 GPT）（Shliazhko 等人，2022 年）。然而，虽然有些方法适合自编码模型并且在自回归模型中表现不佳（Meng et al. 2022a），但其他方法是专门为自回归模型设计的并且不能很好地适应自编码模型（Li et al. 2022），在跨两种 PLM 架构都表现良好的通用方法中留下了空白。(2)缺乏多层次探索多种语言：实质性知识与语言无关，当前的大型语言模型支持多语言。然而，现有的方法仅关注英语数据集，可能无法提供跨语言知识存储机制的全面见解，限制了得出多语言结论的能力。 为了更精确地定位知识神经元，我们遵循基于梯度的方法，提出了一种新颖的知识定位方法，称为架构适应多语言集成梯度（AMIG）。&lt;span style=&quot;background-color: #ff666680&quot;&gt;首先，针对不同 PLM 架构中缺乏通用方法，我们设计了一种架构适配技术，使得集成梯度算法（Lundstrom、Huang 和 Razaviyayn 2022）中的基线向量在不同 PLM 架构之间普遍兼容。其次，针对多语言探索的缺乏，我们引入了多语言阈值调整技术，针对不同语言调整综合梯度计算中的阈值。&lt;/span&gt;多语言数据集上的实验结果表明，与之前最先进的模型相比，我们的方法可以更精确地定位知识神经元。此外，我们还对知识神经元进行了深入的探索，得出了以下两个重要发现。 与语言无关的知识神经元：我们在多语言 PLM 中发现了一种新型神经元，能够跨语言存储事实知识。我们将它们命名为与语言无关的知识神经元，因为它们的存在超越了特定语言的界限。如图1a所示，这些神经元是通过将源自不同语言的知识神经元相交而获得的，封装了跨多种语言一致的知识表示。独立于语言的知识神经元可以帮助跨语言的知识编辑任务：对某些知识的单次编辑可以同时影响所有语言的相应知识。例如，如果我们将事实⟨Tanzania, Capital, Dar es Salaam⟩对应的语言无关神经元编辑为⟨Tanzania, Capital, Dodoma⟩，则该事实在所有语言中都会相应更改。我们设计实验来验证与语言无关的知识神经元的作用。与现有的跨语言知识编辑模型相比，我们的方法的编辑性能更为优越。该实验证明了我们的方法在跨语言知识编辑应用中的潜力。 **退化知识神经元**：我们发现了一个有趣的现象，对应于一种全新类型的神经元。&lt;span style=&quot;background-color: #ff666680&quot;&gt;给定事实及其相应的知识神经元，知识神经元的某些子集表现出独特的属性。即使该子集中的某些元素被抑制，模型仍然可以正确地表达事实；然而，如果子集中的所有元素都被抑制，模型就无法再正确地表达事实。这一现象表明，一些知识神经元存储着相同的事实知识，模型需要激活至少一个神经元才能正确表达事实。&lt;/span&gt;它与生物系统中的“简并”现象非常相似（Tononi, Sporns, and Edelman 1999; Mason 2015），因此我们将此类神经元命名为简并知识神经元。与冗余不同，&lt;span style=&quot;background-color: #ff666680&quot;&gt;简并知识神经元不能简单地删除，因为它们仅部分重叠。一个退化的知识神经元可能存储多条事实知识，删除它对特定知识没有影响，但可能会影响其他知识。&lt;/span&gt; 图1b说明了简并知识神经元的获取过程。具体来说，&lt;span style=&quot;background-color: #ff666680&quot;&gt;我们首先对知识神经元进行定位，然后对它们进行聚合和过滤以获得简并的知识神经元。&lt;/span&gt;对于查询“坦桑尼亚的首都是”，PLM 必须激活至少一个相应的简并知识神经元来预测正确的事实 Dodoma。直观上，&lt;span style=&quot;background-color: #ff666680&quot;&gt;简并知识神经元的功能重叠特性赋予 PLM 对事实知识的强大理解，确保其对事实的掌握保持稳定且不易出错&lt;/span&gt;。受此启发，我们设计了一个实验，使用简并知识神经元进行事实检查。我们的实验表明，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;简并知识神经元可以帮助 PLM 检测错误事实，从而说明它们的存在增强了 PLM 对事实知识的稳定掌握。&lt;/span&gt; 总的来说，主要贡献总结如下：（1）我们提出了一种新颖的知识本地化方法，称为架构适应的多语言集成梯度，它可以有效解决传统方法的两个挑战：缺乏针对不同 PLM 架构的通用方法和缺乏对多种语言的探索，从而实现知识神经元更精确的定位。 （2）我们发现了独立于语言的知识神经元，它们以超越语言障碍的形式存储事实知识。实验结果表明它们有利于跨语言知识编辑任务。 （3）我们发现了简并知识神经元，这是一种具有功能重叠特性的新型神经元，使得模型对事实知识的掌握更加稳健。实验证明它们可以帮助检测不正确的事实。 2 Methodology图 2 示意性地展示了我们提出的框架。它由三个主要模块组成，包括知识神经元定位（模块1）、语言无关知识神经元检测（模块2）和简并知识神经元检测（模块3）。我们详细说明了每个模块。 2.1KnowLedge Neuron Localization图 2 的模块 1 展示了知识定位模块，该模块旨在查明 PLM 中知识神经元的确切位置。使用填空完形填空任务（Petroni 等人，2019a），我们评估对 PLM 对特定事实的理解。例如，给定一个事实 ⟨Tanzania, Capital, Dodoma⟩ 以及相应的查询“坦桑尼亚的首都是”，Petroni 等人 (2019a) 描述，如果模型能够预测正确答案，则模型知道一个事实。在本研究中，我们通过引入架构适应多语言集成梯度方法来扩展此分析，以定位专门负责处理事实信息的神经元。 从数学上来说，给定一个查询 q，PLM 预测的正确答案的概率可以定义为： 其中 y* 是正确答案，w(l) j 是第 l 层的第 j 个神经元，^ w(l) j 是 w(l) j 分配的值。为了计算每个神经元的归因分数，我们使用积分梯度（Sundararajan、Taly 和 Yan 2017）。考虑一个神经元 w(l) j ，我们可以计算它的归因分数： 其中 w(l) j 是 w(l) j 的值，w′(l) j 是 w(l) j 的基线向量，并且 ∂ F(w′(l) j +α(w(l)) j −w′(l) j )) ∂ w(l) j 计算梯度。当 α 从 0 变为 1 时，(w′(l) j +α(w(l) j −w′(l) j )) 从 w′(l) j 变为 w(l) j ，因此 Attr (w(l) j )可以通过对梯度进行积分来累积因w(l) j 变化而引起的概率变化。理想的基线向量 w′(l) j 应该缺乏信息（Liu et al. 2022)，当前的方法用零向量对其进行近似。然而，这样的设置没有考虑各种 PLM 架构之间的差异，导致性能不佳。为了缓解这个问题，我们设计了一种架构适应技术来计算各种 PLM 架构的基线向量。 首先，为了最小化基线向量中的信息内容，我们遵循Enguehard（2023）的方法，将输入查询q分成m个单词，然后将每个单词分别输入到PLM中以计算神经元的激活分数对应每个词qi。随后，我们精心设计了不同 PLM 架构的基线向量。设qi对应的基线句子为q′ i，q′ i包含m个单词，长度与q一致，记为q′ i = (q′ i1 . . . q′ ik . . . q′ im) ， 在哪里： 其中 ⟨mask⟩ 用于屏蔽自动编码模型，⟨eos⟩ 代表自回归模型中的“序列结束”，qk 是查询的第 k 个单词。在此设计中，第 l 层中的第 i 个神经元（用 w(l) j 表示）对应于 qi，其相关基线向量 w’(l) j 对应于 q’ i。然后，我们可以根据方程（2）计算使用 qi 作为输入时每个神经元的归因得分 Attri(w(l) j )。为了计算积分，我们使用黎曼近似： 其中 N 是近似步数。然后对每个单词 qi 的归因进行求和并标准化，得出查询的最终归因分数： 其中 n 是第 l 层中的神经元数量。最后，我们可以找到归因分数大于阈值τ的神经元，并将其视为知识神经元，记为N。 2.2 Language-Indepent Knowledge Neuron Dectection解释 许多 PLM 支持多语言，并且这些模型中的事实知识的很大一部分是与语言无关的（Xu 等人，2023 年；Wang、Lipton 和 Tsvetkov，2020 年）。这种必要性对于探索多语言 PLM 中事实知识的存储机制变得越来越重要。我们将存储多种语言共有的事实知识的神经元定义为与语言无关的知识神经元，记为 L。为了识别这些类型的知识神经元，我们设计了一种检测算法，如下所示。 算法 如图 2 的模块 2 所示，给定 K 种语言中具有相同语义的事实三元组，让相应的查询用 qk 表示，其中 k = 1, 2, …。 。 。 ，K。对于每个查询，我们使用知识神经元定位模块来获取相应的知识神经元，其中神经元 w(l) i 的属性得分记为攻击 (w(l) i )。多语言PLM对不同语言的敏感度不同，导致不同语言查询的归因分数存在显着差异。因此，很难通过设置统一的阈值来获得所有语言的知识神经元。为了解决这个问题，我们设计了一种多语言阈值调整技术。我们为不同的语言设置不同的缩放因子τk，并记录查询qk对应的神经元的最大归因得分，然后确定动态阈值： 然后，我们使用阈值过滤来识别第 k 种语言的知识神经元 Nk ，如下所示： 最后，我们计算所有语言的知识神经元的交集： 其中 L 代表独立于语言的知识神经元，编码在所有考虑的语言中一致的事实知识。通过上述算法，我们最终可以得到它们。 2.3Degenerate Knowledge Neuron Detection解释 通过进行深入分析，我们发现了一个有趣的现象：不同的神经元组负责存储相同的事实知识。例如，对于表示为 ⟨h, r, t⟩ 的特定事实，假设我们定位 10 个标记为 N = {1, 2, … 的知识神经元。 。 。 ，10}。如果我们抑制集合 A = {1, 2} 或 B = {3, 4, 5}（N 的两个子集）的神经元，我们观察到预测概率没有显着下降。相反，同时抑制这两组神经元（即 A∪B）会导致预测概率的大幅损失。这表明 A 组和 B 组都包含相同的事实知识，至少其中一个必须是活跃的，模型才能准确理解事实。此外，这两组神经元并不相互冗余。也就是说，除了事实⟨h，r，t⟩之外，A还可以存储事实⟨h1，r1，t1⟩，而B可以存储⟨h2，r2，t2⟩，从而在PLM中发挥附加作用。鉴于这种行为与生物神经网络中的退化现象相似（Tononi、Sporns 和 Edelman 1999；Mason 2015），我们为这些神经元创造了术语“退化知识神经元”。接下来详细介绍这个概念。算法 正式地，令 N = {n1, . 。 。 , nk} 是所有局部知识神经元 1 的集合，我们将退化知识神经元定义为 D = {d1D, . 。 。 , dDm}，其中每个 dD i = {ni1, . 。 。 ,niv}包含v个知识神经元，并且满足以下条件： 其中 Ps(ni) 是并集 Sv j=1 nij 的真子集，即 Ps(ni) ⊊ Sv j=1 nij。 Prob(X)是当神经元集合X被激活时模型的预测概率，Tlow和Thigh是可接受的预测概率差的预定义阈值。式(9)表明，抑制dD i 的任意真子集，即Ps(ni)，不会导致预测概率显着下降；而等式（10)表明，抑制dD i 中的所有神经元将导致预测概率显着下降。这表明这些神经元存储相同的知识。 一般情况下，&lt;span style=&quot;background-color: #ff666680&quot;&gt;考虑到我们有n个知识神经元，我们需要评估所有可能的子集，找到D的复杂度是O(2n)。为了使问题易于处理，我们通过假设每个 dD i 仅包含两个知识神经元来简化问题。这个假设将问题复杂度降低到 O(n2)。&lt;/span&gt; 为了进一步减少计算量，我们设计了两步过滤过程。如图2的算法1和模块3所示，我们&lt;span style=&quot;background-color: #ff666680&quot;&gt;首先抑制每个神经元并记录不会导致预测概率显着下降的神经元，这些神经元被视为潜在的简并知识神经元Pd&lt;/span&gt;。对于Pd中的元素，进行二次过滤：&lt;span style=&quot;background-color: #ff666680&quot;&gt;抑制其中的神经元对，如果该操作导致模型的预测概率显着下降，则将该神经元对记录为退化知识神经元dD i &lt;/span&gt;。最后我们可以将退化的知识神经元返回为D。 3 Experiments3.1Experimental Settings模型选择和数据集 在我们的实验中，我们选择了两种不同的多语言 PLM：m-BERT (Devlin et al. 2018) 和 m-GPT (Shliazhko et al. 2022)。 m-BERT 是一种自动编码模型，针对多种多语言数据集进行了预训练，而 m-GPT 是一种自回归模型，旨在处理 61 种语言的广泛语料库。关于数据集，我们采用 mLAMA (Kassner, Dufter, and Sch utze 2021)，它是原始 LAMA (Petroni et al. 2019a, 2020) 的多语言扩展，用于本地化多语言 PLM 中的知识。 评估指标 我们对这两种方法应用相同的神经元编辑操作，其中检测到的知识神经元被抑制或增强，然后计算 PLM 对相关和不相关事实的预测概率。为了全面比较不同方法的知识定位精度，我们提出了一种新的评估指标来评估整个数据集知识定位的结果： 其中SRx是编辑成功率，x代表我们抑制或增强神经元的编辑操作。给定一个查询，它本身被认为是相关事实，并且随机选择不同类型的事实作为其不相关事实。 ΔP robrx 和 ΔP robix 分别表示相关事实和不相关事实在操作 x 下预测概率的平均变化。总体而言，我们希望相关事实随着知识神经元的变化而变化，而不相关事实保持不变；因此，成功率越高，定位效果越好2。由于我们分别对神经元进行抑制和增强操作，因此将这两种情况的成功率总结为最终的成功率：SR = SRenhance + SRsuppress。 3.2Localization of Knowledge Neurons我们使用模块1在英语和中文数据集上的m-BERT和m-GPT模型上进行实验，并以Dai等人（2022）提出的方法作为基线，我们将其表示为B-KN。我们的研究结果如表 1 和图 3 所示，从中我们得出了一些重要的见解。 (1) 我们的方法在所有设置下都取得了更好的结果。在表1中，我们使用AMIG来表示我们的方法，表中的结果代表平均成功率SR。在所有设置下，我们的方法都优于 B-KN，特别是对于中国数据集，m-BERT 和 m-GPT 的成功率分别提高了 84.34% 和 44.49%。这表明我们的方法定位的知识神经元更加精确。 &lt;span style=&quot;background-color: #5fb23680&quot;&gt;（2）在m-BERT中，知识神经元主要位于最后层，而在m-GPT中，知识神经元位于前、中、最后层，如图3所示，其中x和y轴代表PLM分别是层数和知识神经元的百分比。这可能是由于自动编码模型（例如 m-BERT）共享编码空间并在最后几层中编码高级特征，而自回归模型（例如 m-GPT）逐渐细化每层的特征来预测下一个单词。&lt;/span&gt; (3)汉语和英语的知识神经元分布较为相似，但也存在差异。相似之处可能是由于事实具有相同的含义语言之间存在差异，而差异可能是由于语言之间固有的结构和句法差异或预训练语料库质量的差异造成的。 3.3 Language-Independence Neurons and Cross-Lingual Knowledge Editing语言无关神经元的定位通过我们对模块 2 的实验，我们捕获了图 3 中的结果。结果表明，无论是 m-BERT 还是 m-GPT，语言无关的知识神经元主要集中在最后一两个层。这可能是因为独立于语言的事实充当高级特征，而 PLM 只能在最后几层成功地对它们进行编码。跨语言知识编辑实验设置我们基于与语言无关的知识神经元设计跨语言编辑实验。与知识本地化实验的设置类似，我们抑制或者增强语言无关的知识神经元并计算编辑成功率SR。为了证明独立于语言的知识神经元的作用，我们设计了两个比较实验。（1）编辑一种语言的知识神经元，观察另一种语言相应事实的变化。 （2）依次编辑两种语言的知识神经元，观察两种语言对应事实的变化。跨语言知识编辑实验结果我们对表2的分析揭示了两个见解： （1）独立于语言的知识神经元促进跨语言编辑。与仅编辑中文或英文相比，编辑与语言无关的知识神经元在所有设置下都有更高的成功率；在中国数据集中，m-BERT 和 m-GPT 的成功率分别提高了 213.05% 和 277.36%。这意味着用一种语言编辑事实知识并期望其他语言发生相应变化的挑战；然而，利用独立于语言的知识神经元可以实现这一点。 （2） 单独编辑每种语言并不能保证获得更好的结果。尽管人们可以直观地编辑每种语言以实现跨语言的更改，但我们的实验表明，这种方法不仅依赖更多的计算资源，而且可能表现不佳。与使用语言无关神经元相比，顺序编辑导致 mBERT 和 m-GPT 的成功率分别降低 42.97% 和 58.80%，这可能是由于多次编辑造成的混乱。这强调了语言独立神经元的重要性。 3.4Degenerate Knowledge Neurons and Fact-Checking Experiment多语言PLM中简并知识神经元的识别我们使用模块3设置了一个实验来研究简并知识神经元，结果如图4所示。根据我们的观察，m-BERT和m-GPT中的简并知识神经元表现出分布模式类似于知识神经元。这不仅表明了简并性之间存在很强的相关性。单语言 PLM 中简并知识神经元的识别在我们的单语言 PLM 实验中，我们成功识别了简并知识神经元，并证明它们本质上存在于 PLM 中。关于简并知识神经元的一个可能的问题是：PLM 是否以多种语言存储相同的事实，从而利用多个神经元集来获取相同的信息？为了消除这种观念并证明简并知识神经元的存在与 PLM 中多语言的支持无关，我们将探索扩展到单语言 PLM，特别是 BERT 和 GPT-2。这些简并知识神经元的分布如图 5 所示，进一步证实了我们的结论。事实检查实验设置 PLM 可能会隐藏虚假事实（Edwards 2023；Pitt 2022），而当前的解决方案通常依赖外部数据进行事实检查（Vladika 和 Matthes 2023）。考虑到简并知识神经元功能重叠的性质，我们设计了一个事实检查实验，以在不依赖外部数据的情况下基于简并知识神经元检测错误事实。接下来，我们详细介绍我们的实验设置。 首先，mLAMA 数据集被修改以包含错误的事实属性。&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;对于与某个事实关系名称相关的事实三元组，例如 ⟨Tanzania, Capital, Dodoma⟩ ，我们从相同的关系名称中随机选择一个对象（例如，达累斯萨拉姆）作为错误事实。&lt;/span&gt;然后，为了验证我们的发现的实际意义，&lt;span style=&quot;background-color: #ff666680&quot;&gt;我们将数据集中的每种类型的查询按比例分为两部分。对于每种类型，第一段用于获取简并知识神经元，并识别数量超过 t% 特定阈值的神经元。随后，我们将第二部分中的查询以及相应的正确或错误事实作为输入并计算简并知识神经元的平均激活分数。如果平均激活分数超过预定义的阈值 λ，则该事实被分类为正确；否则，它被归类为 false。&lt;/span&gt;我们使用原始PLM直接评估事实的正确性进行比较分析。这种配置可以防止 PLM 使用查询本身的简并知识神经元进行事实检查，从而使实验更加令人信服。我们在表3中将我们的方法表示为“with DKN”。最后，由于当前的事实核查方法必须依赖于外部数据，因此我们使用PLM直接执行事实核查作为我们方法的基线，表示为“wo表 3 中的“DKN”。我们使用 Precision、Recall 和 F1-score 作为评估指标。 事实核查实验结果 表 3 中的结果使我们得出以下结论。 （1）退化的知识神经元可以帮助 PLM 检测错误的事实。在各种设置下，我们的方法比基线方法更好，特别是对于中国数据集和自回归模型。例如，在 m-GPT 和中文数据集的背景下，我们的方法的 F1 分数与基线相比增加了 167150%。这一实质性改进表明简并知识神经元的存在增强了 PLM 对事实知识的稳定掌握。 （2）使用PLM进行事实检查，他们经常判断一个事实是正确的，从而导致极高的召回率。这与观察结果一致，即如果提出错误的前提，生成语言模型可能会产生不正确的信息（Edwards 2022；Lakshmanan 2022；Metz 2022）。 （3）自回归模型比自编码模型表现出更高的召回率。这可能是由于自回归设计更注重一致性而不是准确性，并且自动编码在评估中可能更加保守（Zhou et al. 2023）。 (4)简并知识神经元的存在与PLM中多语言的支持无关。在单语言 PLM 中，即 BERT 和 GPT-2，事实检查也可以基于简并知识神经元进行。这一结果进一步证明了简并知识神经元的存在及其有用性。 4 Related Work知识定位现有的方法大致分为两类：（1）基于梯度的方法：Dai et al.（2022）首先引入了知识神经元的概念，并通过评估每个神经元的贡献来定位它们（​​Geva et al. 2021）使用积分梯度计算他们的归因得分。 （2）Causal-inspired方法，由Meng等人（2022a）提出，将知识神经元定义为PLM中对预测某些事实知识具有最强因果效应的神经元激活，该方法启发了知识编辑算法的创建例如 ROME（Meng 等人，2022a）、MEMIT（Meng 等人，2022b）和 MEND（Mitchell 等人，2022）。然而，当前的方法缺乏针对不同 PLM 架构和多种语言探索的通用方法。公理归因方法 Sundararajan、Taly 和 Yan（2017）介绍了公理归因方法，强调敏感性和实现不变性作为归因方法的核心公理，从而产生了积分梯度（IG）。后续研究包括Discretized IG (Sanyal and Ren 2021)，它使用插值策略来提高梯度精度； Sequential IG (Enguehard 2023) 专为单词重要性评估而设计；有效 Shapley 值以及 Shapley IG，由 Liu 等人 (2022) 开发，用于提高效率和效果。我们改进了 IG 的基线向量，以最大限度地减少其信息内容。 5 Conclusion 在这项研究中，我们使用适应架构的多语言集成梯度方法探索多语言 PLM 中的事实知识本地化。我们进一步设计了两个模块，导致了语言无关知识神经元和简并知识神经元的两个发现。前者肯定了多语言PLM中的一部分知识以超越语言的形式存在，而后者则引入了一种新型神经元，类似于生物系统中观察到的退化现象，这些神经元可以用来检测不正确的信息。事实。","tags":["EMNLP2023"],"categories":["NLP顶会"]},{"title":"算法笔记V2.0","path":"/2023/11/02/suan-fa-bi-ji-v2.0/","content":"第一章 基础算法 暴力的调整区间的方法： 双指针调整区间的方法： 1.1快速排序123456789101112void quick_sort(int q[], int l, int r)&#123; // 没有数或者只有一个数 if (l &gt;= r) return; int i = l - 1, j = r + 1, x = q[l + r &gt;&gt; 1]; while (i &lt; j)&#123; do i ++ ; while (q[i] &lt; x); do j -- ; while (q[j] &gt; x); if (i &lt; j) swap(q[i], q[j]); &#125; quick_sort(q, l, j), quick_sort(q, j + 1, r);&#125; 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstring&gt;using namespace std;const int N = 100;int n;int q[N];void quick_sort(int q[], int l, int r)&#123; // 没有数或者只有一个数 if (l &gt;= r) return; // 定义两个指针，定义区间分割点 x int i = l - 1, j = r + 1, x = q[l + r &gt;&gt; 1]; // 两个指针向中间靠 while (i &lt; j)&#123; // 找到第一个 q[i] &gt;= x do i ++ ; while (q[i] &lt; x); // 找到第一个 q[j] &lt;= x do j -- ; while (q[j] &gt; x); // 两个指针 i j 都找到了当前不符合的数的位置，则交换它们 if (i &lt; j) swap(q[i], q[j]); &#125; // 递归处理左右两段 quick_sort(q, l, j), quick_sort(q, j + 1, r);&#125;int main()&#123; cin &gt;&gt; n; for (int i = 0; i &lt; n; i ++ )&#123; cin &gt;&gt; q[i]; &#125; quick_sort(q, 0, n - 1); for (int i = 0; i &lt; n; i ++ ) cout &lt;&lt; q[i] &lt;&lt; &#x27; &#x27;; return 0;&#125; 1.2归并排序 1234567891011121314151617void merge_sort(int q[], int l, int r)&#123; if (l &gt;= r) return; int mid = l + r &gt;&gt; 1; merge_sort(q, l, mid); merge_sort(q, mid + 1, r); int k = 0, i = l, j = mid + 1; while (i &lt;= mid &amp;&amp; j &lt;= r) if (q[i] &lt;= q[j]) tmp[k ++ ] = q[i ++ ]; else tmp[k ++ ] = q[j ++ ]; while (i &lt;= mid) tmp[k ++ ] = q[i ++ ]; while (j &lt;= r) tmp[k ++ ] = q[j ++ ]; for (i = l, j = 0; i &lt;= r; i ++, j ++ ) q[i] = tmp[j];&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstring&gt;using namespace std;const int N = 100;int n;int q[N], tmp[N];void merge_sort(int q[], int l, int r)&#123; if (l &gt;= r) return; // 确定分界点 int mid = l + r &gt;&gt; 1; // 递归左右两边 merge_sort(q, l, mid); merge_sort(q, mid + 1, r); // 归并，需要格外数组 int k = 0, i = l, j = mid + 1; // k 代表已经合并了多少个数，i 指向左半边的起点，j 指向右半边的起点 while (i &lt;= mid &amp;&amp; j &lt;= r) if (q[i] &lt;= q[j]) tmp[k ++ ] = q[i ++ ]; else tmp[k ++ ] = q[j ++ ]; // 把剩余的数拿过来 while (i &lt;= mid) tmp[k ++ ] = q[i ++ ]; while (j &lt;= r) tmp[k ++ ] = q[j ++ ]; // 将结果复制回来 for (i = l, j = 0; i &lt;= r; i ++, j ++ ) q[i] = tmp[j];&#125;int main()&#123; cin &gt;&gt; n; for (int i = 0; i &lt; n; i ++ )&#123; cin &gt;&gt; q[i]; &#125; merge_sort(q, 0, n - 1); for (int i = 0; i &lt; n; i ++ ) cout &lt;&lt; q[i] &lt;&lt; &#x27; &#x27;; return 0;&#125; 1.3整数二分算法1.3.1模板1234567891011121314151617181920212223bool check(int x)&#123;\t/**/ // 检查x是否满足某种性质&#125;// 区间[l, r]被划分为[l, mid]和[mid + 1, r]时使用int bsearch_1(int l, int r)&#123; while (l &lt; r)&#123; int mid = l + r &gt;&gt; 1; if (check(mid)) r = mid; else l = mid + 1; &#125; return l;&#125;// 区间[l, r]被划分为[l, mid - 1]和[mid, r]时使用int bsearch_2(int l, int r)&#123; while (l &lt; r)&#123; int mid = l + r + 1 &gt;&gt; 1; if (check(mid)) l = mid; else r = mid - 1; &#125; return l;&#125; 技巧： 假设有一个总区间，经由我们的 check 函数判断后，可分成两部分，这边以o作 true，…..作 false 示意较好识别 如果我们的目标是下面这个v，那麽就必须使用模板 1 …………….vooooooooo 假设经由 check 划分后，整个区间的属性与目标v如下，则我们必须使用模板 2 oooooooov………………. 1.3.2例题 12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 100010;int n, m; // n数组中数的个数，m询问的个数int q[N];int main()&#123; scanf(&quot;%d%d&quot;, &amp;n, &amp;m); for (int i = 0; i &lt; n; i ++ ) scanf(&quot;%d&quot;, &amp;q[i]); while (m -- )&#123; int x; scanf(&quot;%d&quot;, &amp;x); int l = 0, r = n - 1; // 我们二分查找的是下标，而下标的范围是0~n-1 while (l &lt; r)&#123; int mid = l + r &gt;&gt; 1; if (q[mid] &gt;= x) r = mid; else l = mid + 1; &#125; if (q[l] != x) cout &lt;&lt; &quot;-1 -1&quot; &lt;&lt; endl; // 二分确定的不是答案，返回找不到 else&#123; cout &lt;&lt; l &lt;&lt; &#x27; &#x27;; // 找到了左端点，先输出即可 int l = 0, r = n - 1; // 再次初始化查找右端点 while (l &lt; r)&#123; int mid = r + l + 1 &gt;&gt; 1; if (q[mid] &lt;= x) l = mid; else r = mid - 1; &#125; cout &lt;&lt; r &lt;&lt; endl; &#125; &#125; return 0;&#125; 1.4浮点数二分算法1234567891011bool check(double x) &#123;/*...*/&#125; //检查x是否满足某种性质double bsearch_3(double l, double r)&#123; const double eps = 1e-6; // eps表示精度，取决于题目对精度的要求 while (r - l &gt; eps)&#123; double mid = (l + r) / 2; if (check(mid)) r = mid; else l = mid; &#125; return l;&#125; 1.5高精度加法12345678910111213141516// C = A + B, A &gt;= 0, B &gt;= 0vector&lt;int&gt; add(vector&lt;int&gt; &amp;A, vector&lt;int&gt; &amp;B)&#123; if (A.size() &lt; B.size()) return add(B, A); vector&lt;int&gt; C; int t = 0; for (int i = 0; i &lt; A.size(); i ++ )&#123; t += A[i]; if (i &lt; B.size()) t += B[i]; C.push_back(t % 10); t /= 10; &#125; if (t) C.push_back(t); return C;&#125; 1.6高精度减法1234567891011121314// C = A - B，满足A &gt;= B, A &gt;= 0, B &gt;= 0vector&lt;int&gt; sub(vector&lt;int&gt; &amp;A, vector&lt;int&gt; &amp;B)&#123; vector&lt;int&gt; C; for (int i = 0, t = 0; i &lt; A.size(); i ++ )&#123; t = A[i] - t; if (i &lt; B.size()) t -= B[i]; C.push_back((t + 10) % 10); if (t &lt; 0) t = 1; else t = 0; &#125; while (C.size() &gt; 1 &amp;&amp; C.back() == 0) C.pop_back(); return C;&#125; 1.7高精度乘以低精度1234567891011121314// C = A * b, A &gt;= 0, b &gt;= 0vector&lt;int&gt; mul(vector&lt;int&gt; &amp;A, int b)&#123; vector&lt;int&gt; C; int t = 0; for (int i = 0; i &lt; A.size() || t; i ++ )&#123; if (i &lt; A.size()) t += A[i] * b; C.push_back(t % 10); t /= 10; &#125; while (C.size() &gt; 1 &amp;&amp; C.back() == 0) C.pop_back(); return C;&#125; 1.8高精度除以低精度12345678910111213// A / b = C ... r, A &gt;= 0, b &gt; 0vector&lt;int&gt; div(vector&lt;int&gt; &amp;A, int b, int &amp;r)&#123; vector&lt;int&gt; C; r = 0; for (int i = A.size() - 1; i &gt;= 0; i -- )&#123; r = r * 10 + A[i]; C.push_back(r / b); r %= b; &#125; reverse(C.begin(), C.end()); while (C.size() &gt; 1 &amp;&amp; C.back() == 0) C.pop_back(); return C;&#125; 1.9一维前缀和下标从1开始，S0 = 0， 方便处理边界，如求[1, 10] = S10 - S0 = S10 - 0 = S10 原数组：a1, a2, …, an 前缀和数组：Si = a1 + a2 + … + ai 如何求Si 1for i = 1; i &lt;= n; s[i] = s[i - 1] + a[i]; Si的作用：快速求原数组中的一段数的和， 12345678910111213141516171819202122232425262728[l, r] = S[r] - S[l - 1];#include &lt;iostream&gt;using namespace std;const int N = 100010;int n, m;int a[N], S[N];int main()&#123; ios::sync_with_stdio(false); scanf(&quot;%d%d&quot;, &amp;n, &amp;m); for (int i = 0; i &lt;= n; i ++ ) scanf(&quot;%d&quot;, &amp;a[i]); // 前缀和初始化 for (int i = 1; i &lt;= n; i ++ ) S[i] = S[i - 1] + a[i]; // m个询问 while (m -- )&#123; int l, r; scanf(&quot;%d%d&quot;, &amp;l, &amp;r); printf(&quot;%d &quot;, s[r] - s[l - 1]); &#125; return 0;&#125; 12S[i] = a[1] + a[2] + ... a[i]a[l] + ... + a[r] = S[r] - S[l - 1] 1.10二维前缀和快速计算子矩阵的和，原矩阵是aij， Sij代表以Sij这个点为界限的左上角的矩阵的和 求内部子矩阵的和时，子矩阵的左上角为x1, y1, 右下角是x2, y2; w = S(x2, y2) - S(x2, y1 - 1) - S(x1 - 1, y2) + S(x1 - 1, y1 - 1) 123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 1010;int n, m; // 矩阵的长和宽int q; // q个询问int a[N][N], s[N][N];int main()&#123; scanf(&quot;%d%d&quot;, &amp;n, &amp;m); for (int i = 0; i &lt; n; i ++ ) for (int j = 0; j &lt; m; j ++ ) scanf(&quot;%d&quot;, &amp;a[i][j]); // 前缀和初始化 for (int i = 1; i &lt;= n; i ++ )&#123; for (int j = 1; j &lt;= m; j ++ )&#123; s[i][j] = s[i - 1][j] + s[i][j - 1] - s[i - 1][j - 1] + a[i][j]; &#125; while (m -- )&#123; int x1, y1, x2, y2; scanf(&quot;%d%d%d%d&quot;, &amp;x1, &amp;y1, &amp;x2, &amp;y2); printf(&quot;%d &quot;, s[x2][y2] - s[x1 - 1][y2] - s[x2][y1 - 1] + s[x1 - 1][y1 - 1]); &#125; return 0; &#125;&#125; 123S[i, j] = 第i行j列格子左上部分所有元素的和以(x1, y1)为左上角，(x2, y2)为右下角的子矩阵的和为：S[x2, y2] - S[x1 - 1, y2] - S[x2, y1 - 1] + S[x1 - 1, y1 - 1] 1.11一维差分原数组是a1, a2, …, an 构造b数组b1, b2, …, bn；使得ai = b1 + b2 + b3 + … + bi，使得a数组是b数组的前缀和 构造方法：b1 = a1; b2 = a2 - a1；b3 = a3 - a2; … ; bn = an - an - 1; b称为a的差分，a称为b的前缀和 对b求前缀和，就可以在O(n)的时间内求出a数组 差分主要用来快速处理这样一种操作：给定区间[l, r]， 对于这个区间的所有数全部加上C，用差分可以使用O(1)的时间完成这个操作；如果还想求出操作后的a数组，就可以扫描一遍b数组，然后求前缀和即可 12345678910111213141516171819202122232425262728293031323334353637#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 100010;int n, m;int a[N], b[N];// 区间[l, r] 内全部加上cvoid insert(int l, int r, int c)&#123; b[l] += c; b[r + 1] -= c;&#125;int main()&#123; sacnf(&quot;%d%d&quot;, &amp;n, &amp;m); for (int i = 1; i &lt;= n; i ++ ) scanf(&quot;%d&quot;, &amp;a[i]); for (int i = 1; i &lt;= n; i ++ ) insert(i, i, a[i]); // m个操作 while (m -- )&#123; int l, r, c; scanf(&quot;%d%d%d&quot;, &amp;l, &amp;r, &amp;c); insert(l, r, c); &#125; // 再求原来的数组 for (int i = 1; i &lt;= n; i ++ ) b[i] += b[i - 1]; for (int i = 1; i &lt;= n; i ++ ) printf(&quot;%d &quot;, b[i]); return 0;&#125; 1给区间[l, r]中的每个数加上c: B[l] += c, B[r + 1] -= c 1.12二维差分1.12.1模板12给以(x1, y1)为左上角，(x2, y2)为右下角的子矩阵中的所有元素加上cS[x1, y1] += c, S[x2 + 1, y1] -= c, S[x1, y2 + 1] -= c, S[x2 + 1, y2 + 1] += c 1.12.2差分矩阵 12345678910111213141516171819202122232425262728293031323334353637383940414243444546#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 1010;int n, m, q; // n矩阵长，m矩阵宽，q询问的个数int a[N][N], b[N][N];void insert(int x1, int y1, int x2, int y2, int c)&#123; b[x1][y1] += c; b[x2 + 1][y1] -= c; b[x1][y2 + 1] -= c; b[x2 + 1][y2 + 1] += c;&#125;int main()&#123; cin &gt;&gt; n &gt;&gt; m &gt;&gt; q; for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= m; j ++ ) scanf(&quot;%d&quot;, &amp;a[i][j]); for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= m; j ++ ) insert(i, j, i, j, a[i][j]); while (q -- )&#123; int x1, y1, x2, y2, c; cin &gt;&gt; x1 &gt;&gt; y1 &gt;&gt; x2 &gt;&gt; y2 &gt;&gt; c; insert(x1, y1, x2, y2, c); &#125; for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= m; j ++ ) b[i][j] += b[i - 1][j] + b[i][j - 1] - b[i - 1][j - 1]; for (int i = 1; i &lt;= n; i ++ )&#123; for (int j = 1; j &lt;= m; j ++ ) printf(&quot;%d &quot;, b[i][j]); puts(&quot;&quot;); &#125; return 0;&#125; 1.13位运算1.13.1模板12求n的第k位数字：n &gt;&gt; k &amp; 1返回n的最后一位1：lowbit(n) = n &amp; -n 1.13.2例题123456789101112131415#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int main()&#123; int n = 10; for (int k = 3; k &gt;= 0; k -- )&#123; cout &lt;&lt; ((n &gt;&gt; k) &amp; 1); &#125; return 0;&#125; 1.13.3Lowbit123456789101112131415161718192021222324252627282930313233// 返回x的最后一位1x = 1010, lowbit(x) = 10实现：x&amp;-xc++ 中，-x=~x+1x &amp; -x = x &amp; (~x + 1)作用：统计x中1的个数：每次把1去掉，最后返回即可#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt; using namespace std;int lowbit(int x)&#123; return x &amp; -x;&#125;int main()&#123; int m; cin &gt;&gt; m; while (m -- )&#123; int x; cin &gt;&gt; x; int res = 0; while (x) x -= lowbit(x), res ++ ; // 每次减去x的最后一位1 cout &lt;&lt; res &lt;&lt; &#x27; &#x27;; &#125; return 0;&#125; 1.14双指针算法1.14.1模板1234567for (int i = 0, j = 0; i &lt; n; i ++ )&#123; while (j &lt; i &amp;&amp; check(i, j)) j ++; // 具体问题的逻辑&#125;常见问题分类： （1）对于一个序列，用两个指针维护一段区间 （2）对于两个序列，维护某种次序，比如归并排序中合并两个有序序列的操作 1.14.2最长连续不重复子序列 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 100010;int n;int q[N], s[N];int main()&#123; scanf(&quot;%d&quot;, &amp;n); for (int i = 0; i &lt; n; i ++ ) scanf(&quot;%d&quot;, &amp;q[i]); int res = 0; for (int i = 0, j = 0; i &lt; n; i ++ )&#123; s[q[i]] ++ ; while (j &lt; i &amp;&amp; s[q[i]] &gt; 1) s[q[j ++ ]] -- ; res = max(res, i - j + 1); &#125; cout &lt;&lt; res &lt;&lt; endl; return 0;&#125; 1.14.3数组元素的目标和 1234567891011121314151617181920212223242526#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 1e5 + 10;int a[N], b[N], n, m, x;int main()&#123; scanf(&quot;%d%d%d&quot;, &amp;n, &amp;m, &amp;x); for (int i = 0; i &lt; n; i ++ ) scanf(&quot;%d&quot;, &amp;a[i]); for (int i = 0; i &lt; m; i ++ ) scanf(&quot;%d&quot;, &amp;b[i]); for (int i = 0, j = m - 1; i &lt; n; i ++ )&#123; // 因为保证一定有解 while (a[i] + b[j] &gt; x) j -- ; if (a[i] + b[j] == x)&#123; printf(&quot;%d %d &quot;, i, j); break; &#125; &#125; return 0;&#125; 1.15离散化1.15.1模板1234567891011121314vector&lt;int&gt; alls; // 存储所有待离散化的值sort(alls.begin(), alls.end()) // 将所有值进行排序alls.erase(unique(alls.begin(), alls.end()), alls.end()); // 去除重复元素// 二分求出x对应的离散化的值int find(int x)&#123; // 找到第一个大于等于x的值 int l = 0, r = alls.size() - 1; while (l &lt; r)&#123; int mid = l + r &gt;&gt; 1; if (alls[mid] &gt;= x) r = mid; else l = mid + 1; &#125; return r + 1; // 映射到1，2，...n&#125; 1.15.2区间和 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;typedef pair&lt;int, int&gt; PII;// 查询10万， 插入10万，查询需要两个坐标，插入需要1个坐标，共30万const int N = 300010;int n, m; // int a[N],s[N]; // 存的数 前缀和vector&lt;int&gt; alls; // 存需要离散化的值vector&lt;PII&gt; adds, query; // 定义两种操作，插入和查询// 求x离散化后的值int find(int x)&#123; int l = 0, r = alls.size() - 1; while (l &lt; r)&#123; int mid = l + r &gt;&gt; 1; if (alls[mid] &gt;= x) r = mid; // 找到的是大于x的最小的数 else l = mid + 1; &#125; return r + 1;&#125;int main()&#123; cin &gt;&gt; n &gt;&gt; m; for (int i = 0; i &lt; n; i ++ )&#123; int x, c; cin &gt;&gt; x &gt;&gt; c; adds.push_back(&#123;x, c&#125;); alls.push_back(x); // 加入到待离散化的数组中 &#125; for (int i = 0; i &lt; m; i ++ )&#123; int l, r; cin &gt;&gt; l &gt;&gt; r; query.push_back(&#123;l, r&#125;); alls.push_back(l); // 查询的端点区间也需要离散化 alls.push_back(r); &#125; // 去重 sort(alls.begin(), alls.end()); alls.erase(unique(alls.begin(), alls.end()), alls.end()); // 遍历所有操作 for (auto item : add)&#123; int x = find(item.first); a[x] += item.second; &#125; // 预处理前缀和 for (int i = 1; i &lt;= alls.size(); i ++ ) s[i] = s[i - 1] + a[i]; // 处理询问 for (auto item : query)&#123; int l = find(item.first), r = find(item.second); cout &lt;&lt; s[r] - s[l - 1] &lt;&lt; endl; &#125; return 0;&#125; 1.16区间合并1.16.1模板 123456789101112131415161718// 将所有存在交集的区间合并void merge(vector&lt;PII&gt; &amp;segs)&#123; vector&lt;PII&gt; res; sort(segs.begin(), segs.end()); int st = -2e9, ed = -2e9; for (auto seg : segs)&#123; if (ed &lt; seg.first)&#123; if (st != -2e9) res.push_back(&#123;st, ed&#125;); st = seg.first, ed = seg.second; &#125;else ed = max(ed, seg.second); &#125; if (st != -2e9) res.push_back(&#123;st, ed&#125;); segs = res;&#125; 1.16.2区间合并题目 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;uisng namespace std;typedef pair&lt;int, int&gt; PII;const int N = 100010;int n;vector&lt;PII&gt; segs;void merge(vector&lt;PII&gt; &amp; segs)&#123; vector&lt;PII&gt; res; sort(segs.begin(), segs.end()); int st = -2e9, ed = -2e9; for(auto seg : segs)&#123; if (ed &lt; seg.first)&#123; // 维护的区间严格在枚举的区间的左边 if (st != -2e9) res.push_back(&#123;st, ed&#125;); // 不存在交集，直接加到答案里面去 st = seg.first, ed = seg.second; // 更新当前所维护的区间 &#125;else ed = max(ed, seg.second); // 有交集，那么更新右端点即可 &#125; if (st != -2e9) res.push_back(&#123;st, ed&#125;); // 防止一开始segs里面是空的 segs = res;&#125;int main()&#123; cin &gt;&gt; n; for (int i = 0; i &lt; n; i ++ )&#123; int l, r; cin &gt;&gt; l &gt;&gt; r; segs.push_back(&#123;l, r&#125;); &#125; merge(segs); cout &lt;&lt; segs.size() &lt;&lt; endl; return 0;&#125; 第二章 数据结构2.1单链表123456789101112131415161718// head存储链表头，e[]存储节点的值，ne[]存储节点的next指针，idx表示当前用到了哪个节点int head, e[N], ne[N], idx;// 初始化void init()&#123; head = -1; idx = 0;&#125;// 在链表头插入一个数avoid insert(int a)&#123; e[idx] = a, ne[idx] = head, head = idx ++ ;&#125;// 将头结点删除，需要保证头节点存在void remove()&#123; head = ne[head];&#125; 2.2双链表12345678910111213141516171819202122// e[]表示节点的值，l[]表示节点的左指针，r[]表示节点的右指针，idx表示当前用到了哪个点int e[N], l[N], r[N], idx;// 初始化void init()&#123; // 0是左端点，1是右端点 r[0] = 1, l[1] = 0; idx = 2;&#125;// 在节点a的右边插入一个数xvoid insert(int a, int x)&#123; e[idx] = x; l[idx] = a, r[idx] = r[a]; l[r[a]] = idx, r[a] = idx ++ ;&#125;// 删除节点void remove(int a)&#123; l[r[a]] = l[a]; r[l[a]] = r[a];&#125; 2.3栈12345678910111213141516// tt 表示栈顶int stk[N], tt = 0;// 向栈顶插入一个数stk[ ++ tt] = x;// 从栈顶弹出一个数tt -- ;// 栈顶的值stk[tt];// 判断栈是否为空，如果 tt &gt; 0 , 则表示不为空if (tt &gt; 0)&#123; &#125; 2.4队列2.4.1普通队列12345678910111213// hh 表示队头，tt表示队尾int q[N], hh = 0, tt = -1;// 向队尾插入一个数q[ ++ tt] = x;// 队头的值q[hh];// 判断队列是否为空，如果hh &lt;= tt, 则表示不为空if (hh &lt;= tt)&#123; &#125; 2.4.2循环队列123456789101112131415161718// hh表示队头，tt表示队尾的后一个位置int q[N], hh = 0, tt = 0;// 向队尾插入一个数q[tt ++ ] = x;if (tt == N) tt = 0;// 从队头弹出一个数hh ++ ;if (hh == N) hh = 0;// 队头的值q[hh];// 判断队列是否为空，如果hh != tt, 则表示不为空if (hh != tt)&#123; &#125; 2.5单调栈123456常见模型：找出每个数左边离他最近的比他大/小的数int tt = 0;for (int i = 1; i &lt;= n; i ++ )&#123; while (tt &amp;&amp; check(stk[tt], i)) tt -- ; stk[ ++ tt] = i;&#125; 2.6单调队列1234567常见模型：找出滑动窗口中的最大值/最小值int hh = 0, tt = -1;for (int i = 0; i &lt; n; i ++ )&#123; while (hh &lt;= tt &amp;&amp; check_out(q[hh])) hh ++ ; // 判断队头是否滑出窗口 while (hh &lt;= tt &amp;&amp; check(q[tt]), i) tt -- ; q[ ++ tt] = i;&#125; 2.7KMP2.7.1模板 1234567891011121314151617// s[]是长文本，p[]是模式串，n是s的长度，m是p的长度求模式串的Next数组for (int i = 2, j = 0; i &lt;= m; i ++ )&#123; while (j &amp;&amp; p[i] != p[j + 1]) j = ne[j]; if (p[i] == p[j + 1]) j ++ ; ne[i] = j;&#125;// 匹配for (int i = 1, j = 0; i &lt;= n; i ++ )&#123; while (j &amp;&amp; s[i] != p[j + 1]) j = ne[i]; if (s[i] == p[j + 1]) j ++ ; if (j == m)&#123; j = ne[j]; // 匹配之后的逻辑 &#125;&#125; 2.7.2KMP字符串 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;using namespace std;const int N = 10010, M = 100010;int n, m;char p[N], s[M];int ne[N];int main()&#123; cin &gt;&gt; n &gt;&gt; p + 1 &gt;&gt; m &gt;&gt; s + 1; // 求next数组 for (int i = 2, j = 0; i &lt;= n; i ++ )&#123; while (j &amp;&amp; p[i] != p[j + 1]) j = ne[j]; if (p[i] == p[j + 1]) j ++ ; ne[i] = j; &#125; // KMP匹配过程 for (int i = 1, j = 0; i &lt;= m; i ++ )&#123; //i j错开一位，i是j + 1进行匹配的 while (j &amp;&amp; s[i] != p[j + 1]) j = ne[j]; // 如果没有返回起点，并且i和j + q不匹配的了，那就往前退 if (s[i] == p[j + 1]) j ++ ; // 匹配成功了，移到下一位 if (j == n)&#123; printf(&quot;%d &quot;, i - n); j = ne[j]; // 匹配到最后一位想要继续匹配 &#125; &#125; return 0;&#125; 2.8Trie树1234567891011121314151617181920212223242526int son[N][26], cnt[N], idx;// 0号点既是根结点，又是空节点// son[][]存储树中每个节点的子节点// cnr[]存储每个节点结尾的单词数量// 插入一个字符串void insert(char *str)&#123; int p = 0; for (int i = 0; str[i]; i ++ )&#123; int u = str[i] - &#x27;a&#x27;; if (!son[p][u]) son[p][u] = ++ idx; p = son[p][u]; &#125; cnt[p] ++ ;&#125;// 查询字符串出现的次数int query(char *str)&#123; int p = 0; for (int i = 0; str[i]; i ++ )&#123; int u = str[i] - &#x27;a&#x27;; if (!son[p][u]) return 0; p = son[p][u]; &#125; return cnt[p];&#125; 2.9并查集2.9.1模板 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758（1）朴素并查集：int p[N]; // 存储每个节点的祖宗节点// 返回x的祖宗节点int find(int x)&#123; if (p[x] != x) p[x] = find(p[x]); return p[x];&#125;// 初始化，假定节点编号是1~nfor (int i = 1; i &lt;= n; i ++ ) p[i] = i;//合并a和b所在的两个集合p[find(a)] = find(b);（2）维护size的并查集int p[N], size[N];// p[]存储每个点的祖宗节点，size[]只有祖宗节点的有意义// 返回x的祖宗节点int find(int x)&#123; if (p[x] != x) p[x] = find(p[x]); return p[x];&#125;// 初始化，假定节点编号是1~nfor (int i = 1; i &lt;= n; i ++ )&#123; p[i] = i; size[i] = 1;&#125;// 合并a和b所在的两个集合size[find(b)] += size[find(a)];p[find(a)] = find(b);（3）维护到祖宗节点距离的并查集：int p[N], d[N];// p[]存储每个点的祖宗节点，d[x]存储x到p[x]的距离// 返回x的祖宗节点int find(int x)&#123; if (p[x] != x)&#123; int u = find(p[x]); d[x] += d[p[x]]; p[x] = u; &#125; return p[x];&#125;// 初始化，假定节点编号是1~nfor (int i = 1; i &lt;= n; i ++ )&#123; p[i] = i; d[i] = 0;&#125;// 合并a和b所在的两个集合p[find(a)] = find(b);d[find(a)] = distance; // 根据具体问题，初始化find(a)的便宜量 2.9.2合并集合 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;cstirng&gt;#include &lt;algorithm&gt;using namespace std;const int N = 100010;int n, m; // 点的数量和操作的数量int p[N];int find(int x)&#123; // 返回x的祖宗节点+路径压缩 if (p[x] != x) p[x] = find(p[x]); return p[x];&#125;int main()&#123; scanf(&quot;%d%d&quot;, &amp;n, &amp;m); for (int i = 1; i &lt;= n; i ++ ) p[i] = i; while (m -- )&#123; char op[2]; int a, b; scanf(&quot;%s%d%d&quot;, op, &amp;a, &amp;b); if (op[0] == &#x27;M&#x27;) p[find(a)] = find(b); else&#123; if (find(a) == find(b)) puts(&quot;Yes&quot;); else puts(&quot;No&quot;); &#125; &#125; return 0;&#125; 2.10堆12345678910111213141516171819202122232425262728293031// h[N]存储堆中的值，h[1]是堆顶，x的左儿子是2x，右儿子是2x+1// ph[k]存储第k个插入的点在堆中的位置// hp[k]存储堆中下标是k的点是第几个插入的int h[N], ph[N], hp[N], size;// 交换两个点，及其映射关系void heap_swap(int a, int b)&#123; swap(ph[hp[a]], ph[hp[b]]); swap(hp[a], hp[b]); swap(h[a], h[b]);&#125;void down(int u)&#123; int t = u; if (u * 2 &lt;= size &amp;&amp; h[u * 2] &lt; h[t]) t = u * 2; if (u * 2 + 1 &lt;= size &amp;&amp; h[u * 2 + 1] &lt; h[t]) t = u * 2 + 1; if (u != t)&#123; heap_swap(u, t); down(t); &#125;&#125;void up(int u)&#123; while (u / 2 &amp;&amp; h[u] &lt; h[u / 2])&#123; heap_swap(u, u / 2); u &gt;&gt;= 1; &#125;&#125;// O(n)建堆for (int i = n / 2; i; i -- ) down(i); 2.11一般哈希2.11.1模板12345678910111213141516171819202122232425262728293031323334（1）拉链法int h[N], e[N], ne[N], idx;// 向哈希表中插入一个数void insert(int x)&#123; int k = (x % N + N) % N; e[idx] = x; ne[idx] = h[k]; h[k] = idx ++ ;&#125;// 在哈希表中查询某个数是否存在bool find(int x)&#123; int k = (x % N + N) % N; for (i = h[k]; i != -1; i = ne[i])&#123; if (e[i] == x) return true; &#125; return false;&#125;（2）开放寻址法int h[N];// 如果x在哈希表中，返回x的下标；如果x不在哈希表中，返回应该插入的位置int find(int x)&#123; int t = (x % N + N) % N; while (h[t] != null &amp;&amp; h[t] != x)&#123; t ++ ; if (t == N) t = 0; &#125; return t;&#125; 2.11.2模拟散列表 123456789101112131415161718192021222324252627282930313233343536373839404142434445// 拉链法#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;const int N = 10010;int h[N], e[N], ne[N], idx;void insert(int x)&#123; int k = (x % N + N) % N; memset(h, -1, sizeof h); e[idx] = x, ne[idx] = h[k], h[k] = idx ++;&#125;bool find(int x)&#123; int k = (x % N + N) % N; for (int i = h[k]; i != -1; i = ne[i])&#123; if (e[i] == x) return true; &#125;&#125;int main()&#123; int n; scanf(&quot;%d&quot;, &amp;n); while (n -- )&#123; char op[2]; int x; scanf(&quot;%s%d&quot;, op, &amp;x); if (*op == &#x27;I&#x27;) insert(x); else&#123; if (find(x)) puts(&quot;Yes&quot;); else puts(&quot;No&quot;); &#125; &#125; return 0;&#125; 2.12字符串哈希12345678910111213141516核心思想：将字符串看成p进制数，p的经验值是131或13331， 取这两个值冲突概率较低小技巧：取模的数用2^64，这样直接用unsigned long long存储，溢出的结果就是取模的结果typedef unsigned long long ULL;ULL h[N], p[N]; // h[k]存储字符串前k个字母的哈希值，p[k]存储p^k mod 2^64// 初始化p[0] = 1;for (int i = 1; i &lt;= n; i ++ )&#123; h[i] = h[i - 1] * p + str[i]; p[i] = p[i - 1] * p;&#125;// 计算字串str[1 ~ r]的哈希值ULL get(int l, int r)&#123; return h[r] - h[l - 1] * p[r - l + 1];&#125; 2.13C ++ STL简介123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100vector, 边长数组，倍增的思想 size() 返回元素的个数 empty() 返回是否为空 clear() 清空 front()/back() push_back()/pop_back() begin()/end()vector&lt;int&gt;::iterator it;for (it = vec.begin(); it != vec.end(); it ++ ) cout &lt;&lt; *it &lt;&lt; endl;pair&lt;int, int&gt; first 第一个元素 second 第二个元素 支持比较运算，以first为第一关键字，以second为第二关键字（字典序） string 字符串 size()/length() 返回字符串的长度 empty() clear() substr(起始下标，(子串长度)) 返回字串 c_str() 返回字符串数组的起始地址 queue 队列 size() empty() push() 向队尾处插入一个元素 front() 返回队尾元素 pop() 弹出队头元素 priority_queue 优先队列，默认是大根堆 size() empty() push() top() pop() 定义成小根堆的方式：priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt;&gt; q;stack 栈 size() empty() push() 向栈顶插入一个元素 top() 返回栈顶元素 pop() 弹出栈顶元素 dequeue 双端队列 size() empty() clear() front()/back() push_back()/pop_back() push_front()/pop_front() begin()/end() []set, map, multiset, multimap, 基于平衡二叉树（红黑树），动态维护有序序列 size() empty() clear() begin()/end() ++, -- 返回前驱和后继，时间复杂度O(logn) set/multiset insert() 插入一个数 find() 查找一个数 count() 返回某一个数的个数 erase() （1）输入是一个数x，删除所有x O(k + logn) （2）输入一个迭代器，删除这个迭代器 lower_bound()/upper_bound() lower_bound(x) 返回大于等于x的最小的数的迭代器 upper_bound(x) 返回大于x的最小的数的迭代器map/multimap insert() // 插入的数是一个pair erase() 输入的参数是pair或者迭代器 find() [] 注意multimap不支持此操作。时间复杂度是O(logn) lower_bound()/upper_bound() unordered_set, unordered_map, unordered_multiset, unordered_multimap，哈希表 和上面的类似，增删改查的时间复杂度是O(1) 不支持lower_bound()/upper_bound(), 迭代器的++， -- bitset 压位 bitset&lt;10000&gt; s;\t~, &amp;, |, ^ &gt;&gt;, &lt;&lt; ==, != [] count() 返回有多少个1 any() 判断是否至少有一个1 none() 判断是否全为0 set() 把所有位置为1 set(k, v) 将第k位变为v reset() 把所有位变为0 flip() 等价于~ flip(k) 把第k位取反 第三章 搜索与图论 3.1树与图的存储树是一种特殊的图，与图的存储方式相同。 对于无向图中的边ab，存储两条有向边a-&gt;b, b-&gt;a。 因此我们可以只考虑有向图的存储。 （1）邻接矩阵： 1g[a][b] 存储边a-&gt;b （2）邻接表： 1234567891011// 对于每个点k, 开一个单链表，存储k所有可以走到的点。h[k]存储这个单链表的头结点int h[N], e[N], ne[N], idx;// 添加一条边a-&gt;bvoid add(int a, int b)&#123;\te[idx] = b, ne[idx] = h[a], h[a] = idx ++ ;&#125;// 初始化idx = 0;memset(h, -1, sizeof h); 3.2树与图的遍历 时间复杂度O(n + m)， n表示点数，m表示边数 3.2.1深度优先遍历12345678int dfs(int u)&#123; st[u] = true; // st[u] 表示点u已经被遍历过 for (int i = h[u]; i != -1; i = ne[i])&#123; int j = e[i]; if (!st[j]) dfs(j); &#125;&#125; 3.2.1.1树的重心： 思路是DFS。从任意一个点出发进行DFS，同时计算如果删去之，所得剩余最大连通块节点个数。先搜集其所有未访问的子树的节点个数，每次搜集到一棵子树之后，就得到了该子树的节点个数（”当前“的意思是如果删去当前节点，所得最大连通块节点个数），搜集完所有子树之后，再用总结点个数减去所有子树节点和再减去当前节点自己，就得到了当前节点父亲的那棵子树节点个数，这样就得到了最大子树的节点个数。如果其小于等于 ⌊ n / 2 ⌋，说明当前点是重心（证明参考https://blog.csdn.net/qq_46105170/article/details/125841504），答案已经求出，将这个信息返回给上一层；否则返回当前节点子树的节点个数给上一层。 在深度优先遍历的过程中可以求出每个点的子树的点的数量 而当前节点上面的连通块的点的数量就是总的点数n减去当前点所有子树的点之和后得到的数 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;const int N = 100010, M = 2 * N;int n, m;int h[N], e[M], ne[M], idx;bool st[N];int ans = N;void add(int a, int b) &#123;e[idx] = b, ne[idx] = h[a], h[a] = idx ++; &#125;// 以u为根的子树的点的数量int dfs(int u)&#123;\tst[u] = true; int sum = 1, res = 0; // 当前点的数量，每一个连通块点数量的最大值 for (int i = h[u]; i != -1; i = ne[i])&#123; int j = e[i]; if (!st[j])&#123; int s = dfs(j); // 当前子树的大小 res = max(res, s); // 当前子树也相当于一个连通块，需要比较 sum += s; // 当前子树是以u为根的子树的点的数量的一部分，需要加和 &#125; &#125; // 外层的那个剩余的部分 res = max(res, n - sum); ans = min(ans, res); return sum;&#125;int main()&#123; cin &gt;&gt; n &gt;&gt; m; memset(h, -1, sizeof h); for (int i = 0; i &lt; n - 1; i ++ )&#123; int a, b; cin &gt;&gt; a &gt;&gt; b; add(a, b), add(b, a); &#125; dfs(1); printf(&quot;%d &quot;, ans); return 0;&#125; 3.2.1.2排列数字 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;using namespace std;const int N = 10;int n;int path[N];bool st[N];void dfs(int u)&#123; if (u == n)&#123; for (int i = 0; i &lt; n; i ++ ) printf(&quot;%d&quot;, path[i]); puts(&quot;&quot;); return; &#125; for (int i = 1; i &lt;= n; i ++ )&#123; if (!st[i])&#123; path[u] = i; st[i] = true; dfs(u + 1); path[u] = 0; st[i] = false; &#125; &#125;&#125;int main()&#123; cin &gt;&gt; n; dfs(0); return 0;&#125; 3.2.1.3n-皇后问题（全排列搜索顺序） 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;using namespace std;const int N = 20;int n;char g[N][N]; // 一种方案bool col[N], dg[N], udg[N]; // 行 对角线 反对角线void dfs(int u)&#123; if (u == n)&#123; for (int i = 0; i &lt; n; i ++ ) puts(g[i]); puts(&quot;&quot;); return; &#125; for (int i = 0; i &lt; n; i ++ )&#123; if (!col[i] &amp;&amp; !dg[u + i] &amp;&amp; !udg[n - u + i])&#123; g[u][i] = &#x27;Q&#x27;; col[i] = dg[u + i] = udg[n - u + i] = true; dfs(u + 1); col[i] = dg[u + i] = udg[n - u + i] = false; g[u][i] = &#x27;.&#x27;; &#125; &#125;&#125;int main()&#123; cin &gt;&gt; n; for (int i = 0; i &lt; n; i ++ ) for (int j = 0; j &lt; n; j ++ ) g[i][j] = &#x27;.&#x27;; dfs(0); return 0;&#125; 3.2.1.4n-皇后问题（逐个格子进行枚举搜索） 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647#include &lt;iostream&gt;using namespace std;const int N = 10;int n;char g[N][N];bool row[N], col[N], dg[N], udg[N];void dfs(int x, int y, int s)&#123; if (y == n) y = 0, x ++ ; // 当前行的格子全部枚举完了，横坐标置为0，纵坐标跳到下一行 if (x == n)&#123; // 全部格子枚举完了 if (s == n)&#123; // 皇后的个数等于n了，找到了一组解 for (inr i = 0; i &lt; n; i ++ ) puts[g[i]]; puts(&quot;&quot;); &#125; return; &#125; // 每个格子有下面两种处理方式 // 不放皇后 dfs(x, y + 1, s); // 不放皇后，递归到下一个格子 // 放皇后，需要判断:这一行没有、这一列没有、对角线没有、反对角线也没有皇后，才可以放 if (!row[x] &amp;&amp; !col[y] &amp;&amp; !dg[x + y] &amp;&amp; !udg[x - y + n])&#123; g[x][y] = &#x27;Q&#x27;; row[x] = col[y] = dg[x + y] = udg[x - y + n] = true; dfs(x, y + 1, s + 1); row[x] = col[y] = dg[x + y] = udg[x - y + n] = false; g[x][y] = &#x27;.&#x27;; &#125;&#125;int main()&#123; cin &gt;&gt; n; for (int i = 0; i &lt; n; i ++ ) for (int j = 0; j &lt; n; j ++ ) g[i][j] = &#x27;.&#x27;; dfs(0, 0, 0); return 0;&#125; 3.2.2宽度优先遍历3.2.2.1模板 12345678910111213141516queue&lt;int&gt; q;st[1] = true; // 表示1号点已经被遍历过q.push(1);while (q.size())&#123; int t = q.front(); q.pop(); for (int i = h[t]; i != -1; i = ne[i])&#123; int j = e[i]; if (!st[j])&#123; st[j] = true; // 表示点j已经被遍历过 q.push(j); &#125; &#125;&#125; 3.2.2.2图中点的层次 本题是图的存储+BFS的结合 图的存储用邻接表 图的权值是1的时候，重边和环不用考虑。 所有长度都是1，表示可以用BFS来求最短路，否则应该用迪杰斯特拉等算法来求图中的最短路径。 BFS需要记录的是出发点到当前点的距离，就是d数组，每次d要增加1。 一定要注意数组的初始化！！！！ 12memset(h, -1, sizeof h); //数组的整体初始化为-1，这是链表结束循环的边界，缺少会TLEmemset(d,-1,sizeof d); //表示没有走过。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;using namespace std;const int N = 10010, M = N &lt;&lt; 1;int n, m;int h[N], e[M], ne[M], idx;int d[N];void add(int a, int b)&#123;\te[idx] = b, ne[idx] = h[a], h[a] = idx ++ ;&#125;int bfs()&#123;\tqueue&lt;int&gt; q;\tq.push(1); memset(d, -1, sizeof d);\td[1] = 0; while (q.size())&#123; auto t = q.front(); q.pop(); for (int i = h[t]; i != -1; i = ne[i])&#123; int j = e[i]; if (d[j] == -1)&#123; d[j] = d[t] + 1; q.push(j); &#125; &#125;\t&#125; return d[n];&#125;int main()&#123;\tmemset(h, -1, sizeof h); cin &gt;&gt; n &gt;&gt; m; for (int i = 1; i &lt;= m; i ++ )&#123; int a, b; cin &gt;&gt; a &gt;&gt; b; add(a, b);\t&#125; cout &lt;&lt; bfs() &lt;&lt; endl; return 0;&#125; 3.2.2.3走迷宫 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;typedef pair&lt;int, int&gt; PII;const int N = 110;int n, m;int g[N][N]; // 存储迷宫int d[N][N]; // 存储每个点到起始点的距离PII q[N * N], Prev[N][N]; // 队列int bfs()&#123; int hh = 0, tt = 0; // 队头hh 队尾tt q[0] = &#123;0, 0&#125;; // 把起始点加入队列准备扩展 memset(d, -1, sizeof d); // 表示没有走过 d[0][0] = 0; // 代表【0，0】已经走过并且距离起始点的距离是0 int dx[4] = &#123;-1, 0, 1, 0&#125;, dy[4] = &#123;0, 1, 0, -1&#125;; while (hh &lt;= tt)&#123; auto t = q[hh ++ ]; for (int i = 0; i &lt; 4; i ++ )&#123; int x = t.first + dx[i], y = t.second + dy[i]; if (x &gt;= 0 &amp;&amp; x &lt; n &amp;&amp; y &gt;= 0 &amp;&amp; y &lt; m &amp;&amp; g[x][y] == 0 &amp;&amp; d[x][y] == -1)&#123; d[x][y] = d[t.first][t.second] + 1; Prev[x][y] = t; q[ ++ tt] = &#123;x, y&#125;; &#125; &#125; &#125; // 使用Prev数组往前推 int x = n - 1, y = m - 1; while (x || y)&#123; // x y不同时为0，也就是不是起始点的时候 cout &lt;&lt; x &lt;&lt; &#x27; &#x27; &lt;&lt; y &lt;&lt; endl; auto t = Prev[x][y]; x = t.first, y = t.second; &#125; return d[n - 1][m - 1];&#125;int main()&#123; cin &gt;&gt; n &gt;&gt; m; for (int i = 0; i &lt; n; i ++ ) for (int j = 0; j &lt; m ; j ++ ) cin &gt;&gt; g[i][j]; cout &lt;&lt; bfs() &lt;&lt; endl; return 0;&#125; 3.3完全二叉树 1234567891011121314151617181920212223242526272829303132333435363738#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 10000 + 10;int n;long long q[N];int main()&#123; cin &gt;&gt; n; for (int i = 1; i &lt;= n; i ++ ) cin &gt;&gt; q[i]; int max_ = -1e18; /* 完全二叉树 每层的开头为 2 ^ (n - 1)结尾则是2^n - 1 计算每层的数值只需要两个positioner分别指向开头和结尾 */ int depth = 1; int res = 1; for (int i = 1; i &lt;= n; i *= 2)&#123; long long s = 0; for (int j = i; j &lt;= i * 2 - 1 &amp;&amp; j &lt;= n; j ++ )&#123; s += q[j]; &#125; if (s &gt; max_)&#123; max_ = s; res = depth; &#125; depth ++ ; &#125; cout &lt;&lt; res; return 0;&#125; 3.4堆优化版Dijkstra1234567891011121314151617181920212223242526272829303132333435363738// 时间复杂度O(mlogn) , n表示点数，m表示边数typedef pair&lt;int, int&gt; PII;int n; // 点的数量int h[N], w[N], e[N], ne[N], idx; // 邻接表存储所有边int dist[N]; // 存储所有点到1号的距离bool st[N]; // 存储每个点的最短距离是否已经确定// 求1号点到n号点的最短距离，如果不存在，则返回-1int dijkstra()&#123; memset(dist, 0x3f, sizeof dist); dist[1] = 0; priority_queue&lt;PII, vector&lt;PII&gt;, greater&lt;PII&gt;&gt; heap; heap.push(&#123;0, 1&#125;); // firatu存储距离，second存储节点的编号 while (heap.size())&#123; auto t = heap.top(); heep.pop(); int ver = t.second, distance = t.first; if (st[ver]) continue; st[ver] = true; for (int i = h[ver]; i != -1; i = ne[i])&#123; int j = e[i]; if (dist[j] &gt; distance + w[i])&#123; dist[j] = distance + w[i]; heap.push(&#123;dist[j], j&#125;); &#125; &#125; &#125; if (dist[n] == 0x3f3f3f3f) return -1; return dist[n];&#125; 3.4.4Dijkstra求最短路（朴素） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 510;int n, m;int g[N][N];int dist[N];bool st[N];int dijkstra()&#123; memset(dist, 0x3f, sizeof dist); dist[1] = 0; for (int i = 0; i &lt; n; i ++ )&#123; int t = -1; for (int j = 1; j &lt;= n; j ++ )&#123; if (!st[j] &amp;&amp; (t == -1 || dist[t] &gt; dist[j])) t = j; &#125; st[t] = true; for (int j = 1; j &lt;= n; j ++ ) dist[j] = min(dist[j], dist[t] + g[t][j]); &#125; if (dist[n] == 0x3f3f3f3f) return -1; return dist[n];&#125;int main()&#123; scanf(&quot;%d%d&quot;, &amp;n, &amp;m); memset(g, 0x3f, sizeof g); while (m -- )&#123; int a, b, c; scanf(&quot;%d%d%d&quot;, &amp;a, &amp;b, &amp;c); g[a][b] = min(g[a][b], c); &#125; int t = dijkstra(); printf(&quot;%d &quot;, t); return 0;&#125; 3.4.5Dijkstra求最短路（堆优化） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;#include &lt;queue&gt;using namespace std;typedef pair&lt;int, int&gt; PII;const int N = 100010;int n, m;int h[N], e[N], w[N], ne[N], idx;int dist[N];bool st[N];void add(int a, int b, int c)&#123; e[idx] = b, w[idx] = c, ne[idx] = h[a], h[a] = idx ++ ;&#125;int dijkstra()&#123; memset(dist, 0x3f, sizeof dist); dist[1] = 0; // 到1号点的距离是0 priority_queue&lt;PII, vector&lt;PII&gt;, greater&lt;PII&gt;&gt; heap; heap.push(&#123;0, 1&#125;); // 1号点的距离已经知道了，我们需要用它来更新其他点，距离是0，编号是1 while (heap.size())&#123; auto = heap.top(); // 每次找到距离最短的点 heap.pop(); int ver = t.second, distance = t.first; if (st[ver]) continue; // 这个点已经出现过，是一个冗余备份，我们跳过即可 for (int i = h[ver]; i != -1; i = ne[i])&#123; int j = e[i]; // j存储当前点的编号 if (dist[j] &gt; distance + w[i])&#123; dist[j] = distance + w[i]; heap.push(&#123;dist[j], j&#125;); &#125; &#125; &#125; if (dist[n] == 0x3f3f3f3f) return -1; return dist[n];&#125;int main()&#123; memset(h, -1, sizeof h); scanf(&quot;%d%d&quot;, &amp;n, &amp;m); while (m -- )&#123; int a, b, c; scanf(&quot;%d%d%d&quot;, &amp;a, &amp;b, &amp;c); add(a, b, c); &#125; int t = dijkstra(); printf(&quot;%d &quot;, t); return 0;&#125; 3.5Bellman-Ford算法 1234567891011121314151617181920212223242526// 时间复杂度O(nm)，n表示点数，m表示边数int n, m;int dist[N]; // dist[x] 存储1到x的最短路距离struct Edge&#123; // 边， a表示出点，b表示入点，w表示边权 int a, b, w;&#125;edges[M];// 求1到n的最短路距离，如果无法从1走到n，则返回-1int bellman_ford()&#123; memset(dist, 0x3f, sizeof dist); dist[1] = 0; // 如果第n次迭代任然会松弛三角不等式，就说明存在一条长度是n + 1的最短路径，由抽屉原理，路径中至少存在两个相同的点， 说明图中存在负权回路。 for (int i = 0; i &lt; n; i ++ )&#123; for (int j = 0; j &lt; m; j ++ )&#123; int a = edges[j].a, b = edges[j].b, w = edges[j].w; if (dist[b] &gt; dist[a] + w)&#123; dist[b] = dist[a] + w; &#125; &#125; &#125; if (dist[n] &gt; 0x3f3f3f3f / 2) return -1; return dist[n];&#125; 3.5.1有边数限制的最短路 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 510, M = 10010;int n, m;int dist[N], backup[N];struct Edge&#123; int a, b, w;&#125;edges[M];int bellman_ford()&#123; memset(dist, 0x3f, sizeof dist); dist[1] = 0; for (int i = 0; i &lt; k; i ++ )&#123; memcpy(backup, dist, sizeof dist); for (tin j = 0; j &lt; m; j ++ )&#123; int a = edges[j].a, b = edges[j].b, w = edges[j].w; dist[b] = min(dist[b], backup[a] + w); // 只用上一次的结果，来更新当前的距离 &#125; &#125; if (dist[n] &gt; 0x3f3f3f3f / 2) return -1; return dist[n];&#125;int main()&#123; scanf(&quot;%d%d%d&quot;, &amp;n, &amp;m, &amp;k); for (int i = 0; i &lt; m; i ++ )&#123; int a, b, w; scanf(&quot;%d%d%d&quot;, &amp;a, &amp;b, &amp;m); edges[i] = &#123;a, b, w&#125;; &#125; int t = bellman_ford(); if (t == -1) puts(&quot;impossible&quot;); else printf(&quot;%d &quot;, t); return 0;&#125; 3.6spfa算法（队列优化的Bellman_Ford算法）123456789101112131415161718192021222324252627282930313233343536// 时间复杂度平均情况下O(m), 最坏情况下O(nm)，n表示点数，m表示边数int n; // 总点数int h[N], w[N], e[N], ne[N], idx; // 邻接表存储所有边int dist[N];bool st[N]; // 存储每个点是否在队列中// 求1号点到n号点的最短距离，如果1号点无法走到n号点则返回-1int spfa()&#123; memset(dist, 0x3f, sizeof 0x3f); dist[1] = 0; queue&lt;int&gt; q; q.push(1); st[1] = true; while (q.size())&#123; auto t = q.front(); q.pop(); st[t] = false; for (int i = h[t]; i != -1; i = ne[i])&#123; int j = e[i]; if (dist[j] &gt; dist[t] + w[i])&#123; dist[j] = dist[t] + w[i]; if (!st[j])&#123; // 如果队列中已存在j，则不需要将j重复插入 q.push(j); st[j] = true; &#125; &#125; &#125; &#125; if (dist[n] == 0x3f3f3f3f) return -1; return dist[n];&#125; 3.7spfa判断图中是否存在负环1234567891011121314151617181920212223242526272829303132333435363738// 时间复杂度O(nm)，n表示点数，m表示边数int n; // 总点数int h[N], w[N], e[N], ne[N], idx; // 邻接表存储所有边int dist[N], cnt[N]; // dist[x]存储1号点到x的最短距离，cnt[x]存储1到x的最短路中经过的点数bool st[N]; // 存储每个点是否在队列中// 如果存在负环，则返回true， 否则返回falsebool spfa()&#123; // 不需要初始化dist数组 // 原理：如果某条最短路径上有n个点（除了自己），那么加上自己之后一共有n + 1个点，由抽屉原理一定有两个点相同，所以存在环 queue&lt;int&gt; q; for (int i = 1; i &lt;= n; i ++ )&#123; q.push(i); st[i] = true; &#125; while (q.size())&#123; auto t = q.front(); q.pop(); st[t] = false; for (int i = h[t]; i != -1; i = ne[i])&#123; int j = e[i]; if (dist[j] &gt; dist[t] + w[i])&#123; cnt[j] = cnt[t] + 1; if (cnt[j] &gt;= n) return true; if (!st[j])&#123; q.push(j); st[j] = true; &#125; &#125; &#125; &#125; return false;&#125; 3.8floyd算法 12345678910111213// 时间复杂度是O(n^3)，n表示点数// 初始化for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) if (i == j) d[i][j] = 0; else d[i][j] = INF;// 算法结束后，d[a][b]表示a到b的最短距离void floyd()&#123; for (int k = 1; k &lt;= n; k ++ ) for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) d[i][j] = min(d[i][j], d[i][k] + d[k][j]);&#125; 3.8.1Floyd求最短路 12345678910111213141516171819202122232425262728293031323334353637383940414243#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 210, INF = 1e9;int n, m, Q;int d[N][N];void floyd()&#123; for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) for (int k = 1; k &lt;= n; k ++ ) d[i][j] = min(d[i][j], d[i][k] + d[k][j]);&#125;int main()&#123; scanf(&quot;%d%d%d&quot;, &amp;n, &amp;m, &amp;Q); for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) if (i == j) d[i][j] = 0; else d[i][j] = INF; while (m -- )&#123; int a, b, w; scanf(&quot;%d%d%d&quot;, &amp;a, &amp;b, &amp;w); d[a][b] = min(d[a][b], w); &#125; floyd(); while (Q -- )&#123; int a, b; scanf(&quot;%d%d&quot;, &amp;a, &amp;b); if (d[a][b] &gt; INF / 2) printf(&quot;impossible&quot;); else printf(&quot;%d &quot;, d[a][b]); &#125; return 0;&#125; 3.9朴素版prim 12345678910111213141516171819202122232425262728// 时间复杂度是O(n^2 + m), n表示点数，m表示边数int n; // n表示点数int g[N][N]; // 邻接矩阵，存储所有边int dist[N]; // 存储其他点到当前最小生成树的距离bool st[N]; // 存储每个点是否已经在生成树中// 如果图不连通，则返回INF（0x3f3f3f3f），否则返回最小生成树的树边权重之和int prim()&#123; memset(dist, 0x3f, sizeof dist); int res = 0; for (int i = 0; i &lt; n; i ++ )&#123; int t = -1; for (int j = 1; j &lt;= n; j ++ )&#123; if (!st[j] &amp;&amp; (t == -1 || dist[t] &gt; dist[j])) t = j; &#125; if (i &amp;&amp; dist[t] == INF) return INF; if (i) res += dist[t]; st[t] = true; for (int j = 1; j &lt;= n; j ++ ) dist[j] = min(dist[j], g[t][j]); &#125; return res;&#125; 3.9.1Prim算法求最小生成树 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253#include &lt;cstirng&gt;#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;const int N = 510, INF = 0x3f3f3f3f;int n, m;int g[N][N];int dist[N];bool st[N];int prim()&#123; memset(dist, 0x3f, sizeof dist); int res = 0; for (int i = 0; i &lt; n; i ++ )&#123; int t = -1; for (int j = 1; j &lt;= n; j ++ ) if (!st[j] &amp;&amp; (t == -1 || dist[t] &gt; dist[j])) t = j; if (i &amp;&amp; dist[t] == INF) return INF; if (i) res += dist[t]; for (int j = 1; j &lt;= n; j ++ ) dist[j] = min(dist[j], g[t][j]); st[t] = true; &#125; return res;&#125;int main()&#123; scanf(&quot;%d%d&quot;, &amp;n, &amp;m); memset(g, 0x3f, sizeof g); while (m -- )&#123; int a, b, c; scanf(&quot;%d%d%d&quot;, &amp;a, &amp;b, &amp;c); g[a][b] = g[b][a] = min(g[a][b], c); &#125; int t = prim(); if (t == INF) puts(&quot;impossible&quot;); else printf(&quot;%d &quot;, t); return 0;&#125; 3.10Kruskal算法123456789101112131415161718192021222324252627282930313233343536// 时间复杂度O(mlogm)，n表示点数，m表示边数int n, m; // n是点数，m是边数int p[N]; // 并查集的父节点数组struct Edge&#123; int a, b, w; bool operator&lt; (const Edge &amp;W)const&#123; return W &lt; W.w; &#125;&#125;edges[M];int find(int x)&#123; // 并查集核心操作 if (p[x] != x) p[x] = find(p[x]); return p[x];&#125;int kruskal()&#123; sort(edges, edges + m); for (int i = 1; i &lt;= n; i ++ ) p[i] = i; // 初始化并查集 int res = 0, cnt = 0; for (int i = 0; i &lt; m; i ++ )&#123; int a = edges[i].a, b = edges[i].b, w = edges[i].w; a = find(a), b = find(b); if (a != b)&#123; // 如果两个连通块不连通，则将这两个连通块合并 p[a] = b; res += w; cnt ++ ; &#125; &#125; if (cnt &lt; n - 1) return INF; return res;&#125; 第四章 数学知识4.1试除法判定质数12345678bool is_prime(int x)&#123; if (x &lt; 2) return false; // 第一个质数从2开始 for (int i = 2; i &lt;= x / i; i ++ )&#123; // 枚举到sqrt(x)即可 if (x % i == 0) // 如果出现了1和它本身以外的约数，那么判断不是质数 return false; &#125; return true;&#125; 12345678910111213141516171819202122232425#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int n;bool is_prime(int x)&#123; if (x &lt; 2) return false; for (int i = 2; i &lt;= x / i; i ++ )&#123; if (x % i == 0) return false; &#125; return true;&#125;int main()&#123; cin &gt;&gt; n; for (int i = 1; i &lt;= n; i ++ )&#123; if (is_prime(i)) cout &lt;&lt; i &lt;&lt; endl;\t&#125; return 0;&#125; 4.2试除法分解质因数1234567891011void divide(int x)&#123; for (int i = 2; i &lt;= x / i; i ++ )&#123; if (x % i == 0)&#123; int s = 0; while (x % i == 0) x /= i, s ++ ; cout &lt;&lt; i &lt;&lt; &#x27; &#x27; &lt;&lt; s &lt;&lt; endl; &#125; &#125; if (x &gt; 1) cout &lt;&lt; x &lt;&lt; &#x27; &#x27; &lt;&lt; 1 &lt;&lt; endl; cout &lt;&lt; endl;&#125; 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int n;void divide(int x)&#123; for (int i = 2; i &lt;= x / i; i ++ )&#123; if (x % i == 0)&#123; int s = 0; while (x % i == 0) x /= i, s ++ ; cout &lt;&lt; i &lt;&lt; &#x27; &#x27; &lt;&lt; s &lt;&lt; endl; &#125; &#125; if (x &gt; 1) cout &lt;&lt; x &lt;&lt; &#x27; &#x27; &lt;&lt; 1 &lt;&lt; endl; cout &lt;&lt; endl;&#125;int main()&#123; cin &gt;&gt; n; divide(n); return 0;&#125; 4.3朴素筛法求素数123456789101112int primes[N], cnt; // primes[]存储所有的素数bool st[N]; // st[x]存储x是否被筛掉void get_primes(int n)&#123; for (int i = 2; i &lt;= n; i ++ )&#123; if (st[i]) continue; primes[cnt ++ ] = i; for (int j = i + i; j &lt;= n; j += i)&#123; st[j] = true; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 100;int n;int primes[N], cnt; // primes[]存储所有的素数bool st[N]; // st[x]存储x是否被筛掉void get_primes(int n)&#123; for (int i = 2; i &lt;= n; i ++ )&#123; if (st[i]) continue; primes[cnt ++ ] = i; for (int j = i + i; j &lt;= n; j += i)&#123; st[j] = true; &#125; &#125;&#125;int main()&#123; cin &gt;&gt; n; get_primes(n); for (int i = 0; i &lt; N; i ++ ) cout &lt;&lt; primes[i] &lt;&lt; endl; return 0;&#125; 4.4线性筛法求素数123456789101112int primes[N], cnt; // primes[]存储所有素数bool st[N]; // st[x]存储x是否被筛掉void get_primes(int n)&#123; for (int i = 2; i &lt;= n; i ++ )&#123; if (!st[i]) primes[cnt ++ ] = i; for (int j = 0; primes[j] &lt;= n / i; j ++ )&#123; st[primes[j] * i] = true; if (i % primses[j] == 0) break; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 100;int n;int primes[N], cnt; // primes[]存储所有素数bool st[N]; // st[x]存储x是否被筛掉void get_primes(int n)&#123; for (int i = 2; i &lt;= n; i ++ )&#123; if (!st[i]) primes[cnt ++ ] = i; for (int j = 0; primes[j] &lt;= n / i; j ++ )&#123; st[primes[j] * i] = true; if (i % primes[j] == 0) break; &#125; &#125;&#125;int main()&#123; cin &gt;&gt; n; get_primes(n); for (int i = 0; i &lt; n; i ++ ) cout &lt;&lt; primes[i] &lt;&lt; endl; return 0;&#125; 4.5试除法求所有约数1234567891011vector&lt;int&gt; get_divisors(int x)&#123; vector&lt;int&gt; res; for (int i = 1; i &lt;= x / i; i ++ )&#123; if (x % i == 0)&#123; res.push_back(i); if (i != x / i) res.push_back(x / i); &#125; &#125; sort(res.begin(), res.end()); return res;&#125; 12345678910111213141516171819202122232425262728293031#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int n;vector&lt;int&gt; get_divisors(int x)&#123; vector&lt;int&gt; res; for (int i = 1; i &lt;= x / i; i ++ )&#123; if (x % i == 0)&#123; res.push_back(i); if (i != x / i) res.push_back(x / i); &#125; &#125; sort(res.begin(), res.end()); return res;&#125;int main()&#123;\tvector&lt;int&gt; res; cin &gt;&gt; n; res = get_divisors(n); for (auto x : res) cout &lt;&lt; x &lt;&lt; endl; return 0;&#125; 4.6约数个数之和4.6.1如果数N可以表示为 N = p_1^{c_1} \\times p_2^{c_2} \\times \\cdots \\times p_k^{c_k}4.6.2约数的个数为 (c_1 + 1) \\times (c_2 + 1) \\times \\cdots \\times (c_k + 1)4.6.3约数之和为 (1+p_1+p_1^2+\\cdots+p_1^{c_1})\\times(1+p_2+p_2^2+\\cdots+p_2^{c_2})\\times\\cdots\\times(1+p_k+p_k^2+\\cdots+p_k^{c_k})4.7欧几里得算法123int gcd(int a, int b)&#123; return b ? gcd(b, a % b) : a;&#125; 4.8求欧拉函数 123456789101112int phi(int i)&#123; int res = x; for (int i = 2; i &lt;= x / i ; i ++ )&#123; if (x % i == 0)&#123; res = res / i * (i - 1); while (x % i == 0) x /= i; &#125; &#125; if (x &gt; 1) res = res / x * (x - 1); return res;&#125; 12345678910111213141516171819202122232425262728#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstring&gt;using namespace std;int n;int phi(int x)&#123; int res = x; for (int i = 2; i &lt;= x / i; i ++ )&#123; if (x % i == 0)&#123; res = res / i * (i - 1); while (x % i == 0) x /= i; &#125; &#125; if (x &gt; 1) res = res / x * (x - 1); return res;&#125;int main()&#123; cin &gt;&gt; n; cout &lt;&lt; phi(n); return 0;&#125; 4.8.1欧拉函数 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;int main()&#123; int n; cin &gt;&gt; n; while (n -- )&#123; int a; cin &gt;&gt; a; int res = a; for (int i = 2; i &lt;= a / i; i ++ )&#123; if (a % i == 0)&#123; // i是a的一个质因子 res = res / i * (i - 1); while (a % i == 0) a /= i; // a就把这个质因子除干净 &#125; &#125; if (a &gt; 1) res = res / a * (a - 1); // 说明a还有一个大于1的质因子 cout &lt;&lt; res &lt;&lt; endl; &#125; return 0;&#125; 4.9筛法求欧拉函数12345678910111213141516171819202122int primes[N], cnt; // primes[]存储所有素数int euler[N]; // 存储每个数的欧拉函数bool st[N]; //st[x]存储x是否被筛掉void get_eulers(int n)&#123; euler[1] = 1; for (int i = 2; i &lt;= n; i ++ )&#123; if (!st[i])&#123; primes[cnt ++ ] = i; euler[i] = i - 1; &#125; for (int j = 0; primes[j] &lt;= n / i; j ++ )&#123; int t = primes[j] * i; st[t] = true; if (i % primes[j] == 0)&#123; euler[t] = euler[i] * primes[j]; break; &#125; euler[t] = euler[i] * (primes[j] - 1); &#125; &#125;&#125; 1234567891011121314151617181920212223242526272829303132333435363738394041#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstring&gt;using namespace std;const int N = 1001;int n;int primes[N], cnt; // primes[]存储所有素数int euler[N]; // 存储每个数的欧拉函数bool st[N]; //st[x]存储x是否被筛掉void get_eulers(int n)&#123; euler[1] = 1; for (int i = 2; i &lt;= n; i ++ )&#123; if (!st[i])&#123; primes[cnt ++ ] = i; euler[i] = i - 1; &#125; for (int j = 0; primes[j] &lt;= n / i; j ++ )&#123; int t = primes[j] * i; st[t] = true; if (i % primes[j] == 0)&#123; euler[t] = euler[i] * primes[j]; break; &#125; euler[t] = euler[i] * (primes[j] - 1); &#125; &#125;&#125;int main()&#123; cin &gt;&gt; n; get_eulers(n); cout &lt;&lt; euler[n]; return 0;&#125; 4.10快速幂1234567891011求 m^k mod p, 时间复杂度 O(logk)int qmi(int m, int k, int p)&#123; int res = 1 % p, t = m; while (k)&#123; if (k &amp; 1) res = res * t % p; t = t * t % p; k &gt;&gt;= 1; &#125; return res;&#125; 123456789101112131415161718192021#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;cstring&gt;using namespace std;int qmi(int m, int k, int p)&#123; int res = 1 % p, t = m; while (k)&#123; if (k &amp; 1) res = res * t % p; t = t * t % p; k &gt;&gt;= 1; &#125; return res;&#125;int main()&#123; cout &lt;&lt; qmi(2, 1156165, 3); return 0;&#125; 4.11高斯消元1// a[N][N]是增广矩阵 第五章 动态规划 5.1 01背包问题 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;const int N = 1010;int n, m; // 物品个数，背包容量int v[N], w[N]; // 物品体积 物品价值int f[N][N]; // 状态属性int main()&#123; cin &gt;&gt; n &gt;&gt; m; for (int i = 1; i &lt;= n; i ++ ) cin &gt;&gt; v[i] &gt;&gt; w[i]; // f[0][0~m], 表示考虑前0件物品，总体积不超过m的所有选法中的最大值 for (int i = 1; i &lt;= n; i ++ ) // 枚举所有的物品 for (int j = 0; j &lt;= m; j ++ )&#123; // 枚举所有体积 f[i][j] = f[i - 1][j]; // 左边的情况一定存在 if (j &gt;= v[i]) f[i][j] = max(f[i][j], f[i - 1][j - v[i]] + w[i]); // 右边的情况不一定存在，只有当剩余的背包容量大于体积v[i]时才成立 &#125; cout &lt;&lt; f[n][m] &lt;&lt; endl; // 代表从前n件物品中，总体积不超过m的所有选法中价值最大的那一个，就是答案 return 0;&#125; 5.2完全背包问题 12345678910111213141516171819202122#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;const int N = 1010;int n, m;int v[N], w[N];int f[N][N];int main()&#123; cin &gt;&gt; n &gt;&gt; m; for (int i = 1; i &lt;= n; i ++ ) cin &gt;&gt; v[i] &gt;&gt; w[i]; for (int i = 1; i &lt;= n; i ++ ) for (int j = 0; j &lt;= m; j ++ ) for (int k = 0; k * v[i] &lt;= j; k ++ ) f[i][j] = max(f[i][j], f[i - 1][j - k * v[i]] + k * w[i]); cout &lt;&lt; f[n][m] &lt;&lt; endl;&#125; 5.3多重背包问题I 1234567891011121314151617181920212223#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;int n, m;int v[N], w[N], s[N]; int f[N][N];int main()&#123; cin &gt;&gt; n &gt;&gt; m; for (int i = 1; i &lt;= n; i++ ) cin &gt;&gt; v[i] &gt;&gt; w[i] &gt;&gt; s[i]; for (int i = 1; i &lt;= n; i ++ ) for (int j = 0; j &lt;= m; j ++ ) for (int k = 0; k &lt;= s[i] &amp;&amp; k * v[i] &lt;= j; k ++ ) f[i][j] = max(f[i][j], f[i - 1][j - k * v[i]] + k * w[i]); cout &lt;&lt; f[n][m] &lt;&lt; endl; return 0;&#125; 5.4多重背包问题||5.5分组背包 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;const int N =110;int n, m;int v[N][N], w[N][N], s[N];int f[N];int main()&#123; cin &gt;&gt; n &gt;&gt; m; for (i = 1; i &lt;= n; i ++ )&#123; cin &gt;&gt; s[i]; for (int j = 0; j &lt; s[i]; j ++ ) cin &gt;&gt; v[i][j] &gt;&gt; w[i][j]; &#125; for (int i = 1; i &lt;= n; i ++ ) // 从前往后枚举每一组 for (int j = m; j &gt;= 0; j -- ) // 从大到小枚举体积 if (v[i][k] &lt;= j) f[j] = max(f[j], f[j - v[i][k]] + w[i][k]); cout &lt;&lt; f[m] &lt;&lt; endl; return 0;&#125; 第六章 贪心第七章 时空复杂度分析第八章 动态规划——从集合角度考虑DP问题8.1数字三角形模型第九章 搜索第十章 图论11.6最近公共祖先 11.6.1向上标记法11.6.2倍增法 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 40010, M = N * 2;int n, m;int h[N], e[M], ne[M], idx;int depth[N], fa[N][16];int q[N];void add(int a, int b)&#123; e[idx] = b, ne[idx] = h[a], h[a] = idx ++ ;&#125;void bfs(int root)&#123; memset(depth, 0x3f, sizeof depth); // 初始时，dist初始化为正无穷 depth[0] = 0, depth[root] = 1; // 其中0号点是哨兵，我们定义为0， 根结点是1 int hh = 0, tt = 0; q[0] = root; // 根结点先加入到队列中去 while (hh &lt;= tt)&#123; // 宽搜求这两个数组 int t = q[hh ++ ]; for (int i = h[t]; i != -1; i = ne[i])[ int j = e[i]; if (depth[j] &gt; depth[t] + 1)&#123; depth[j] = depth[t] + 1; q[ ++ tt] = j; fa[j][0] = t; for (int k = 1; k &lt;= 15; k ++ ) fa[j][k] = fa[fa[j][k - 1]][k - 1]; &#125; ] &#125;&#125;int lca(int a, int b)&#123; if (depth[a] &gt; depth[b]) swap(a, b); // 如果a在b的上面，则交换 // 跳到同一层，从高往低跳 for (int k = 15; k &gt;= 0; k -- )&#123; if (depth[fa[a][k]] &gt;= depth[b]) // 跳了之后，在b的下面或者是同一层 a = fa[a][k]; // 那么说明a是可以跳的 &#125; if (a == b) return a; // 说明a或者b就是公共祖先 // 否则同时往上跳 for (int k = 15; k &gt;= 0; k -- )&#123; if (fa[a][k] != fa[a][k])&#123; // 说明还没有跳到公众祖先上面 a = fa[a][k]; b = fa[b][k]; &#125; &#125; return fa[a][0];&#125;int main()&#123; scanf(&quot;%d&quot;, &amp;n); int root = 0; // 定义根结点 memset(h, -1, sizeof h); for (int i = 0; i &lt; n; i ++ )&#123; int a, b; scanf(&quot;%d%d&quot;, &amp;a, &amp;b); if (b == -1) root = a; // b是-1，则a是根结点 else add(a, b), add(b, a); &#125; bfs(root); scanf(&quot;%d&quot;, &amp;m); while (m -- )&#123; int a, b; scanf(&quot;%d%d&quot;, &amp;a, &amp;b); int p = lca(a, b); if (p == a) puts(&quot;1&quot;); else if (p == b) puts(&quot;2&quot;); else puts(&quot;0&quot;); &#125; return 0;&#125; 第十一章 高级数据结构第十二章 数学知识第十三章 基础算法第十七章 图论第十八章 数据结构第十九章 动态规划19.1数字三角形模型 19.1.1摘花生 123456789101112131415161718192021222324252627#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;const int N = 110;int n, m;int w[N][N];int f[N][N];int main() &#123; int T; scanf(&quot;%d%d&quot;, &amp;n, &amp;m); while (T -- ) &#123; scanf(&quot;%d%d&quot;, &amp;n, &amp;m); for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= m; j ++ ) scanf(&quot;%d&quot;, w[i][j]); for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= m; j ++ ) f[i][j] = max(f[i - 1][j] + w[i][j], f[i][j - 1] + w[i][j]); printf(&quot;%d &quot;, f[n][m]); &#125; return 0;&#125; 19.1.2最低通行费 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;const int N = 110, INF = 1e9;int n;int w[N][N];int f[N][N];int main() &#123; scanf(&quot;%d&quot;, &amp;n); for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) scanf(&quot;%d&quot;, &amp;w[i][j]); for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) if (i == 1 &amp;&amp; j == 1) f[i][j] = w[i][j]; // 边界特判，特判左上角 else &#123; f[i][j] = INF; if (i &gt; 1) f[i][j] = min(f[i][j], f[i - 1][j] + w[i][j]); // 只有不在第一行的时候，才可以从上面过来 if (j &gt; 1) f[i][j] = min(f[i][j - 1] + w[i][j], f[i][j]); // 只有不在第一列的时候，才可以从左面过来 &#125; printf(&quot;%d &quot;, f[n][n]); return 0;&#125; 19.1.3方格取数 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 15;int n;int w[N][N];int f[N * 2][N][N];int main() &#123; scanf(&quot;%d&quot;, &amp;n); int a, b, c; while (cin &gt;&gt; a &gt;&gt; b &gt;&gt; c, a || b || c) w[a][b] = c; for (int k = 2; k &lt;= n + n; k ++ ) for (int i1 = 1; i1 &lt;= n; i1 ++ ) for (int i2 = 1; i2 &lt;= n; i2 ++ ) &#123; int j1 = k - i1, j2 = k - i2; if (j1 &gt;= 1 &amp;&amp; j1 &lt;= n &amp;&amp; j2 &gt;= 1 &amp;&amp; j2 &lt;= n) &#123; int t = w[i1][j1]; if (i1 != i2) t += w[i2][j2]; int &amp;x = f[k][i1][i2]; x = max(x, f[k - 1][i1 - 1][i2 - 1] + t); x = max(x, f[k - 1][i1 - 1][i2] + t); x = max(x, f[k - 1][i1][i2 - 1] + t); x = max(x, f[k - 1][i1][i2] + t); &#125; &#125; printf(&quot;%d &quot;, f[n + n][n][n]); return 0;&#125; 19.2最长上升子序列（一） 19.2.1最长上升子序列朴素版 123456789101112131415161718192021222324252627282930#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 1010;int n;int a[N], f[N];int main() &#123; scanf(&quot;%d&quot;, &amp;n); for (int i = 1; i &lt;= n; i ++ ) scanf(&quot;%d&quot;, &amp;a[i]); for (int i = 1; i &lt;= n; i ++ ) &#123; // 空集 f[i] = 1; for (int j = 1; j &lt; i; j ++ ) // 上升序列合法 if (a[j] &lt; a[i]) f[i] = max(f[i], f[j] + 1); &#125; int res = 0; for (int i = 1; i &lt;= n; i ++ ) res = max(res, f[i]); printf(&quot;%d &quot;, res); return 0;&#125; 19.2.2怪盗基德的滑翔翼 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;cstring&gt;#include &lt;algorithm&gt;using namespace std;const int N = 110;int n;int a[N], f[N];int mian() &#123; int T; scanf(&quot;%d&quot;, &amp;T); while (T -- ) &#123; scanf(&quot;%d&quot;, &amp;n); // 读入每一个楼的高度 for (int i = 1; i &lt;= n; i ++ ) scanf(&quot;%d&quot;, &amp;a[i]); // 正向求解LIS问题 int res = 0; for (int i = 1; i &lt;= n; i ++ ) &#123; f[i] = 1; for (int j = 1; j &lt; i; j ++ ) if (a[i] &gt; a[j]) f[i] = max(f[i], f[j] + 1); res = max(res, f[i]); &#125; // 反向求解LIS for (int i = n; i; i -- ) &#123; f[i] = 1; for (int j = n; j &gt; i; j -- ) if (a[i] &gt; a[j]) f[i] = max(f[i], f[j] + 1); res = max(res, f[i]); &#125; printf(&quot;%d &quot;, res); &#125; return 0;&#125; 19.2.3登山 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;const int N = 1010;int n;int a[N];int f[N], g[N];int main() &#123; scanf(&quot;%d&quot;, &amp;n); for (int i = 1; i &lt;= n; i ++ ) scanf(&quot;%d&quot;, &amp;a[i]); // 从左往右求 for (int i = 1; i &lt;= n; i ++ ) &#123; f[i] = 1; for (int j = 1; j &lt; i; j ++ ) if (a[i] &gt; a[j]) f[i] = max(f[i], f[j] + 1); &#125; // 从右往左求 for (int i = n; i; i -- ) &#123; g[i] = 1; for (int j = n; j &gt; i; j -- ) if (a[i] &gt; a[j]) g[i] = max(g[i], g[j] + 1); &#125; // 枚举中间值 int res = 0; fro (int i = 1; i &lt;= n; i ++ ) res = max(res, f[i] + g[i] - 1); printf(&quot;%d &quot;, res); return 0;&#125; 19.2.4合唱队形 最少去掉多少个，就是最多留下的，也就成了登山题 123456789101112131415161718192021222324252627282930313233343536#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;const int N = 110;int n;int a[N];int f[N], g[N];int main() &#123; scanf(&quot;%d&quot;, &amp;n); for (int i = 1; i &lt;= n; i ++ ) scanf(&quot;%d&quot;, &amp;a[i]); // 从左往右求 for (int i = 1; i &lt;= n; i ++ ) &#123; f[i] = 1; for (int j = 1; j &lt; i; j ++ ) if (a[i] &gt; a[j]) f[i] = max(f[i], f[j] + 1); &#125; // 从右往左求 for (int i = n; i; i -- ) &#123; g[i] = 1; for (int j = n; j &gt; i; j -- ) if (a[i] &gt; a[j]) g[i] = max(g[i], g[j] + 1); &#125; // 枚举中间值 int res = 0; fro (int i = 1; i &lt;= n; i ++ ) res = max(res, f[i] + g[i] - 1); printf(&quot;%d &quot;, n - res); return 0;&#125; 19.2.5友好城市 1234567891011121314151617181920212223242526272829303132#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;typedef pair&lt;int, int&gt; PII;const int N = 5010;int n;PII q[N];int f[N];int main() &#123; scanf(&quot;%d&quot;, &amp;n); for (int i = 0; i &lt; n; i ++ ) scanf(&quot;%d%d&quot;, &amp;q[i].first, &amp;q[i].second); sort(q, q + n); int res = 0; for (int i = 0; i &lt; n; i ++ ) &#123; f[i] = 1; for (int j = 0; j &lt; i; j ++ ) if (q[i].sceond &gt; q[j].second) f[i] = max(f[i], f[j + 1]); res = max(res, f[i]); &#125; printf(&quot;%d &quot;, res); return 0;&#125; 19.2.6最大上升子序列和 1234567891011121314151617181920212223242526272829#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;const int N = 1010;int n;int a[N];int f[N];int main() &#123; cin &gt;&gt; n; for (int i = 1; i &lt;= n; i ++ ) cin &lt;&lt; a[i]; int res = 0; for (int i = 1; i &lt;= n; i ++ ) &#123; f[i] = a[i]; for (int j = 1; j &lt; i; j ++ ) if (a[j] &lt; a[k]) f[i] = max(f[i], f[j] + a[i]); &#125; for (int i = 1; i &lt;= n; i ++ ) res = max(res, f[i]); cout &lt;&lt; res &lt;&lt; endl; return 0;&#125; 19.2.7拦截导弹 12345678910111213141516171819202122232425262728293031323334353637383940#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;const int N = 1010;int n;int q[N];int f[N];int g[N]; // 现有的所有子序列int main() &#123; while (cin &gt;&gt; q[n]) n ++ ; int res = 0; for (int i = 0; i &lt; n; i ++ ) &#123; int res = 0; for (int j = 0; j &lt; i; j ++ ) if (q[j] &gt;= q[i]) f[i] = max(f[i], f[j] + 1); res = max(res, f[i]); &#125; cout &lt;&lt; res &lt;&lt; endl; int cnt = 0; // 当前子序列的个数 for (int i = 0; i &lt; n; i ++ ) &#123; int k = 0; // 从前往后找的序列 // 只要我们没有遍历完序列 &amp;&amp; 当前序列的结尾是小于当前数的 while (k &lt; cnt &amp;&amp; g[k] &lt; q[i]) k ++ ; g[k] = q[i]; // 没有任何一个序列是大于等于当前数的 if (k &gt;= cnt cnt ++ ; // 开一个新的序列 &#125; cout &lt;&lt; cnt &lt;&lt; endl; return 0;&#125; 19.2.8导弹防御系统 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556#include &lt;iostream&gt;#include &lt;cstring&gt;using namespace std;const int N = 55;int n;int q[N];int up[N], down[N]; // 分别表示上升子序列的结尾和下降子序列的结尾int ans; // 全局最小值，使用dfs进行更新// u 当前枚举到了第几个数， su 当前上升子序列的个数，sd 当前下降子序列的个数void dfs(int u, int su, int sd) &#123; // 不可能使得答案变小了，直接返回 if (su + sd &gt;= ans) return; // 说明找到一个方案 if (u == n) &#123; // 更新 ans ans = su + sd; return; &#125; // 情况1：将当前数放到上升子序列中 int k = 0; // 先进行备份 while (k &lt; su &amp;&amp; up[k] &gt;= q[u]) k ++ ; int t = up[k]; up[k] = q[u]; if (k &lt; su) dfs(u + 1, su, sd); else dfs(u + 1, su + 1, sd); // 恢复现场 up[k] = t; // 情况2：将当前数放到下降子序列中 k = 0; while (k &lt; sd &amp;&amp; down[k] &lt;= q[u]) k ++ ; t = down[k]; down[k] = q[u]; if (k &lt; sd) dfs(u + 1, su, sd); else dfs(u + 1, su, sd + 1); down[k] = t;&#125;int main() &#123; while (cin &gt;&gt; n, n) &#123; for (int i = 0; i &lt; n; i ++ ) cin &gt;&gt; q[i]; // 进行暴搜之前，先记录最大值 n，就是最坏情况下，每个数单独一个序列 ans = n; dfs(0, 0, 0); cout &lt;&lt; ans &lt;&lt; endl; &#125; return 0;&#125; 19.2.9最长公共上升子序列 123456789101112131415161718192021222324252627282930313233343536373839#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;const int N = 3010;int n;int a[N], b[N]；int f[N][N];int main() &#123; scanf(&quot;%d&quot;, &amp;n); for (int i = 1; i &lt;= n; i ++ ) scanf(&quot;%d&quot;, &amp;a[i]); for (int i = 1; i &lt;= n; i ++ ) scanf(&quot;%d&quot;, &amp;b[i]); for (int i = 1; i &lt;= n; i ++ ) for (int j = 1; j &lt;= n; j ++ ) &#123; // 先考虑右半部分 f[i][j] = f[i - 1][j]; // 再考虑左半边，情况又细分为多种，需要依次枚举 // 当 a[i] == b[j] 的时候才会存在 if (a[i] == b[j]) &#123; // 先考虑小块中存在的空集 f[i][j] = max(f[i][j], 1); for (int k = 1; k &lt; j; k ++ ) &#123; // 这里需要再判断每个小块是否存在 if (b[k] &lt; b[j]) f[i][j] = max(f[i][j], f[i][k] + 1); &#125; &#125; &#125; int res = 0; for (int i = 1; i &lt;= n; i ++ ) res = max(res, f[n][i]); printf(&quot;%d &quot;, res); return 0;&#125; 12345678910111213141516171819202122232425262728293031323334#include &lt;iostream&gt;#include &lt;algorithm&gt;using namespace std;const int N = 3010;int n;int a[N], b[]；int f[N][N];int main() &#123; scanf(&quot;%d&quot;, &amp;n); for (int i = 1; i &lt;= n; i ++ ) scanf(&quot;%d&quot;, &amp;a[i]); for (int i = 1; i &lt;= n; i ++ ) scanf(&quot;%d&quot;, &amp;b[i]); for (int i = 1; i &lt;= n; i ++ ) &#123; int maxv = 1; // 满足 b[k]&lt;a[i] 的情况下 1 到 j-1 的最大值 for (int j = 1; j &lt;= n; j ++ ) &#123; // 先考虑右半部分 f[i][j] = f[i - 1][j]; // 再考虑左半边，情况又细分为多种，需要依次枚举 // 当 a[i] == b[j] 的时候才会存在 if (a[i] == b[j]) f[i][j] = max(f[i][j], maxv); // 将循环优化为它 if (b[j] &lt; a[i]) maxv = max(maxv, f[i][j] + 1); &#125; &#125; int res = 0; for (int i = 1; i &lt;= n; i ++ ) res = max(res, f[n][i]); printf(&quot;%d &quot;, res); return 0;&#125; 19.3背包模型 多重背包III 完全背包 第二十章 计算几何第二十一章 数学第二十二章 搜索第二十三章 基础算法附录：练习题目1.数的进制转换 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748#include &lt;iostream&gt;#include &lt;algorithm&gt;#include &lt;vector&gt;using namespace std;int main()&#123; int T; cin &gt;&gt; T; while (T -- )&#123; int a, b; string a_line, b_line; cin &gt;&gt; a &gt;&gt; b &gt;&gt; a_line; vector&lt;int&gt; number; for (auto c : a_line)&#123; if (c &gt;= &#x27;0&#x27; &amp;&amp; c &lt;= &#x27;9&#x27;) number.push_back(c - &#x27;0&#x27;); if (c &gt;= &#x27;A&#x27; &amp;&amp; c &lt;= &#x27;Z&#x27;) number.push_back(c - &#x27;A&#x27; + 10); if (c &gt;= &#x27;a&#x27; &amp;&amp; c &lt;= &#x27;z&#x27;) number.push_back(c - &#x27;a&#x27; + 36); &#125; reverse(number.begin(), number.end()); vector&lt;int&gt; res; while (number.size())&#123; int r = 0; for (int i = number.size() - 1; i &gt;= 0; i -- )&#123; number[i] += r * a; r = number[i] % b; number[i] /= b; &#125; res.push_back(r); while (number.size() &amp;&amp; number.back() == 0) number.pop_back(); &#125; reverse(res.begin(), res.end()); for (auto x : res)&#123; if (x &lt;= 9) b_line += char(x + &#x27;0&#x27;); if (x &gt;= 10 &amp;&amp; x &lt;= 35) b_line += char(x - 10 + &#x27;A&#x27;); if (x &gt;= 36) b_line += char(x - 36 + &#x27;a&#x27;); &#125; cout &lt;&lt; a &lt;&lt; &#x27; &#x27; &lt;&lt; a_line &lt;&lt; endl; cout &lt;&lt; b &lt;&lt; &#x27; &#x27; &lt;&lt; b_line &lt;&lt; endl; cout &lt;&lt; endl; &#125; return 0;&#125; 2.两数之和 暴力 12345678910class Solution&#123;public: vector&lt;int&gt; twoSum(vector&lt;int&gt; &amp;nums, int target)&#123; for (int i = 0; i &lt; nums.size() - 1; i ++ ) for (int j = i + 1; j &lt; nums.size(); j ++ ) if (nums[i] + nums[j] == target) return &#123;i, j&#125;; return &#123;&#125;; &#125;;&#125;; 两遍哈希 123456789101112131415161718class Solution &#123;public: vector&lt;int&gt; twoSum(vector&lt;int&gt; &amp;nums, int target)&#123; map&lt;int, int&gt; a; //建立hash表存放数组元素 vector&lt;int&gt; b(2, -1); // 存放结果 for (int i = 0; i &lt; nums.size(); i ++ ) a.insert(map&lt;int, int&gt;::value_type(nums[i], i)); for (int i = 0; i &lt; nums.size(); i ++ )&#123; if (a.count(target - nums[i]) &gt; 0 &amp;&amp; (target - nums[i] != i))&#123; // 判断是否找到目标元素且目标元素不能是本身 b[0] = i; b[1] = a[target - nums[i]]; break; &#125; &#125; return b; &#125;;&#125;; 3.LeetCode暑期刷题打卡2019——Week1 二分专题 69.x的平方根12345678910111213class Solution &#123;public: int mySqrt(int x) &#123; int l = 0, r = x; while (l &lt; r) &#123; int mid = l + (long long)r + 1 &gt;&gt; 1; if (mid &lt;= x / mid) l = mid; else r = mid - 1; &#125; return r; &#125;&#125;; 35.搜索插入位置123456789101112131415class Solution &#123;public: int searchInsert(vector&lt;int&gt; &amp;nums, int target) &#123; if (nums.empty() || nums.back() &lt; target) return nums.size(); int l = 0, r = nums.size() - 1; while (l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if (nums[mid] &gt;= target) r = mid; else l = mid + 1; &#125; return r; &#125;&#125;; 34.在排序数组中查找元素的第一个和最后一个位置12345678910111213141516171819202122232425class Solution &#123;public: vector&lt;int&gt; searchRange(vector&lt;int&gt; &amp;nums, int target) &#123; if (nums.empty()) return &#123;-1, -1&#125;; int l = 0, r = nums.size() - 1; while (l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if (nums[mid] &gt;= target) r = mid; else l = mid + 1; &#125; if (nums[r] != target) return &#123;-1, -1&#125;; int start = l; l = 0, r = nums.size() - 1; while (l &lt; r) &#123; int mid = l + r + 1 &gt;&gt; 1; if (nums[mid] &lt;= target) l = mid; else r = mid - 1; &#125; int end = l; return &#123;start, end&#125;; &#125;&#125;; 74.搜索二维矩阵https://leetcode.cn/problems/search-a-2d-matrix/description/ 1234567891011121314151617class Solution &#123;public: bool searchMatrix(vector&lt;int&gt; &amp;matrix, int target) &#123; if (matrix.empty() || matrix[0].empty()) return false; int n = matrix.size(), m = matrix[0].size(); int l = 0, r = n * m - 1; while (l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if (matrix[mid / m][mid % m] &gt;= target) r = mid; else l = mid + 1; &#125; if (matrix[r / m][r % m] != target) return false; return true; &#125;&#125;; 153.寻找旋转排序数组中的最小值https://leetcode.cn/problems/find-minimum-in-rotated-sorted-array/description/ 12345678910111213class Solution &#123;public: int findMin(vector&lt;int&gt; &amp;nums) &#123; int l = 0, r = nums.size() - 1; while (l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if (nums[mid] &lt;= nums.back()) r = mid; else l = mid + 1; &#125; return nums[r]; &#125;&#125;; 33.搜索旋转排序数组1234567891011121314151617181920212223242526class Solution &#123;public: int search(vector&lt;int&gt; &amp;nums, int target) &#123; if (nums.empty()) return -1; // 找到最小值 int l = 0, r = nums.size() - 1; while (l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if (nums[mid] &lt;= nums.back()) r = mid; else l = mid + 1; &#125; if (target &lt;= nums.back()) r = nums.size() - 1; else l = 0, r -- ; while (l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if (nums[mid] &gt;= target) r = mid; else l = mid + 1; &#125; if (nums[l] == target) return l; else return -1; &#125;&#125;; 278.第一个错误的版本 123456789101112131415bool isBadVersion(int version);class Solution &#123;public: int firstBadVersion(int n) &#123; int l = 1, r = n; while (l &lt; r) &#123; int mid = (long long)l + r &gt;&gt; 1; if (isBadVersion(mid)) r = mid; else l = mid + 1; &#125; return r; &#125;&#125;; 162.寻找峰值 1234567891011121314class Solution &#123;public: int findPeakElement(vector&lt;int&gt;&amp; nums) &#123; int l = 0, r = nums.size() - 1; while (l &lt; r) &#123; int mid = l + r &gt;&gt; 1; if (nums[mid] &gt; nums[mid + 1]) r = mid; else l = mid + 1; &#125; return r; &#125;&#125;; 287.寻找重复数 1234567891011121314151617181920class Solution &#123;public: int findDuplicate(vector&lt;int&gt;&amp; nums) &#123; int l = 1, r = nums.size() - 1; while (l &lt; r) &#123; int mid = l + r &gt;&gt; 1; int cnt = 0; for (auto x: nums) &#123; if (x &gt;= l &amp;&amp; x &lt;= mid) cnt ++ ; &#125; if (cnt &gt; mid - l + 1) r = mid; else l = mid + 1; &#125; return r; &#125;&#125;; 275.H指数 II 12345678910111213class Solution &#123;public: int hIndex(vector&lt;int&gt; &amp;nums) &#123; int l = 0, r = nums.size(); while (l &lt; r) &#123; int mid = l + r + 1 &gt;&gt; 1; if (nums[nums.size() - mid] &gt;= mid) l = mid; else r = mid - 1; &#125; return r; &#125;&#125;; 19.删掉倒数第n个结点 1234567891011121314151617class Solution &#123;public: ListNode* removeNthFromEnd(ListNode *head, int n) &#123; auto dummy = new ListNode(-1); dummy-&gt;next = head; auto first = dummy, second = dummy; while (n -- ) first = first-&gt;next; while (first-&gt;next) &#123; first = first-&gt;next; second = second-&gt;next; &#125; second-&gt;next = second-&gt;next-&gt;next; return dummy-&gt;next; &#125;&#125;; 237.删除链表中的结点 1234567class Solution &#123;public: void deleteNode(ListNode *node) &#123; node-&gt;val = node-&gt;next-&gt;val; node-&gt;next = node-&gt;next-&gt;next; &#125;&#125;; 123456class Solution &#123;public: void deleteNode(ListNode *node) &#123; *(ndoe) = *(node-&gt;next); &#125;&#125;; 83.从排序列表中删除重复项 123456789101112class Solution &#123;public: ListNode* deleteDuplicates(ListNode *head) &#123; auto cur = head; while (cur) &#123; if (cur-&gt;next &amp;&amp; cur-&gt;next-&gt;val == cur-&gt;val) cur-&gt;next = cur-&gt;next-&gt;next; else cur = cur-&gt;next; &#125; return head; &#125;&#125;; 61.旋转链表 1234567891011121314151617181920212223class Solution &#123;public: ListNode* rotateRight(ListNode *head, int k)&#123; if (!head) return NULL; int n = 0; for (auto p = head; p; p = p-&gt;next) n ++ ; k %= n; auto first = head, second = head; while (k -- ) first = first-&gt;next; while (first-&gt;next) &#123; first = first-&gt;next; second = second-&gt;next; &#125; first-&gt;next = head; head = second-&gt;next; second-&gt;next = NULL; return head; &#125;&#125;; 24.交换链表中相邻的两个节点，最后返回头结点 1234567891011121314151617class Soultion &#123;public: ListNode * swapPairs(ListNode *head)&#123; auto dummy = new ListNode(-1); dummy-&gt;next = head; for (auto p = dummy; p-&gt;next &amp;&amp; p-&gt;next-&gt;next;) &#123; auto a = p-&gt;next, b = a-&gt;next; p-&gt;next = b; a-&gt;next = b-&gt;next; b-&gt;next = a; p = a; &#125; return dummy-&gt;next; &#125;&#125;; 206.翻转链表I 1234567891011121314151617class Solution &#123;public: ListNode* reverseList(ListNode *head) &#123; if (!head) return NULL; auto a = head, b = head-&gt;next; while (b) &#123; auto c = b-&gt;next; b-&gt;next = a; a = b, b = c; &#125; head-&gt;next = NULL; return a; &#125;&#125;; 92.翻转链表II 1234567891011121314151617181920212223242526class Solution &#123;public: ListNode* reverseBetween(ListNode *head, int m, int n) &#123; if (m == n) return head; // 定义虚拟节点 auto dummy = new ListNode(-1); dummy-&gt;next = head; // 找到b和d的位置 auto a = dummy, d = dummy; for (int i = 0; i &lt; m - 1; i ++ ) a = a-&gt;next; for (int i = 0; i &lt; n; i ++ ) d = d-&gt;next; // 进行翻转 auto b = a-&gt;next, c = d-&gt;next; for (auto p = b, q = b-&gt;next; q != c; ) &#123; auto o = q-&gt;next; q-&gt;next = p; p = q, q = o; &#125; // 改变指针 a-&gt;next = d; b-&gt;next = c; return dummy-&gt;next; &#125;&#125;; 160.两个链表的交点 1234567891011121314class Solution &#123;public: ListNode *getIntersectionNode(ListNode *headA, ListNode *headB) &#123; auto p = headA, q = headB; while (p != q) &#123; if (p) p = p-&gt;next; else p = headB; if (q) q = q-&gt;next; else q = headA; &#125; return p; &#125;&#125;; 142.链表循环II 1234567891011121314151617181920212223class Solution &#123;public: ListNode* detectCycle(ListNode *head) &#123; auto fast = head, slow = head; while (slow) &#123; fast = fast-&gt;next; slow = slow-&gt;next; if (slow) slow = slow-&gt;next; else break; if (fast == slow) &#123; slow = head; while (slow != fast) &#123; fast = fast-&gt;next; slow = slow-&gt;next; &#125; return slow; &#125; &#125; return NULL; &#125;&#125;; 148.排序链表 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * Definition for singly-linked list. * struct ListNode &#123; * int val; * ListNode *next; * ListNode() : val(0), next(nullptr) &#123;&#125; * ListNode(int x) : val(x), next(nullptr) &#123;&#125; * ListNode(int x, ListNode *next) : val(x), next(next) &#123;&#125; * &#125;; */class Solution &#123;public: ListNode* sortList(ListNode* head) &#123; int n = 0; for (auto p = head; p; p = p-&gt;next) n ++ ; auto dummy = new ListNode(-1); dummy-&gt;next = head; for (int i = 1; i &lt; n; i *= 2) &#123; auto cur = dummy; for (int j = 0; j + i &lt; n; j += i * 2) &#123; auto left = cur-&gt;next, right = cur-&gt;next; for (int k = 0; k &lt; i; k ++ ) right = right-&gt;next; int l = 0, r = 0; while (l &lt; i &amp;&amp; r &lt; i &amp;&amp; right) &#123; if (left-&gt;val &lt;= right-&gt;val) &#123; cur-&gt;next = left; cur = left; left = left-&gt;next; l ++ ; &#125;else &#123; cur-&gt;next = right; cur = right; right = right-&gt;next; r ++ ; &#125; &#125; while (l &lt; i) &#123; cur-&gt;next = left; cur = left; left = left-&gt;next; l ++ ; &#125; while (r &lt; i &amp;&amp; right) &#123; cur-&gt;next = right; cur = right; right = right-&gt;next; r ++ ; &#125; cur-&gt;next = right; &#125; &#125; return dummy-&gt;next; &#125;&#125;; 98.判断二叉搜索树 12345678910111213class Solution &#123;public: bool isVaildBST(TreeNode *root) &#123; return dfs(root, INT_MIN, INT_MAX); &#125; bool dfs(TreeNode *root, long long minv, long long maxv)&#123; if (!root) return true; if (root-&gt;val &lt; minv || root-&gt;val &gt; maxv) return false; return dfs(root-&gt;left, miv, root-&gt;val - 1ll) &amp;&amp; dfs(root-&gt;right, root-&gt;val + 1ll, maxv); &#125;&#125; 94.二叉树的中序遍历 12345678910111213141516171819202122class Solution &#123;public: vector&lt;int&gt; inorderTraversal(TreeNode *root) &#123; vector&lt;int&gt; res; stack&lt;TreeNode*&gt; stk; auto p = root; while (p || stk.size())&#123; while (p)&#123; stk.push(p); p = p-&gt;left; &#125; p = stk.top(); stk.pop(); res.push_back(p-&gt;val); p = p-&gt;right; &#125; return res; &#125;&#125; 101.镜像二叉树 1234567891011121314151617181920212223242526272829303132333435363738class Solution &#123;public: bool isSymmetric(TreeNode *root)&#123; if (!root) return true; // 空节点是特殊的镜像二叉树 return dfs(root-&gt;left, root-&gt;right); &#125; bool dfs(TreeNode *p, TreeNode *q)&#123; if (!p || !q) return !p &amp;&amp; !q; // 只有当左右子树同时为空时才为对称二叉树 return p-&gt;val == q-&gt;val &amp;&amp; dfs(p-&gt;left, q-&gt;right) &amp;&amp; dfs(p-&gt;right, q-&gt;left); &#125;&#125;;// 迭代法// 左边：左中右遍历；右边：右中左遍历，同时检查是否相等class Solution&#123;public: bool isSymmetric(TreeNode *root)&#123; if (!root) return true; stack&lt;TreeNode*&gt; left, right; auto l = root-&gt;left, r = root-&gt;right; while (l || r || left.size() || right.size())&#123; while (l &amp;&amp; r)&#123; left.push(l), l = l-&gt;left; right.push(r), r = r-&gt;right; &#125; if (l || r) return false; l = left.top(), left.pop(); r = right.top(), right.pop(); if (l-&gt;val != r-&gt;val) return false; l = l-&gt;right, r = r-&gt;left; &#125; return true; &#125;&#125;; 105.通过前序和中序构建二叉树 1234567891011121314151617181920212223class Solution&#123;public: unordered_map&lt;int, int&gt; pos; TreeNode *buildTree(vector&lt;int&gt; &amp; preorder, vector&lt;int&gt; &amp;inorder)&#123; int n = preorder.size(); for (int i = 0; i &lt; n; i ++ ) pos[inorder[i]] = i; return dfs(preorder, inorder, 0, n - 1, 0, n - 1); &#125; TreeNode *dfs(vector&lt;int&gt; &amp;preorder, vector&lt;int&gt; &amp;inorder, int pl, int pr, int il, int ir)&#123; if (pl &gt; pr) return NULL; int val = preorder[pl]; // 前序遍历的起点、终点；中序遍历的起点、终点 int k = pos[val]; // 前序遍历点在中序遍历中的位置 int len = k - il; // 左子树长度 auto root = new TreeNode(val); root-&gt;left = dfs(preorder, inorder, pl + 1, pl + len, il, k - 1); root-&gt;right = dfs(preorder, inorder, pl + len + 1, pr, k + 1, ir); return root; &#125;&#125;; 102.层序遍历 1234567891011121314151617181920212223242526272829class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode *root)&#123; vector&lt;vector&lt;int&gt;&gt; res; if (!root) return res; queue&lt;TreeNode*&gt; q; q.push(root); while (q.size())&#123; int n = q.size(); // 当前这一层点的个数 vector&lt;int&gt; level; //当前遍历的结果 for (int i = 0; i &lt; len; i ++ )&#123; auto t = q.front(); q.pop(); level.push_back(t-&gt;val); if (t-&gt;left) q.push(t-&gt;left); if (t-&gt;right) q.push(t-&gt;right); &#125; res.push_back(level); &#125; return res; &#125;&#125;; 236.求两个点的最近公共祖先 1234567891011121314151617class Solution&#123;public: TreeNode *lowestCommonAncestor(TreeNode *root, TreeNode *p, TreeNode *q)&#123; // 如果以root为根的子树中包含q和q,则返回他们的最近公共祖先 // 如果只包含p, 则返回p // 如果只包含q, 则返回q // 如果都不包含，则返回NULL if (!root || root == p || root == q) return root; auto left = lowestCommonAncestor(root-&gt;left, p, q); auto right = lowestCommonAncestor(root-&gt;right, p, q); if (!left) return right; if (!right) return left; return root; &#125;&#125;; 543.二叉树的直径 12345678910111213141516171819class Solution&#123;public: int ans = 0; int diameterOfBinaryTree(TreeNode *root)&#123; dfs(root); return ans; &#125; int dfs(TreeNode *root)&#123; if (!root) return 0; auto left = dfs(root-&gt;left); auto right = dfs(root-&gt;right); ans = max(ans, left + right); return max(left + 1, right + 1); &#125;&#125;; 124.二叉树最大路径和 12345678910111213141516171819202122232425class Solution &#123;public: int ans = INT_MIN; int maxPathSum(TreeNode* root) &#123; dfs(root); return ans; &#125; // 返回从root向下走的最大值 int dfs(TreeNode *root)&#123; // 如果当前为空，则不存在 if (!root) return 0; // 分别求从左子树向下走的最大值和从右子树向下走的最大值 auto left = dfs(root-&gt;left); auto right = dfs(root-&gt;right); // 答案是从当前答案、左子树+右子树+当前值中的最大值 ans = max(ans, left + right + root-&gt;val); // 返回分为：向左走、向右走、不走，0就没有必要返回了，返回0宁可不要 return max(0, root-&gt;val + max(left, right)); &#125;&#125;; 173.二叉树迭代器1234567891011121314151617181920212223242526272829303132333435363738394041// 时间复杂度O(1) 空间复杂度O(h), 即可用栈来模拟中序遍历的过程，栈的最大长度和树的高度成正比class BSTIterator &#123;public: stack&lt;TreeNode*&gt; stk; BSTIterator(TreeNode* root) &#123; // 初始化时，将树的最左边那条链插入到栈中 while (root)&#123; // 每一次把根节点插入 stk.push(root); root = root-&gt;left; &#125; &#125; // 每次调用返回中序遍历的下一个节点 int next() &#123; // 每次取next就是取栈顶元素 auto p = stk.top(); stk.pop(); // 每次遍历完一个点的时候，就需要将右子树加入栈 // 首先保存答案，因为最后要返回答案 int res = p-&gt;val; // 先指向右子树 p = p-&gt;right; // 将右子树加进来，就是将最左边的一条链加进来 while (p)&#123; stk.push(p); p = p-&gt;left; &#125; return res; &#125; // 询问是否有下一个节点 bool hasNext() &#123; // 当我们遍历完所有点的时候，我们的栈就是空的 return !stk.empty(); &#125;&#125;; 297.序列化和反序列化二叉树 仅有前序遍历的话，是不能唯一确定一颗二叉树 前序遍历 + 中序遍历/后序遍历 + 中序遍历，就可以唯一确定一颗二叉树 但是如果前序、中序、后序遍历序列中包含了空节点，那么也可以唯一确定二叉树 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566class Codec &#123;public: // 序列化：二叉树转字符串 string serialize(TreeNode* root)&#123; string res; dfs1(root, res); return res; &#125; void dfs1(TreeNode* root, string &amp;res)&#123; // 如果当前节点为空 if (!root)&#123; res += &quot;#,&quot;; return; &#125; // 否则，先遍历根节点，先加上当前节点的值，加上逗号 res += to_string(root-&gt;val) + &#x27;,&#x27;; // 遍历完根节点，就遍历左子树和右子树 dfs1(root-&gt;left, res); dfs1(root-&gt;right, res); &#125; // 反序列化：字符串转二叉树 TreeNode* deserialize(string data)&#123; // 需要一个指针，指向当前构建到哪个字符了 int u = 0; return dfs2(data, u); &#125; // 返回根节点的指针 TreeNode* dfs2(string &amp;data, int &amp;u)&#123; // 如果当前字符是#，那就跳过2个位置 if (data[u] == &#x27;#&#x27;)&#123; u += 2; return NULL; &#125; // 当前节点不空，那就先求出当前节点的值 // 节点中的值可能是负数，那就需要特判 int t = 0; bool is_minus = false; if (data[u] == &#x27;-&#x27;)&#123; is_minus = true; u ++ ; &#125; // 只要指针没有碰到逗号，就一直做 while (data[u] != &#x27;,&#x27;)&#123; // 计算值 t = t * 10 + data[u] - &#x27;0&#x27;; u ++ ; &#125; // 最后还有加加，是为了跳过逗号 u ++ ; // 是否是负数 if (is_minus) t = -t; // 创建根节点，之后是根左右的顺序 auto root = new TreeNode(t); root-&gt;left = dfs2(data, u); root-&gt;right = dfs2(data, u); return root; &#125;&#125;; 38.计数然后说 1234567891011121314151617181920class Solution &#123;public: string countAndSay(int n)&#123; string s = &quot;1&quot;; // 产生第n行，递推n-1次即可 for (int i = 0; i &lt; n - 1; i ++ )&#123; // 先定义上一行的字符串，是空的 string ns; // 然后遍历当前这一行中所有连续的段，从前往后枚举每一个位置 for (int j = 0; j &lt; s.size(); j ++ )&#123; int k = j; while (k &lt; s.size() &amp;&amp; s[k] == s[j]) k ++ ; ns += to_string(k - j) + s[i]; j = k - 1; &#125; s = ns; &#125; return s; &#125;&#125;; 49.Group Anagrams 1234567891011121314151617class Solution&#123;public: vector&lt;vector&lt;string&gt;&gt; groupAnagrams(vector&lt;string&gt;&amp; strs)&#123; unordered_map&lt;string, vector&lt;string&gt;&gt; hash; for (auto str : strs)&#123; string key = str; // 乱序字符串的本质是排序后都是一样的 sort(key.begin(), key.end()); // 将排序后一样的字符串作为key，原来的字符串作为值，这样就做到了分组的效果 hash[key].push_back(str); &#125; vector&lt;vector&lt;int&gt;&gt; res; for (auto item : hash) res.push_back(item.second); return res; &#125;&#125;; 151.Reverse Words in a String 123456789101112131415161718192021222324class solution&#123;public: string reverseWords(string s)&#123; int k = 0; for (int i = 0; i &lt; s.size(); i ++ )&#123; // 过滤所有的空格 while (i &lt; s.size() &amp;&amp; s[i] != &#x27; &#x27;) i ++ ; if (i == s.size()) break; // 找到一段连续的非空格 int j = i; while (j &lt; s.size() &amp;&amp; s[j] != &#x27; &#x27;) j ++ ; reverse(s.begin() + i, s.begin() + j); // 若k不是0，则先需要隔开一个空格 if (k) s[k ++ ] = &#x27; &#x27;; // 复制过程 while (i &lt; j) s[k ++ ] = s[i ++ ]; &#125; // k后多余的部分删去 s.erase(s.begin() + k, s.end()); // 最后再翻转一遍 reverse(s.begin(), s.end()); return s; &#125;&#125;; 165.Compare Version Numbers 如何找到字符串中的数字？ 1234567891011121314151617181920212223class Solution&#123;public: int compareVersion(string s1, string s2)&#123; // 定义两个指针，分别从0开始 int i = 0, j = 0; // 两个指针只要有一个不空，就一直做 while (i &lt; s1.size() || j &lt; s2.size())&#123; int x = i, y = j; while (x &lt; s1.size() &amp;&amp; s1[x] != &#x27;.&#x27;) x ++ ; while (y &lt; s2.size() &amp;&amp; s2[y] != &#x27;.&#x27;) y ++ ; // 特判一些，可能为0 int a = i == x ? 0 : atoi(s1.substr(i, x - i).c_str()); int b = j == y ? 0 : atoi(s2.substr(j, y - j).c_str()); // 返回答案 if (a &gt; b) return 1; if (a &lt; b) return -1; // 跳过&#x27;.&#x27; i = x + 1, j = y + 1; &#125; return 0; &#125;&#125;; 929.Unique Email Addresses先处理邮箱字符串 后使用哈希表数出不同的邮箱的种类 12345678910111213141516class Solution&#123;public: int numUniqueEmails(vector&lt;string&gt;&amp; emails)&#123; unordered_set&lt;string&gt; hash; for (auto email : emails)&#123; int at = email.find(&#x27;@&#x27;); string name; for (auto c : email.substr(0, at)); if (c == &#x27;+&#x27;) break; else if (c != &#x27;.&#x27;) name += c; string doumain = email.substr(at + 1); hash.insert(name + &#x27;@&#x27; + doumain); &#125; return hash.size(); &#125;&#125;; 5.Longest Palindromic Substring返回最长回文串 123456789101112131415161718192021class Solution&#123;public: string longestPalindrome(string s)&#123; string res; // 遍历字符串 for (int i = 0; i &lt; s.size(); i ++ )&#123; int j, k; // j 往左走，k 往右走 // 回文串长度是奇数 for (j = i, k = i; j &gt;= 0 &amp;&amp; k &lt; s.size() &amp;&amp; s[j] == s[k]; j --, k ++ )&#123; if (res.size() &lt; k - j + 1) res = s.substr(j, k - j + 1); &#125; // 回文串长度是偶数 for (j = i, k = i + 1; j &gt;= 0 &amp;&amp; k &lt; s.size() &amp;&amp; s[j] == s[k]; j --, k ++ )&#123; if (res.size() &lt; k - j + 1) res = s.substr(j, k - j + 1); &#125; return res; &#125; &#125;&#125;; 6.ZigZag Conversion 1234567891011121314151617181920212223class Solution&#123;public: string convert(string s, int n)&#123; // 特判，后面有i == n - 1的判断 if (n == 1) return s; string res; // 枚举行 for (int i = 0; i &lt; n; i ++ )&#123; // i = 0 或者 i == n - 1就是公差是2 * (n - 1)的等差数列 if (!i || i == n - 1)&#123; for (int j = i; j &lt; s.size(); j += 2 * (n - 1)) res += s[j]; &#125;else&#123; // 两个等差数列交错 for (int j = i, k = 2 * (n - 1) - i; j &lt; s.size() || k &lt; s.size(); j += 2 * (n - 1), k += 2 * (n - 1))&#123; if (j &lt; s.size()) res += s[j]; if (k &lt; s.size()) res += s[k]; &#125; &#125; &#125; return res; &#125;&#125;; 3.Longest Substring Without Repeating Characters最长的没有包含重复字符的字符串 使用哈希表存储两个指针间出现没有重复字母的次数 12345678910111213141516class Solution&#123;public: int lengOfLongestSubstring(string s)&#123; unordered_map&lt;char, int&gt; hash; int res = 0; for (int i = 0, j = 0; i &lt; s.size(); i ++ )&#123; // 绿颜色的指针向后走的时候，会加进来一个新的字母 hash[s[i]] ++ ; // 如果加完之后，两个指针间有重复字母，那么这个字母一定是s[i] // 有重复，那么将s[i]从Hash中删去，并让j后移 while (hash[s[i]] &gt; 1) hsah[s[j ++ ]] -- ; res = max(i - j + 1); &#125; return res; &#125;&#125;; 208.Implement Tire (Prefix Tree) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071class Trie &#123;public: // Tire树节点一般用结构体表示 struct Node &#123; // 有没有以这个节点为结尾的单词 bool is_end; // 最多26个儿子 Node *son[26]; // 初始化 Node() &#123; is_end = false; for (int i = 0; i &lt; 26; i ++ ) son[i] = NULL; &#125; &#125;*root; /** Initialize your data structure here. */ Trie() &#123; // 先创建根节点 root = new Node(); &#125; /** Inserts a word into the trie*/ void insert(string word) &#123; // 就从根节点开始遍历 auto p = root; // 从前往后遍历每一个字母 for (auto c : word) &#123; // 首先存储这个字母的编号 int u = c - &#x27;a&#x27;; // 判断儿子是否存在，不存在就创建 if (p-&gt;son[u] == NULL) p-&gt;son[u] = new Node(); // p走到儿子上去 p = p-&gt;son[u]; &#125; // 最后标记一下，现在有了以p结尾的单词 p-&gt;is_end = true; &#125; // bool search(string word) &#123; // 查询也是一样，从前往后遍历这个单词 auto p = root; for (auto c : word) &#123; // 求一下当前的编号 int u = c - &#x27;a&#x27;; // 如果当前儿子不存在的话，说明当前路径不存在，也就是说该单词不存在 if (p-&gt;son[u] == NULL) return false; // 否则p继续向下走即可 p = p-&gt;son[u]; &#125; // 最后进行判断是否有单词结束标记 return p-&gt;is_end; &#125; /** Returns if there is any word in the trie that starts with the given prefix*/ bool startWith(string prefix) &#123; // 查询也是一样，从前往后遍历这个单词 auto p = root; for (auto c : prefix) &#123; // 求一下当前的编号 int u = c - &#x27;a&#x27;; // 如果当前儿子不存在的话，说明当前路径不存在，也就是说该单词不存在 if (p-&gt;son[u] == NULL) return false; // 否则p继续向下走即可 p = p-&gt;son[u]; &#125; // 找前缀，直接返回即可 return true; &#125;&#125;; 273.Iteger to English Words 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051class Solution &#123;public: string small[20] = &#123;&quot;Zero&quot;, &quot;One&quot;, &quot;Two&quot;, &quot;Three&quot;, &quot;Four&quot;, &quot;Five&quot;, &quot;Six&quot;, &quot;Seven&quot;, &quot;Eight&quot;, &quot;Nine&quot;, &quot;Ten&quot;, &quot;Eleven&quot;, &quot;Twelve&quot;, &quot;Thirteen&quot;, &quot;Fourteen&quot;, &quot;Fifteen&quot;, &quot;Sixteen&quot;, &quot;Seventeen&quot;, &quot;Eighteen&quot;, &quot;Nineteen&quot;&#125;; string decade[10] = &#123;&quot;&quot;, &quot;&quot;, &quot;Twenty&quot;, &quot;Thrity&quot;, &quot;Forty&quot;, &quot;FIfty&quot;, &quot;Sixty&quot;, &quot;Seventy&quot;, &quot;Eighty&quot;, &quot;Ninety&quot;&#125;; string big[4] = &#123;&quot;Billion&quot;, &quot;Million&quot;, &quot;Thousand&quot;, &quot;&quot;&#125;; string numberToWords(int num) &#123; // 先判断num为0的特殊情况 if (!num) return small[0]; string res; // 开始枚举 for (int i = 100000000, j = 0; i &gt; 0; i /= 1000, j ++ ) &#123; if (num &gt;= i) &#123; res += get_part(num / i) + big[j] + &#x27; &#x27;; // 做完以后去除最大的三位 num %= i; &#125; &#125; // 最后如果会多出很多空格，需要删除 while (res.back() == &#x27; &#x27;) res.pop_back(); return res;&#125; // 输入1000以内的数，然后输出它 string get_part(int num) &#123; string res; // 如果大于等于100 if (num &gt;= 100) &#123; // 先把百位输出 res += small[num / 100] + &quot; Hundred &quot;; // 取出十位加个位数字，百位去除了 num %= 100; &#125; // 此时如等于0了，就不需要再做了 if (!num) return res; // 如果小于20，可以直接取small if (num &gt;= 20) &#123; // 先输出十位 res += decade[num / 10] + &#x27; &#x27;; // 拿出个位，去除百位和十位 num %= 10; &#125; // 此时如等于0了，就不需要再做了 if (!num) return res; // 最后加上小于20后的数字即可 res += small[num] + &#x27; &#x27;; return res; &#125; &#125;; 4.寻找两个正序数组的中位数 1234567891011121314151617181920212223242526272829303132333435363738394041class Solution &#123;public: double findMedianSortedArrays(vector&lt;int&gt;&amp; nums1, vector&lt;int&gt;&amp; nums2) &#123; // 先求数的总数 int tot = nums1.size() + nums2.size(); // 分两种情况 // 如果 tot 是偶数 if (tot % 2 == 0) &#123; int left = find(nums1, 0, nums2, 0, tot / 2); int right = find(nums1, 0, nums2, 0, tot / 2 + 1); return (left + right) / 2.0; &#125; else &#123; return find(nums1, 0, nums2, 0, tot / 2 + 1); &#125; &#125; int find(vector&lt;int&gt;&amp; nums1, int i, vector&lt;int&gt;&amp; nums2, int j, int k) &#123; // 假定第一个数组比较短，如果比较长的话，反过来做 if (nums1.size() - i &gt; nums2.size() - j) return find(nums2, j, nums1, i, k); // 处理边界问题，如果k=1,也就是只有一个数的话，那么取前两个数组的最小值即可，第一个数是第一数组的最小值，第二个数是第二数组的最小值，整个的最小值就是它们俩的最小值 // 但是还需要特判 if (k == 1) &#123; // 第一个数组是空的 if (nums1.size() == i) return nums2[j]; // 否则返回最小值 else return min(nums1[i], nums2[j]); &#125; // 边界二，数组1是空的，那么k就是第二个数组的中位数 if (nums1.size() == i) return nums2[j + k - 1]; // si sj 是中间位置的下一个位置 int si = min((int)nums1.size(), i + k / 2), sj = j + k - k / 2; if (nums1[si - 1] &gt; nums2[sj - 1]) &#123; // 删除数组2的右半边 return find(nums1, i, nums2, sj, k - (sj = j)); &#125; else &#123; // 删除数组1的右半边 return find(nums1, si, nums2, j, k - (si - i)); &#125; &#125;&#125;; 17.Letter Combinations of a Phone Number 123456789101112131415161718192021222324252627class Solution &#123;public: // 备选字符 string chars[8] = &#123;&quot;abc&quot;, &quot;def&quot;, &quot;ghi&quot;, &quot;jkl&quot;, &quot;mno&quot;, &quot;pqrs&quot;, &quot;tuv&quot;, &quot;wxyz&quot;&#125;; vector&lt;string&gt; letterCombinations(string digits) &#123; // 如果数字为空的话，返回的字符串为空 if (digits.empty()) return vector&lt;string&gt;(); vector&lt;string&gt; state(1, &quot;&quot;); // 枚举所有的数字 for (auto u : digits) &#123; // 每次定义一个新的状态 vector&lt;string&gt; now; // 取当前数字的备选字符，从 2 开始才有备选方案 for (auto c : chars[u - &#x27;2&#x27;]) &#123; // 枚举上一次的所有状态 for (auto s : state) &#123; now.push_back(s + c); &#125; &#125; state = now; &#125; return state; &#125;&#125;; 79.Word Search 123456789101112131415161718192021222324252627282930313233343536373839404142class Solution &#123;public: int n, m; // 矩阵的长和宽 int dx[4] = &#123;-1, 0, 1, 0&#125;, dy[4] = &#123;0, 1, 0, -1&#125;; bool exist(vector&lt;&lt;char&gt;&gt;&amp; board, string word) &#123; // 如果矩阵是空的，或者行是空的，直接返回 if (board.empty() || board[0].empty()) return false; n = board.size(), m = board[0].size(); for (int i = 0; i &lt; n; i ++ ) for (int j = 0; j &lt; m; j ++ ) if (dfs(board, i, j, word, 0)) return true; return false; &#125; // x, y 表示矩阵中的位置，u 表示单词中的位置 bool dfs(vector&lt;vector&lt;char&gt;&gt;&amp; board, int x, int y, string&amp; word, int u) &#123; // 如果当前矩阵的位置上字符不等于单词的字符 if (board[x][y] != word[u]) return false; // 如果全部匹配了，直接返回 if (u == word.size() - 1) return true; // 否则，当前这个 xy 位置已经使用了，需要标记，后续不再使用 board[x][y] = &#x27;.&#x27;; // 枚举上下左右四个方向 for (int i = 0; i &lt; 4; i ++ ) &#123; int a = x + dx[i], b = y + dy[i]; // 判断是否在界内 if (a &gt;= 0 &amp;&amp; a &lt; n &amp;&amp; b &gt;= 0 &amp;&amp; b &lt; m) &#123; // 如果走到下一个格子，可以匹配的话 if (dfs(board, a, b, word, u + 1)) return true; &#125; &#125; // 恢复现场 board[x][y] = word[u]; // 否则，最后都没有匹配成功的话，就直接返回 return false; &#125;&#125;; 46.Permutations 1234567891011121314151617181920212223242526272829303132333435class Solution &#123;public: int n; // 输入大小 vector&lt;bool&gt; st; // 当前这个分支里面可以用的数字是哪些 vector&lt;vector&lt;int&gt;&gt; ans; // 答案 vector&lt;int&gt; path; // 存当前方案 vector&lt;vector&lt;int&gt;&gt; permute(vector&lt;int&gt;&amp; nums) &#123; n = nums.size(); st = vector&lt;bool&gt;(n); dfs(nums, 0); return ans; &#125; void dfs(vector&lt;int&gt;&amp; nums, int u) &#123; if (u == n) &#123; ans.push_back(path); return; &#125; for (int i = 0; i &lt; n; i ++ ) &#123; if (!st[i]) &#123; st[i] = true; path.push_back(nums[i]); dfs(nums, u + 1); st[i] = false; path.pop_back(); &#125; &#125; &#125;&#125;; 47.Permutations II 123456789101112131415161718192021222324252627282930313233343536class Solution &#123;public: int n; vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; path; vector&lt;bool&gt; st; vector&lt;vector&lt;int&gt;&gt; permuteUnique(vector&lt;int&gt;&amp; nums) &#123; n = nums.size(); st = vector&lt;bool&gt;(n); path = vector&lt;int&gt;(n); dfs(nums, 0, 0); return ans; &#125; void dfs(vector&lt;int&gt;&amp; nums, int u, int start) &#123; if (u == n) &#123; ans.push_back(path); return; &#125; for (int i = start; i &lt; n; i ++ ) &#123; if (!st[i]) &#123; st[i] = true; path[i] = nums[i]; dfs(nums, u + 1, u + 1 &lt; n &amp;&amp; nums[u + 1] == nums[u] ? i + 1 : 0); st[i] = false; path[i] = 0; &#125; &#125; &#125;&#125;; 78.Subsets 12345678910111213141516171819class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; subsets(vector&lt;int&gt;&amp; nums) &#123; vector&lt;vector&lt;int&gt;&gt; res; // 左移表示 2^n for (int i = 0; i &lt; 1 &lt;&lt; nums.size(); i ++ ) &#123; vector&lt;int&gt; now; // 当前的集合 for (int j = 0; j &lt; nums.size(); j ++ ) &#123; // 如果 i 的第 j 位是 1 的话，就加入集合 if (i &gt;&gt; j &amp; 1) &#123; now.push_back(nums[j]); &#125; &#125; res.push_back(now); &#125; return res; &#125;&#125;; 90.Subsets II 12345678910111213141516171819202122232425262728293031323334class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; subsetsWithDup(vector&lt;int&gt; &amp; nums) &#123; // 为了把相同数字放在一起，先排序 sort(nums.begin(), nums.end()); dfs(nums, 0); return ans; &#125; void dfs(vector&lt;int&gt;&amp; nums, int u) &#123; if (u == nums.size()) &#123; ans.push_back(path); return; &#125; // 计算当前数字的个数 int k = 0; while (u + k &lt; nums.size() &amp;&amp; nums[u + k] == nums[u]) k ++ ; // 每次从 0 枚举到 k for (int i = 0; i &lt;= k; i ++ ) &#123; dfs(nums, u + k); path.push_back(nums[u]); &#125; // 恢复现场 for (int i = 0; i &lt;= k; i ++ ) path.pop_back(); &#125;&#125;; 216.Combination Sum III 12345678910111213141516171819202122232425262728class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; ans; vector&lt;int&gt; path; vector&lt;vector&lt;int&gt;&gt; combinationSum3(int k, int n) &#123; dfs(k, 1, n); return ans; &#125; void dfs(int k, int start, int n) &#123; // 如果当前枚举完了所有数的话 if (!k) &#123; // 再判断当前的和是否是 n if (!n) return ans.push_back(path); return; &#125; // 从 start 位置开始往后枚举 i &lt;= 10 - k for (int i = start; i &lt;= 9; i ++ ) &#123; // 先把 i 加进来 path.push_back(i); // 枚举下一个数，是倒着枚举，从 i + 1 位置开始找，总和就是 n - i dfs(k - 1, i + 1, n - i); // 恢复现场 path.pop_back(); &#125; &#125;&#125;; 52.N-Queens II 123456789101112131415161718192021222324252627282930class Solution &#123;public: int ans = 0, n; vector&lt;bool&gt; col, d, ud; int totalNQueens(int _n) &#123; n = _n; col = vector&lt;bool&gt;(n); d = ud = vector&lt;bool&gt;(n * 2); // 从前往后枚举每一行 dfs(0); return ans; &#125; void dfs(int u) &#123; if (u == n) &#123; ans ++ ; return; &#125; // 枚举每一列 for (int i = 0; i &lt; n; i ++ ) if (!col[i] &amp;&amp; !d[u + i] &amp;&amp; !ud[u - i + n]) &#123; col[i] = d[u + i] = ud[u - i + n] = true; dfs(u + 1); col[i] = d[u + i] = ud[u - i + n] = false; &#125; &#125;&#125;;","tags":["C++算法题目"],"categories":["软件开发"]},{"title":"编辑大型语言模型：问题、方法和机遇","path":"/2023/11/02/bian-ji-da-xing-yu-yan-mo-xing-wen-ti-fang-fa-he-ji-yu/","content":"编辑大型语言模型：问题、方法和机遇Abstruct尽管有能力培养有能力的LLMs，但维持其相关性和纠正错误的方法仍然难以捉摸。为此，过去几年见证了LLMs编辑技术的激增，其目标是有效地改变特定领域内LLMs的行为，而不会对其他输入的性能产生负面影响。本文深入探讨了LLMs模型编辑相关的问题、方法和机遇。特别是，我们对任务定义和与模型编辑相关的挑战进行了详尽的概述，并对我们目前掌握的最先进的方法进行了深入的实证分析。我们还构建了一个新的基准数据集，以促进更稳健的评估并查明现有技术固有的持久问题。我们的目标是为每种编辑技术的有效性和可行性提供有价值的见解，从而帮助社区做出明智的决定，为特定任务或上下文选择最合适的方法。 1 Introduction大型语言模型（LLM）已经表现出理解和生成类人文本的非凡能力（Brown et al., 2020；OpenAI, 2023；Anil et al., 2023；Touvron et al., 2023；Qiao et al., 2022；赵等人，2023）。尽管LLMs的训练非常熟练，但确保其相关性和修复错误的策略仍不清楚。理想情况下，随着世界形势的发展，我们的目标是更新LLMs，避免与训练全新模型相关的计算负担。如图1所示，解决这个问题模型编辑的概念被提出 （Sinitsin 等人，2020；De Cao 等人，2021），能够对模型的行为进行数据有效的改变，特别是在指定的感兴趣领域内，同时确保不会对其他输入产生不利影响。目前，大量关于LLMs模型编辑的工作（De Cao et al., 2021；Meng et al., 2022, 2023；Sinitsin et al., 2020；Huang et al., 2023)在各种编辑任务和设置方面取得了长足的进步。如图 2 所示，这些工作通过将辅助网络与原始未更改的模型集成或更改导致不良输出的模型参数来操纵特定情况下的模型输出。尽管文献中存在广泛的模型编辑技术，但明显缺乏在统一实验条件下评估这些方法的全面比较分析。缺乏直接比较会削弱我们辨别每种方法相对优缺点的能力，从而阻碍我们理解它们在不同问题领域的适应性。 为了解决这个问题，本研究致力于建立一个标准的问题定义，并对这些方法进行细致的评估（§2，§3）。我们在规定的条件下进行实验，促进对各自的优缺点进行公正的比较（§4）。&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;我们最初使用两个流行的模型编辑数据集，ZsRE (Levy et al., 2017) 和 COUNTERFACT (Meng et al., 2022)，以及两个结构上的数据集不同的语言模型，T5（Raffel et al.，2020a）（编码器-解码器）和 GPT-J（Wang 和 Komatsuzaki，2021a）（仅解码器）作为我们的基础模型&lt;/span&gt;。我们还评估了较大模型 OPT-13B（Zhang 等人，2022a）和 GPT-NEOX20B（Black 等人，2022）的性能。除了基本编辑设置之外，我们还评估批量和顺序编辑的性能。虽然我们观察到当前的方法在事实模型编辑任务中表现出当大的能力，但&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;我们重新考虑当前的评估并创建一个更具包容性的评估数据集（§5）：可移植性（强大的泛化能力）、局部性（副作用）和效率（时间）和内存使用情况）&lt;/span&gt;。我们发现当前的模型编辑方法在这些层面上有所限制，从而限制了它们的实际应用，未来值得更多的研究。通过系统评估，我们的目标是为每种模型编辑技术的有效性提供有价值的见解，帮助研究人员为特定任务选择合适的方法。 2 Problem Definition模型编辑，由 Mitchell 等人阐明。 （2022b），旨在有效地调整特定编辑描述符（xe，ye）上的初始基础模型（fθ，θ表示模型的参数）行为，而不影响其他样本上的模型行为。最终目标是创建一个编辑模型，表示为 fθe。具体来说，基本模型 fθ 由函数 f : X → Y 表示，该函数将输入 x 与其相应的预测 y 相关联。给定一个由编辑输入 xe 和编辑标签 ye 组成的编辑描述符，使得 fθ(xe) ̸= ye，后期编辑模型 fθe 被设计为产生预期输出，其中 fθe(xe) = ye。 &lt;span style=&quot;background-color: #2ea8e580&quot;&gt;模型编辑过程通常会影响与编辑示例密切相关的大量输入的预测。这个输入集合称为编辑范围。&lt;/span&gt;成功的编辑应该调整编辑范围内示例的模型行为，同时保持范围外示例的性能不变： 范围内 I(xe, ye) 通常包含 xe 及其等价邻域 N (xe, ye)，其中包括相关的输入/输出对。相反，超出范围的 O(xe, ye) 由与编辑示例无关的输入组成。模型fe应该满足以下三个属性：可靠性、泛化性和局部性。 可靠性 先前的工作（Huang et al., 2023；De Cao et al., 2021；Meng et al., 2022）定义了当后期编辑模型 fθe 给出案例 (xe, ye) 的目标答案时的可靠编辑被编辑。可靠性以编辑案例的平均准确度来衡量： 泛化 编辑后模型 fθe 还应该编辑等效邻居 N (xe, ye)（例如改写的句子)。它是通过模型 fθe 在从等价邻域中均匀抽取的示例上的平均精度来评估的： 局部性 在一些工作中，也被称为特异性。编辑应该在本地实现，这意味着编辑后模型 fθe 不应更改范围外 O(xe, ye) 中不相关示例的输出。因此，局部性是通过编辑后模型 fθe 的预测与编辑前 fθ 模型相同的来评估的 3 Current Methods目前LLMs的模型编辑方法可以分为两种主要范式，如图2所示：修改模型参数或保留模型参数。更多比较见表 6。 3.1 Methods for Preserving LLMs Parameters基于内存的模型 这种方法将所有编辑示例显式存储在内存中，并使用检索器为每个新输入提取最相关的编辑事实，以指导模型生成编辑事实。 SERAC（Mitchell 等人，2022b）提出了一种采用独特的反事实模型，同时保持原始模型不变的方法。具体来说，它采用范围分类器来计算新输入落入存储的编辑示例范围内的可能性。如果输入与内存中任何缓存的编辑相匹配，则反事实模型的预测将基于输入和最可能的编辑。否则，如果输入超出了所有编辑的范围，给出了原始模型的预测。此外，最近的研究表明LLMs拥有强大的情境学习能力。模型本身可以生成与所提供的知识相对应的输出，而不是求助于用新事实训练的额外模型，并给出精炼的知识上下文作为提示。这种方法通过用编辑后的事实提示模型并从编辑记忆中检索编辑演示来编辑语言模型，包括以下工作：MemPrompt (Madaan et al., 2022)、IKE (Zheng et al., 2023) 和MeLLo（Zhong 等人，2023）。 附加参数 此范例在语言模型中引入了额外的可训练参数。这些参数在修改后的知识数据集上进行训练，而原始模型参数保持静态。 T-Patcher（Huang et al., 2023）在模型前馈网络（FFN）的最后一层针对一个错误集成了一个神经元（补丁），仅在遇到其对应错误时才生效。 CaliNET（Dong et al., 2022）整合了多个神经元以用于多个编辑案例。不同的是，GRACE（Hartvigsen et al., 2022）维护一个离散的密码本作为适配器，随着时间的推移添加和更新元素以编辑模型的预测。 3.2 Methods for Modifying LLMs Paramete该范例将更新部分参数 θ，它应用更新 Δ 矩阵来编辑模型。 定位然后编辑 该范例首先识别与特定知识相对应的参数，并通过直接更新目标参数来修改它们。知识神经元（KN）方法（Dai et al., 2022）引入了知识归因技术来精确定位体现知识的“知识神经元”（FFN 矩阵中的键值对），然后更新这些神经元。 ROME（Meng et al., 2022）应用因果中介分析来定位编辑区域。 ROME 不是修改 FFN 中的知识神经元，而是改变整个矩阵。 ROME 将模型编辑视为具有线性等式约束的最小二乘法，并使用拉格朗日乘子来求解。然而，KN 和 ROME 一次只能编辑一个事实关联。为此，MEMIT（Meng et al., 2023）对ROME的设置进行了扩展，实现了多病例同步编辑的情况。基于 MEMIT，PMET（Li et al., 2023a）涉及注意力值以获得更好的性能。 元学习 元学习方法采用超网络来学习编辑 LLM 所需的 Δ。知识编辑器（KE）（De Cao et al., 2021）利用超网络（特别是双向 LSTM）来预测每个数据点的权重更新，从而能够在不干扰其他知识的情况下对编辑目标知识进行约束优化。然而，这种方法在编辑LLMs方面存在不足。为了克服这个限制，模型编辑器网络梯度分解（MEND）（Mitchell et al., 2022a）学习通过采用梯度的低秩分解来变换微调语言模型的梯度，这可以应用于具有更好性能的LLM。 4 Preliminary Experiments考虑到大量以事实知识为中心的研究和数据集，我们将其用作主要比较基础。我们最初的对照实验使用两个著名的事实知识数据集（表 1）进行，促进了方法的直接比较，突出了它们独特的优势和局限性（Wang 等人，2023b）。 4.1 Experiment Setting我们使用两个著名的模型编辑数据集：ZsRE 和 COUNTERFACT，其详细信息请参见附录 B。以前的研究通常使用较小的语言模型 (&lt;1B)，并证明了当前编辑方法在 BERT 等较小模型上的有效性（Devlin 等人， 2019）。然而，这些方法是否适用于更大的模型仍有待探索。因此，考虑到编辑任务和未来的发展，我们专注于基于生成的模型并选择更大的模型：T5-XL（3B）和GPT-J（6B），代表编码器-解码器和仅解码器结构。 我们从每种方法类型中选择了有影响力的作品。除了现有的模型编辑技术之外，我们还检查了微调的结果，这是模型更新的基本方法。为了避免重新训练所有层的计算成本，我们采用了Meng等人提出的方法。 (2022)，由 ROME 识别的微调层，我们将其表示为 FT-L。该策略确保与其他直接编辑进行公平比较方法，增强我们分析的有效性。更多详细信息请参见附录 A。 4.2 Experiment Results基本模型表 1 揭示了 SERAC 和 ROME 在 ZsRE 和 COUNTERFACT 数据集上的卓越性能，SERAC 在多个指标上超过 90%。虽然 MEMIT 缺乏通用性，但它在可靠性和局部性方面表现出色。 KE、CaliNET 和 KN 表现不佳，在较小的模型中表现尚可，但在较大的模型中表现平平。 MEND 在这两个数据集上表现良好，在 T5 上的结果达到了 80% 以上，尽管不如 ROME 和 SERAC 那样令人印象深刻。 T-Patcher 模型的性能因模型架构和大小的不同而有所不同。例如，它在 ZsRE 数据集的 T5-XL 上表现不佳，而在 GPT-J 上表现完美。在 COUNTERFACT 数据集的情况下，T-Patcher 在 T5 上实现了令人满意的可靠性和局部性，但缺乏泛化性。相反，在 GPT-J 上，该模型在可靠性和泛化性方面表现出色，但在局部性方面表现不佳。这种不稳定性可归因于模型架构，因为 T-Patcher 在 T5 的最终解码器层添加了一个神经元；然而，编码器可能仍然保留原始知识。 FT-L 在 PLM 上的表现不如 ROME，即使修改相同的位置。它在 ZsRE 数据集上显示出令人印象深刻的性能，但在 GPT-J 上的 COUNTERFACT 数据集上与 ROME 的可靠性和泛化能力相当。然而，其较低的局部性得分表明对不相关知识领域的潜在影响。 IKE 表现出良好的可靠性，但在局部性方面遇到困难，因为预先设置的提示可能会影响不相关的输入。它的泛化能力也可以提高。情境学习 该方法可能会遇到上下文调解失败的问题（Hernandez et al., 2023)，因为预先训练的语言模型可能无法始终生成与提示对齐的文本。 模型缩放 由于计算限制，我们使用更大的模型进行实验，在 OPT-13B 和 GPT-NEOX-20B 上测试 IKE、ROME 和 MEMIT。结果（表 2）令人惊讶地显示 ROME 和 MEMIT 在 GPT-NEOX-20B 模型上表现良好，但在 OPT-13B 上表现不佳。这是由于这两种方法都依赖于矩阵求逆运算。然而，在 OPT-13B 模型中，矩阵是不可逆的。我们甚至根据经验发现，用最小二乘法逼近解会产生不令人满意的结果。我们认为这是 ROME 和 MEMIT 的局限性，因为它们不能应用于不同的模型。 MEMIT 由于依赖多层矩阵计算而表现较差，并且对于较大模型，其可靠性和泛化性比 ROME 下降得更多。 IKE 的性能受到模型本身的上下文学习能力的影响。 OPT的结果比GPT-J的结果还要差，这可能归因于OPT本身的上下文学习能力。此外，随着模型大小的增加，其泛化和局部性的性能都会下降。 批量编辑 鉴于许多研究通常将更新限制为几十个事实或仅关注单个编辑案例，我们进行了进一步的批量编辑分析。然而，通常需要同时修改具有多个知识片段的模型。我们重点关注支持批量编辑的方法（FT、SERAC、MEND 和 MEMIT），并在图 3 中展示了它们的性能。值得注意的是，MEMIT 支持LLMs的大规模知识编辑，允许以最少的时间和内存进行数百甚至数千个同时编辑成本。其在可靠性和泛化方面的性能在最多 1000 次编辑时仍然保持稳健，但局部性在此级别下降。而 FT-L、SERAC、和MEND还支持批量编辑，它们需要大量内存来处理更多情况，超出了我们当前的能力。因此，我们将测试限制为 100 次编辑。 SERAC 可以完美地进行最多 100 次编辑的批量编辑。 MEND 和 FT-L 在批量编辑中的性能并不那么强，随着编辑数量的增加，模型的性能迅速下降。 顺序编辑 请注意，默认评估过程是更新单个模型知识，评估新模型，然后回滚更新，然后对每个测试点重复该过程。在实际场景中，模型在进行新的编辑时应保留先前的更改。因此，进行连续编辑的能力是模型编辑的一个重要特征（Huang et al., 2023）。我们评估了具有强大的单编辑性能的顺序编辑方法，并在图 4 中报告了结果。冻结模型参数的方法（如 SERAC 和 T-Patcher)通常在顺序编辑中表现出稳定的性能。然而，那些改变模型参数的人却很困难。 ROME 在 n = 10 之前表现良好，然后在 n = 100 时下降。MEMIT 的性能也会在超过 100 次编辑后下降，但不如 ROME 大幅下降。同样，MEND 在 n = 1 时表现良好，但在 n = 10 时表现明显下降。随着编辑过程的继续，这些模型越来越偏离其原始状态，导致性能次优。 5 Comprehensive Study考虑到上述几点，我们认为以前的评估指标可能无法充分评估模型编辑能力。因此，我们提出对可移植性、局部性和效率进行更全面的评估。 5.1 Portability - Robust Generalization几项研究使用通过反向翻译生成的样本来评估泛化性（De Cao 等人，2021）。然而，这些释义的句子通常只涉及微小的措辞变化，并不能反映实质性的事实修改。正如 Jacques Thibodeau (2022) 中所述，验证这些方法是否能够处理编辑对实际应用程序的影响至关重要。因此，我们引入了一种称为可移植性的新评估指标，以衡量模型编辑在将知识转移到相关内容方面的有效性，称为鲁棒泛化。因此我们考虑三个方面：（1）主语替换：由于大多数改写的句子保留了主语描述，但更多地改写了关系，我们通过替换来测试泛化能力问题中的主题带有别名或同义词。这测试模型是否可以将编辑的属性推广到同一主题的其他描述。 (2)反向关系：当编辑主体和关系的目标时，目标实体的属性也发生变化。我们通过过滤合适的关系（例如一对一）并询问相反的问题来检查目标实体是否也更新来测试模型处理此问题的能力。 （3）一跳：修改后的知识应该可以被编辑后的语言模型用于下游任务。例如，如果我们更改“瓦茨·汉弗莱 (Watts Humphrey) 就读哪所大学？”这个问题的答案。从“三一学院”到“密歇根大学”，当被问到“Watts Humphrey 在大学学习期间住在哪个城市？”时，模型应该回答“密歇根州的安娜堡”而不是“爱尔兰的都柏林”。因此，我们构建了一个推理数据集来评估编辑后模型使用编辑知识的能力。 我们将一个新部分 P (xe, ye) 合并到现有数据集 ZsRE 中，可移植性计算为应用于 P (xe, ye) 中的推理示例时编辑模型 (fθe) 的平均准确度： 数据集构建 对于一跳数据集，在原始编辑中，我们将主题 s 的答案从 o 更改为 o。然后，我们提示模型生成链接的三元组 (o, r, o′)。随后，GPT-4 根据这个三元组和 s 创建一个问题和答案。尤其，如果模型可以回答这个新问题，意味着它具有三元组 (o, r, o′) 的预先存在的知识。我们通过要求模型从 o 和 r 预测 o’ 来过滤未知的三元组。如果成功，则推断该模型具有先验知识。最后，人类评估者验证三元组的准确性和问题的流畅性。其他详细信息，例如我们使用的演示和数据集构建的其他部分，可以在附录 B 中找到。 结果 我们根据新提出的评估指标和数据集进行实验，结果如表3所示。如表所示，当前模型编辑方法在可移植性方面的性能有些欠佳。尽管 SERAC 在之前的指标上显示出无可挑剔的结果，但在所有三个可移植性方面的准确度均低于 20%。 SERAC的瓶颈在于分类器的准确性和附加模型的能力。对于主题替换场景，包括SERAC、MEND、ROME和MEMIT，只能适应特定的主题实体表达，而不能泛化到主题实体的概念。然而，FT-L、IKE 和 T-patcher 在面对替换主题时表现出了出色的性能。关于反向关系，我们的结果表明，当前的编辑方法主要编辑单向关系，IKE 是一个明显的例外，在 GPT-J 和 GPT-NEOX-20B 上都达到了 90% 以上。其他方法改变主体实体的属性，同时保持客体实体不受影响。在一跳推理环境中，大多数编辑方法都难以将改变的知识转移到相关事实。出乎意料的是，ROME、MEMIT和IKE在可移植性方面表现出相对值得称赞的表现（超过50%）。他们不仅能够编辑原始案件，而且能够在某些方面修改与案件相关的事实。综上所述，在我们的评估中，IKE 在三个场景中都表现出了相对较好的性能。然而，很明显，当前的模型编辑技术在管理编辑的后果方面继续面临挑战，即确保知识的变化在相关上下文中连贯一致地反映。事实上，这一领域需要在未来的研究中进一步调查和创新。 5.2 Locality - Side Effect of Model Editing在上一节中，COUNTERFACT 和 ZsRE 从以下方面评估模型编辑的局部性：COUNTERFACT 使用与目标知识相同分布的三元组，而 ZsRE 使用来自不同自然问题数据集的问题。值得注意的是，一些方法（例如 T-Patcher）在这两个数据集上表现出不同的性能。这凸显出模型编辑对语言模型的影响是多方面的，需要进行彻底、全面的评估才能充分理解其效果。为了彻底检查模型编辑的潜在副作用，我们提出了三个不同层面的评估：（1）其他关系：尽管Meng等人。 (2022)引入了本质的概念，但他们没有明确评价它。我们认为，已更新的主题的其他属性在编辑后应保持不变。 (2)分散邻里的注意力：HoelscherObermaier等人。 （2023a）发现，如果我们将编辑后的案例连接在其他不相关的输入之前，模型往往会受到编辑后的事实的影响，并继续产生与编辑后的案例一致的结果。 (3) 其他任务：基于 Skill Neuron 的断言（Wang 等人，2022），即大语言模型（LLM）中的前馈网络拥有特定于任务的知识能力，我们引入了一个新的挑战来评估模型编辑是否可能对性能产生负面影响关于其他任务。数据集构建的详细信息请参见附录 B.3。 结果 表 4 列出了我们的结果。值得注意的是，当前的编辑方法在其他属性方面表现出色，表明它们仅修改目标特征而不影响其他属性。然而，它们在 Distract-Neighbor 设置中通常表现不佳，如与表 1 中的结果相比性能下降所反映的那样。IKE 是一个例外，它的性能保持相对稳定，因为它继承了以下事实： 完全需要在输入之前连接编辑后的事实。对于常识推理任务，参数保留方法在很大程度上保持了其在其他任务上的性能。相反，改变参数的方法往往会对性能产生负面影响，MEMIT 除外。尽管参数发生了变化，MEMIT 在常识性任务中仍然保持着强劲的性能，展示了其值得称赞的局部性。 5.3 Efficiency模型编辑应最大限度地减少进行编辑所需的时间和内存，而不影响模型的性能。 时间分析表5说明了不同模型编辑技术从提供编辑案例到获得发布后编辑模型所需的时间。我们观察到，一旦超网络经过训练，KE 和 MEND 就会以相当快的速度执行编辑过程。同样，SERAC 还可以快速编辑知识，在经过训练的分类器和反事实模型的情况下，在大约 5 秒内完成该过程。然而，这些方法需要数小时至数天的额外训练和额外的数据集。在我们的实验中，在 ZsRE 数据集上训练 MEND 需要超过 7 个小时，在 3× V100 上训练 SERAC 需要超过 36 个小时。另一方面，ROME 和 MEMIT 需要预先计算维基文本的协方差统计数据。然而，这种计算非常耗时，可能需要数小时至数天才能完成。相比之下，其他方法（例如 KN、CaliNET 和 T-Patcher）可能更快，因为它们不需要任何预计算或预训练。然而，KN 和 CaliNET 在较大模型上的性能 不能令人满意，T-Patcher 是最慢的，因为需要针对每个相应的错误进行单独的神经元训练。考虑到时间方面，需要一种更加省时的模型编辑方法。 内存分析 图 5 显示了每种模型编辑方法的内存 VRAM 使用情况。从该图中，我们观察到大多数方法消耗的内存量相似，但 MEND 除外，它需要超过 60GB 的内存用于训练。引入额外训练的方法（例如 MEND 和 SERAC）会导致额外的计算开销，从而显着增加内存消耗。 6 Relationship with Relevant Works6.1Knowledge in LLMs多种模型编辑方法旨在了解 PLM 中存储的知识如何精确且直接地改变模型参数。现有工作研究了 PLM 如何存储知识的原则（Geva 等人，2021、2022；Haviv 等人，2023；Hao 等人，2021；Hernandez 等人，2023；Yao 等人， 2023；Cao et al., 2023；Lamparth and Reuel, 2023；Cheng et al., 2023；Li et al., 2023b；Chen et al., 2023；Ju and Zhang, 2023），这些都有助于模型编辑过程。此外，一些模型编辑技术与知识增强相似（Zhang et al., 2019；Lewis et al., 2020；Zhang et al., 2022b；Yasunaga et al., 2021；Yao et al., 2022；Pan et al. ., 2023）方法，因为更新模型的知识也可以被视为将知识灌输到模型中。 6.2Lifelong Learning and Unlearning模型编辑包括终身学习和忘却，允许自适应地添加、修改和删除知识。持续学习（Biesialska et al., 2020）可以提高模型跨任务和领域的适应性，已在 PLM 中的模型编辑中显示出有效性（Zhu et al., 2020）。此外，模型忘记敏感知识并与机器遗忘概念保持一致至关重要（Hase 等人，2023；Wu 等人，2022；Tarun 等人，2021；Gandikota 等人，2023）。 6.3Security and Privacy for LLMs过去的研究（Carlini 等人，2020；Shen 等人，2023）表明，LLMs可以根据某些提示生成不可靠或个人的样本。删除大型语言模型 (LLM) 中存储的潜在有害信息和隐私信息的任务对于增强基于 LLM 的应用程序的隐私和安全性至关重要（Sun 等人，2023）。模型编辑可以抑制有害语言的生成（Geva et al., 2022；Hu et al., 2023），可以帮助解决这些问题。 7 Conclusion我们系统地分析了编辑大语言模型（LLM）的方法。我们的目标是通过检查现有编辑技术的特征、优势和局限性，帮助研究人员更好地理解现有编辑技术。我们的分析显示了很大的改进空间，特别是在可移植性、局部性和效率方面。改进的LLMs编辑可以帮助他们更好地适应用户不断变化的需求和价值观。我们希望我们的工作能够促进开放问题和进一步研究的进展。","tags":["EMNLP2023"],"categories":["NLP顶会"]},{"path":"/wiki/nlp-paper/Document-Level Event Argument Extraction With A Chain Reasoning Paradigm.html","content":"Document-Level Event Argument Extraction With A Chain Reasoning Paradigm 文档级事件参数提取旨在识别句子级别之外的事件参数，其中一个重大挑战是对远程依赖关系进行建模。针对这一挑战，我们为该任务提出了一种新的链式推理范式，它可以生成可分解的一阶逻辑规则进行推理。由于链的组合性质，这种范式自然地捕获了远程相互依赖，这也通过显式地建模推理过程来提高可解释性。我们引入 T 范数模糊逻辑进行优化，它允许端到端学习，并有望将逻辑推理的表达能力与神经网络的泛化相结合。在实验中，我们表明我们的方法在两个标准基准上明显优于以前的方法（F1 中超过 6 个点）。此外，它在资源匮乏的情况下具有数据效率，并且足够强大以防御对抗性攻击。 1. 识别事件参数（即事件的参与者）是文档级事件理解的一项关键任务（Ebner et al., 2020；Li et al., 2021）。在此任务中，主要挑战是对事件触发器和参数之间的远程依赖关系进行建模，因为事件表达式可以跨越多个句子（Ebner 等人，2020；Liu 等人，2021；Li 等人，2021）。考虑图 1 中由触发引爆（类型 = 攻击）表示的事件。为了定位其参数 Tartus（语义角色 = 地点），模型应捕获包含三个句子和 178 个单词的大型上下文窗口，以支持推理过程。 目前，有效捕获此类依赖关系仍然是一个悬而未决的问题（Liu et al., 2021, 2022c）。先前的研究提出建模通过结合分层编码机制（Du and Cardie，2020a）、生成范式（Li et al.，2021；Ma et al.，2022；Du et al.，2022）和文档级归纳偏差（Wei等人，2021；Pouran Ben Veyseh 等人，2022；Liu 等人，2022b)。然而，此类方法并未明确表征文档上下文背后的推理模式，这可能会导致性能不佳。此外，大多数以前的方法都是不可解释的，因为它们依赖于黑盒神经网络。 在本文中，我们提出了一种新的推理链范式来解决文档级事件参数提取（EAE）。如图 1 底部所示，我们的方法试图通过一系列局部推理步骤来描述全局参数查找过程。例如，我们可以使用以下链来定位塔尔图斯：引爆目标→Arzunah Bridge locatedIn→塔尔图斯。与以前的方法相比，这种推理链范式具有三个明显的好处：首先，由于推理链的组合结构，它自然地捕获了长距离依赖关系。其次，它只涉及局部推理，这在概念上比直接执行全局推理更容易。第三，它提高了可解释性，因为推理过程是可见的。 我们的方法将推理链形式化为一阶逻辑（FOL）规则（Cresswell 和 Hughes，1996）。具体来说，令 RL(T , ?) 为对事件参数的查询，该事件参数满足关于事件触发器 T 的语义角色 RL （例如，地点）。我们将查询形式化为以下 FOL 规则： 其中规则主体（右侧）由具有低级谓词 {ri}1n 和中间线索实体 {Bi}n−1 1 的合取命题组成。我们构建一个模型，根据文档上下文自动生成规则，然后将规则转换为推理链来定位事件参数。然而，由于 FOL 规则的离散性质，使用 FOL 规则进行优化通常具有挑战性（Qu 等人，2021a）。受到使用 FOL 增强神经网络的工作的启发（Li 和 Srikumar，2019；Ahmed 等人，2022），我们提出了用于松弛的 T-Norm 模糊逻辑（Hajek，1998），这导致了端到端的训练机制。 我们在两个基准上验证了我们的方法的有效性（Ebner 等人，2020；Li 等人，2021）。根据结果​​，我们的方法通过这种链式推理范式提供了有希望的结果，例如与使用大规模外部资源训练的模型相比，F1 提高了 6 个点（第 6.1 节）。有趣的是，除了性能提升之外，我们的方法还表现出良好的鲁棒性，特别是在资源匮乏的情况下和防御对抗性噪音（第 7.2 节）。最后，我们使用彻底的案例研究来评估我们方法的可解释性（第 7.3 节）。 2 相关工作 使用 FOL 规则进行推理。一阶逻辑（FOL）规则可以对声明性知识进行编码，并在符号推理中发挥至关重要的作用（Cresswell 和 Hughes，1996）。在深度学习时代，多项研究探讨了 FOL 规则与神经网络的集成以进行推理（称为神经符号方法），以及在知识库推理（Qu 等人，2021b）、文本蕴涵（Li 和 Srikumar）中的应用，2019）、问答（Wang 和 Pan，2022）以及其他（Medina 等人，2021；Ahmed 等人，2022）。我们的方法受到知识库推理工作的启发，据我们所知，这是在文档级 EAE 背景下合并 FOL 规则进行推理的首次尝试。与其他方法相比，我们研究了使用神经网络自动生成规则，而不是像（Li 和 Srikumar，2019；Wang 和 Pan，2022）那样采用专家编写的规则。此外，与基于强化学习的方法不同（Qu et al., 2021b），我们使用 T 范数进行规则松弛，从而形成具有更稳定学习过程的端到端训练范例。 3 方法 图 2 概述了我们的方法，并提供了一个提取事件引爆的 Place 角色参数的示例。令 D = {w1, · · · , T, · · · , wN } 为包含 N 个单词和事件触发器 T 的文档，并令 RL(T , ?) 为对语义角色 RL 的事件参数的查询。我们的方法不是直接执行可能涉及高级过程的推理，而是将查询表示为具有连接命题和低级谓词的 FOL 规则{ri}1n： 这样，规则的主体就暗示了一个推理链：T r1 → B1 r2 → · · · Bn−1 rn → ?。我们使用双谓词公式，特别是RL(T, ?) ← r1(T, B) ∧ r2(B, ?)为了解释我们的方法，我们在第 4 节中描述了一般情况。 3.1 在我们方法的第一步中，我们创建一组实体，可以从中选择一个实体作为中间线索实体以形成推理链（关于我们的双谓词结构）。我们扩大了“实体”的概念，以包含文档中的任何单个单词，以合并基于动词的提示。为了限制集合的大小，我们给每个单词一个分数源自 BERT 表示（Devlin 等人，2019）。例如，wi 的得分为： 其中hwi是wi的表示，ws和bs是模型参数。我们根据疮口对所有单词进行排序，并选择疮口最高的 K 个单词组成集合，表示为 B = {bi}iK=1。 为了促进训练和测试，我们还生成了一个参数候选集。在这种情况下，我们不使用实体的广义定义，因为事件参数被定义为名词实体（Walker and Consortium，2005；Ahn，2006）。当真实实体可用时（例如在 WikiEvents (Li et al., 2021) 中），我们将候选集视为真实实体集；否则，我们使用外部工具包2来识别实体。我们用 A = {ai}iL=1 表示参数候选集。 3.2 给定实体候选集B和参数候选集A，下一步是生成两个谓词并选择集合中的相关候选者以形成规则。在这里，我们解释了生成关于特定实体参数对（B ∈ B，A ∈ A）的谓词的方法，并且我们在第 4 节中展示了对不同候选对生成的规则进行排名的度量。 谓词表示。在我们的方法中，我们假设有 M 个原子谓词不可分解的语义，由谓词集 R = {Ri}iM=1 表示。我们给每个谓词一个 d 维向量化表示，并导出 R 的矩阵表示 U ∈ RM×d。对于语义角色 RL，我们还给它一个 d 维表示，由 rRL ∈ Rd 表示。 学习角色-谓词关联。给定这些表示，我们首先学习角色-顶级谓词关联，该关联指示哪些谓词可能仅基于角色而不考虑上下文而生成。我们采用自回归学习并生成概率向量 a(1) RL ∈ RM 指示第一个谓词 r1 在谓词集 R 上的分布： 其中 W (1) s ∈ Rd×d 是一个参数。为了了解第二个谓词 r2 的分布，我们首先通过整合第一个谓词的影响来更新角色的表示： 然后计算概率向量 a(2) RL ∈ RM ： 其中 W (1) a ∈ RM×d 和 W (2) s ∈ Rd×d 是要学习的参数。我们可以将 r1 和 r2 分别设置为 a(1) RL 和 a(2) RL 中概率最高的谓词。然而，这种方法总是为语义角色生成相同的谓词，并且性能相当差（7.1）。作为解决方案，我们引入了一种根据上下文对谓词重新排序的机制。 上下文相关谓词生成。设 X 和 Y 为两个实体。我们首先计算一个概率向量 v(X,Y ) ∈ RM 表示 (X,Y ) 与每个谓词 R ∈ R 的兼容性，以形成命题 R(X, Y )： 其中 hX ​​和 hY 是 X 和 Y 的表示，⊕ 是串联运算符，W ∈ Rm×2d 是模型参数。我们将兼容性概率与角色谓词关联概率相结合，以生成最终谓词。具体来说，对于事件触发器 T ，一定实体 B ∈ B 和参数候选 A ∈ A，我们生成以下两个谓词： 其中 是逐元素乘法运算符，sX 表示被选为候选线索实体集 B 中的实体 X 的得分（式（1））。这样，生成的 FOL 规则为 RL(T, A) ← r1(T, B) ∧ r2(B, A)，暗示到达事件参数 A 的推理路径： T r1 −→ B r2 −→ A 。\\4. 由于 FOL 规则的离散性，使用 FOL 规则进行优化通常具有挑战性（Qu 等人，2021a）。在这里，我们提出了用于松弛的 T-Norm 模糊逻辑，它产生了端到端的学习过程。 用于松弛的 T 范数模糊逻辑。 T-Norm 模糊逻辑通过承认 1（真值）和 0（假值）之间的中间真值来概括经典的二值逻辑。对于我们生成的 FOL 规则 RL(T, A) ← r1(T, B) ∧ r2(B, A)，我们将 r1(T, B) 和 r2(B, A) 的真值设置为相应的分数式（6）和式（7）中，分别记为p1和p2。然后，遵循 Łukasiewicz T-Norm 逻辑，两个命题的合取对应于： 我们将其重写为度量4：M (T, B, A) = p(r1(T, B) ∧ r2(B, A)) 并将其用于规则排序和优化。特别地，我们枚举每个实体参数对 (B, A) ∈ B × A，并用 ( ˆ B, ˆ A) 表示得分最高的一个。然后我们得出以下优化损失： 其中θ表示整体参数集（在训练时，ground-truth参数是已知的，我们可以直接将最优参数设置为ground-truth）。尽管我们的方法考虑了每个候选实体和参数，但我们通过并行张量运算表明，我们的方法与先前的方法一样有效地进行竞争（参见附录 A.1）。 概括为一般情况。我们使用结构二谓词结构来解释我们的方法，但是很容易将其适用于具有任意数量谓词的一般情况。现在假设一个 n 谓词结构。我们首先使用类似于等式 1 的自回归机制来学习一系列角色预测关联向量 a(1) RL 、 a(2) RL 、…、a(n) RL 。 （3）和（4）。然后，我们重新排序并生成谓词r1，r2，····，rn以形成逻辑规则。为了优化，我们驱动以下度量 p(r1∧r2∧···∧rn) = min(p1, p2,···, pn)，这类似于等式： (8)进行规则排序和模型训练。 5. 基准和评估。我们使用两个文档级 EAE 基准进行实验：RAMS（Ebner 等人，2020）和 WikiEvents（Li 等人，2021）。 RAMS基准定义了139种事件类型和59种语义角色，并给出了7,329个带注释的文档； WikiEvents 基准定义了 50 个事件类型和 59 个语义角色，并提供了 246 个带注释的文档。详细的数据统计如表1所示。接下来（Ebner et al., 2020; Liu et al., 2021），我们采用类型约束解码（TCD）设置进行评估，假设事件触发器及其类型已知。我们在 RAMS 上使用 Span-F1，在 WikiEvents 上使用 Head-F1 和 Coref-F1 作为评估指标，其中 Head-F1 仅检查参数中的中心词，Coref-F1 还考虑参数之间的共指链接（Du 和Cardie 等人，2020a；Li 等人，2021；Ma 等人，2022）。 实施。在我们的方法中，我们使用 BERTbase 来学习上下文单词表示（Devlin 等人，2019）。使用开发集调整超参数。最后，实体候选集K的大小设置为40，从范围[20,30,40,50]中选择，而参数候选集的大小由外部实体识别器自动确定。谓词数量 M 设置为 [10, 15, 20, 25] 选项中的 20 个。为了优化，我们使用 Adam 优化器（Kingma 和 Ba，2015），批量大小为 10（来自 [5, 10, 15, 20]），学习率为 1e-4（来自 [1e-3, 1e-4, 1e]） -5]。 基线。为了进行比较，我们考虑以下四类方法：1）传统方法，例如 BIOLabel（Shi 和 Lin，2019），它将任务视为顺序标记问题。 2）全局编码方法，例如 QAEE（Du 和 Cardie，2020b）和 DocMRC（Liu 等人，2021），它们将任务形成为基于文档的问答问题，以及 MemNet（Du 等人，2022） ），它使用内存来存储全局事件信息。 3）生成方法，例如BART-Gen（Li et al., 2021），它提出了用于参数提取的序列到序列范式，以及PAIE（Ma et al., 2022），它采用集合生成公式。 4）使用额外监督的方法，例如采用框架相关知识的FEAE（Wei et al., 2021）和利用抽象意义表示（AMR）资源的TSAR（Xu et al., 2022）。 6. 在本节中，我们将介绍关键结果，按整体性能和捕获远程依赖项的结果分开。 6.1 表 2 和表 3 分别显示了不同模型在 RAMS 和 WikiEvents 上的性能。通过采用推理链范式，我们的方法显着优于以前的方法，并实现了最先进的性能——RAMS 上的 F1 为 56.1%，WikiEvents 上的 Head-F1 和 Coref-F1 为 72.3%。值得注意的是，我们的模型不使用外部资源进行训练，但它比以前使用大量外部资源训练的模型在 RAMS 上的 F1 中优于 6%，在 WikiEvents 上的 Head-F1 中优于 4%（在 Coref-F1 中为 7%）。此外，我们发现主要的改进来自于召回率的提高，这表明学习推理逻辑规则有助于定位以前的全局推理方法难以找到的论点。 6.2 然后，我们评估不同模型处理远程依赖关系的能力，这对于文档级任务至关重要。表 4 和表 5 相应地显示了不同参数触发距离 d 的结果，我们的模型在解决远程依赖性方面取得了显着的性能，例如，当 d=-1 时，F1 的绝对改进为 10.9%、15.7% 和 6.7%， RAMS 上分别为 d=1 和 d=2。有效性背后的见解是，通过采用推理链范式，我们的方法可以利用线索实体来缩短触发器和论点之间的距离，从而促进长上下文学习。尽管如此，我们还注意到，当参数是触发器之前的两个句子时（d=-2），我们的方法产生相对较差的性能。一个可能的原因是我们的推理链总是从触发器开始，而我们没有定义反向谓词5，这可能会限制其灵活性。我们将解决这些问题以供进一步的工作。 7. 我们进行了一系列详细的研究，以进一步验证我们模型的有效性。为了方便讨论，我们使用 RAMS 基准测试作为案例。 7.1 我们进行消融研究来分析不同成分的影响。 谓词生成的影响。表6将我们的方法与采用各种谓词生成策略的方法进行了对比：1）“w/o Predicate Generation”，直接生成推理路径而不生成谓词（换句话说，它只关心两个变量之间是否存在关系） ，但没有具体关系）。 2）“w/o Role Association”，消除了角色-谓词关联学习过程，其中谓词纯粹由两个变量确定。 3）“无 CTX 重新排名”，其中省略了上下文相关的谓词重新排序过程，其中谓词完全由角色生成。结果表明，谓词生成对于推理至关重要；如果没有它，性能会显着下降（F1 中为 23.9%）。此外，角色的语义对于谓词生成至关重要；如果没有它，F1 的性能会下降 15.7%。最后，学习上下文相关谓词重新排序是有利的，导致 F1 绝对提高 3.9%。 规则长度的消融。表7检查了LOC规则中谓词计数的影响，其中N（严格）表示我们精确地采用具有N个谓词的规则，N（自适应）表示我们采用最多具有N个谓词的规则并考虑预测自适应地获得最大分数，N（Ensemble）表示我们通过对参数的最终分数求和来集成结果。结果表明，指定固定数量的谓词会导致性能较差，而提供选择不同数量的谓词的选项会带来出色的性能。这也意味着论证寻找过程确实涉及不同的推理模式。此外，我们没有注意到 N（自适应）相对于N（Ensemble），表明FOL规则可能不利于集成。 谓词数量的消融。图3考察了基于RAMS开发集的谓词数量对最终性能的影响，以及它们与规则长度的联合效应（我们使用Adaptive设置）。根据结果​​，我们的方法对谓词数量不敏感，并且当谓词数量超过 15 时始终保持高性能。此外，我们证明当规则长度增加时可以减少谓词数量（例如，从两个到三个）。这是有道理的，因为更长的规则意味着更长的推理链，而推理链已经具有高度的内在表达性。相反，对于长度为 1 的 FOL 规则，即使我们增加谓词数量以增加其多样性，性能也始终不能令人满意。7.2 鉴于我们的方法使用 FOL 规则来捕获基本推理模式，它可能比以前的推理方法更强大。我们通过分析其在低资源场景下的性能和防御对抗性攻击的性能来验证这一假设（Jia和Liang，2017）。 资源匮乏场景中的性能。图 4 比较了资源匮乏条件下的不同模型，其中显示模型仅在部分训练数据上进行训练（我们报告 5 次运行平均值以对抗随机性）。显然，我们的方法始终优于其他方法，并且值得注意的是，在极低的资源设置（少于 5% 的训练数据）下，它优于基于大型预训练语言模型和基于外部资源的 TSAR 提示的 PAIE，这表明了其有效性和学习 FOL 推理规则的普遍性。随着更多训练数据的可用，性能会提高。 防御对抗性攻击。图 5 显示了通过在测试示例中注入三种形式的噪声来防御对抗性攻击的结果。 ATK1：我们随机将句子中包含触发器的单词替换为槽符号[BLANK]； ATK2：我们将损坏的句子“答案是[空白]”放在包含触发器的句子后面。ATK3：我们在包含触发器的句子后面插入句子“[角色]的参数是[空白]”，其中 [ROLE] 被我们关注的语义角色所取代，考虑了两种设置： 攻击（随机），其中槽位填充有参数。在其他情况下发挥相同的作用。攻击（金色），其中槽填充了真实参数，但如果模型预测槽中的参数是答案，我们认为这是一个错误，因为注入的句子与上下文无关。结果表明，我们的方法在防御对抗性攻击方面表现出色，尤其是在“攻击（随机）”设置下（见图 5(a)）。原因之一是我们的方法强制预测与文档上下文中其他实体具有语义关系的参数，因此它受隔离注入参数的影响较小。使用真实参数来防御攻击更具挑战性（图 5(b)），但我们的方法仍然实现了最佳的整体性能。 7.3 表 8 通过案例研究检验了我们方法的可解释性。通过分析案例 1)、2) 和 3)，我们建议我们的方法可以为相同的语义角色生成特定的且依赖于上下文的推理规则。此外，情况 2) 和 3) 的推理模式类似，其中 r2 可以解释为 Attacker 谓词，r4 可以解释为 LocationIn 谓词。情况 4) 生成与情况 2) 和 3) 相同的谓词 r2，可以将其解释为支付事件的 Committer 谓词；它与情况 2) 和 3) 中攻击事件的攻击者共享相似的语义。案例 5) 表明我们的方法可以捕获极远的依赖关系。 8. 总之，我们提出了一种新的文档级 EAE 链推理范式，展示了捕获远程依赖项和提高可解释性的明显好处。我们的方法构建一阶逻辑规则来表示参数查询，并使用 T-Norm 模糊逻辑进行端到端学习。通过这种机制，我们的方法在两个基准测试中实现了最先进的性能，并在解决资源匮乏场景和防御对抗性攻击方面表现出良好的鲁棒性。在未来的工作中，我们寻求将我们的方法扩展到需要远程依赖关系建模的其他任务，例如文档级关系提取。 9. 我们方法的一个限制是，当存在不同长度的规则时，最终结果是由集成决定的，而不是通过构建模型来生成具有最佳长度的单个规则。第二种方式更加自然和重要，因为计算出规则的长度也是符号推理的关键部分。然而，它需要更多的参数化（例如，规则的长度可以是参数）和更高级的优化方法。上述方法的研究留待以后的工作。"}]
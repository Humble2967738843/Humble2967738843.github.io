
<!DOCTYPE html><html lang="zh-CN">

<head>
  <meta charset="utf-8">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.27.0" theme-name="Stellar" theme-version="1.27.0">
  
  <meta name="generator" content="Hexo 7.0.0">
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" content="#f8f8f8">
  
  <title>知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现 - humbleyl</title>

  
    <meta name="description" content="知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现Abstruct预训练语言模型 (PLM) 包含大量事实知识，但这些知识如何存储在参数中仍不清楚。本文深入研究了理解事实知识如何存储在多语言 PLM 中的复杂任务，并介绍了适应架构的多语言集成梯度方法，与现有方法相比，该方法成功地更精确地定位了知识神经元，并且在各种架构和语言中更加通用。此外，我们对知识神经元进行了深入的探索，得到了以下两">
<meta property="og:type" content="article">
<meta property="og:title" content="知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现">
<meta property="og:url" content="http://humble2967738843.github.io/2023/11/02/zhi-shi-shen-jing-yuan-zhong-xin-zhi-lu-yu-yan-wu-guan-zhi-shi-shen-jing-yuan-he-jian-bing-zhi-shi-shen-jing-yuan-de-fa-xian/index.html">
<meta property="og:site_name" content="humbleyl">
<meta property="og:description" content="知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现Abstruct预训练语言模型 (PLM) 包含大量事实知识，但这些知识如何存储在参数中仍不清楚。本文深入研究了理解事实知识如何存储在多语言 PLM 中的复杂任务，并介绍了适应架构的多语言集成梯度方法，与现有方法相比，该方法成功地更精确地定位了知识神经元，并且在各种架构和语言中更加通用。此外，我们对知识神经元进行了深入的探索，得到了以下两">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/35MAP8RY.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/MXWX28JU.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/WXFG2TDF.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/9PTK8AY8.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/YTVPUQLA.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/J2V52PJA.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/TJE9KG2X.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/P9A52SBQ.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/363QLBKW.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/IECZZKEA.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/SUFF78G3.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/GVDGBVQB.png">
<meta property="og:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/KX4LAIXX.png">
<meta property="article:published_time" content="2023-11-02T04:21:08.000Z">
<meta property="article:modified_time" content="2024-05-26T09:36:08.654Z">
<meta property="article:author" content="yuan long">
<meta property="article:tag" content="EMNLP2023">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://humble2967738843.github.io/2023/11/02/imgs/$%7Bfiilename%7D/35MAP8RY.png">
<meta name="twitter:creator" content="@humbleyl">
  
  
  
  <meta name="keywords" content="EMNLP2023">

  <!-- feed -->
  
    <link rel="alternate" href="/atom.xml" title="humbleyl" type="application/atom+xml">
  

  <link rel="stylesheet" href="/css/main.css?v=1.27.0">

  
    <link rel="shortcut icon" href="solar:documents-bold-duotone">
  

  

  
<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">
<!-- hexo injector head_end end --></head>
<body>

<div class="l_body s:aa content tech" id="start" layout="post" ><aside class="l_left"><div class="leftbar-container">


<header class="header"><div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="https://s2.loli.net/2024/04/10/32Y4obwgxJCeOMr.png" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></a><a class="title" href="/"><div class="main" ff="title">humbleyl</div><div class="sub normal cap">院龙的博客</div><div class="sub hover cap" style="opacity:0"> humbleyl</div></a></div></header>

<div class="nav-area">
<div class="search-wrapper" id="search-wrapper"><form class="search-form"><a class="search-button" onclick="document.getElementById(&quot;search-input&quot;).focus();"><svg t="1705074644177" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1560" width="200" height="200"><path d="M1008.839137 935.96571L792.364903 719.491476a56.783488 56.783488 0 0 0-80.152866 0 358.53545 358.53545 0 1 1 100.857314-335.166073 362.840335 362.840335 0 0 1-3.689902 170.145468 51.248635 51.248635 0 1 0 99.217358 26.444296 462.057693 462.057693 0 1 0-158.255785 242.303546l185.930047 185.725053a51.248635 51.248635 0 0 0 72.568068 0 51.248635 51.248635 0 0 0 0-72.978056z" p-id="1561"></path><path d="M616.479587 615.969233a50.428657 50.428657 0 0 0-61.498362-5.534852 174.655348 174.655348 0 0 1-177.525271 3.484907 49.403684 49.403684 0 0 0-58.833433 6.76482l-3.074918 2.869923a49.403684 49.403684 0 0 0 8.609771 78.10292 277.767601 277.767601 0 0 0 286.992355-5.739847 49.403684 49.403684 0 0 0 8.404776-76.667958z" p-id="1562"></path></svg></a><input type="text" class="search-input" id="search-input" placeholder="站内搜索"></form><div id="search-result"></div><div class="search-no-result">没有找到内容！</div></div>


<nav class="menu dis-select"><a class="nav-item active" title="博客" href="/" style="color:#1BCDFC"><svg t="1716731776385" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="17107" width="300" height="300"><path d="M1017.771 511.331c0 280.666-227.523 508.191-508.191 508.191s-508.19-227.525-508.19-508.191c0-280.665 227.523-508.19 508.191-508.19s508.19 227.523 508.19 508.19zM191.726 479.984h26.423v188.788h40.113v-188.788h23.558v-37.567h-23.558v-84.048h-40.113v84.048h-26.423v37.567zM274.182 598.096h46.958l-17.669 18.465c15.493 11.884 29.502 23.241 42.023 34.065l25.787-28.334c-10.932-8.701-21.703-16.766-32.314-24.195h92.165v16.555c0 4.882-1.221 9.022-3.662 12.417-2.443 3.395-8.332 5.093-17.669 5.093-8.703 0-21.437-0.532-38.205-1.591 4.88 16.766 8.063 29.393 9.551 37.885 31.622 0 51.998-1.036 61.126-3.104 9.126-2.068 16.023-6.475 20.692-13.212 4.669-6.74 7.004-15.734 7.004-26.981v-27.061h36.929v-31.2h-36.929v-12.417h20.375v-126.389h-81.5v-15.281h94.871v-31.2h-25.15c-7.217-9.973-14.326-19.314-21.33-28.017l-28.334 14.007 10.029 14.007h-30.085v-25.151h-37.567v25.151h-91.687v31.2h91.687v15.281h-78.636v128.3h38.205v-16.236h40.432v14.964h37.567v-14.964h43.298v9.551h-21.012v17.191h-156.951v31.2zM330.85 456.107h40.432v16.555h-40.432v-16.555zM330.85 498.13h40.432v16.555h-40.432v-16.555zM452.146 472.662h-43.298v-16.555h43.298v16.555zM408.85 498.13h43.298v16.555h-43.298v-16.555zM605.279 670.683v-15.281h139.442v15.281h43.298v-109.039c9.443 1.591 19.205 3.131 29.289 4.615l15.919-39.158c-42.449-2.547-79.008-7.323-109.675-14.326 23.241-12.838 42.819-27.697 58.738-44.571v-10.188h38.205v-75.771h-113.179c-3.715-11.247-7.059-20.692-10.029-28.334l-54.441 5.411 9.87 22.923h-123.526v75.771h43.616v-39.796h204.069v19.739h-119.068c3.82-4.14 7.427-8.382 10.825-12.735h-49.346c-22.711 23.558-52.107 43.086-88.186 58.579 8.063 8.703 16.023 18.465 23.877 29.289 15.068-7.642 29.076-15.703 42.023-24.195 9.973 9.129 20.587 17.139 31.836 24.036-30.563 7.537-68.026 14.062-112.382 19.579 5.731 11.461 11.037 23.666 15.919 36.611 10.188-1.696 20.057-3.447 29.609-5.253v106.81h43.296zM774.487 559.257h-190.062c35.339-7.534 65.687-15.81 91.052-24.832 27.061 9.445 60.063 17.725 99.010 24.832zM744.721 619.108h-139.442v-23.558h139.442v23.558zM628.359 470.433h96.305c-14.54 11.249-30.777 20.534-48.709 27.857-17.723-7.323-33.586-16.607-47.596-27.857z" fill="#272636" p-id="17108"></path></svg></a><a class="nav-item" title="文档" href="/wiki/" style="color:#3DC550"><svg t="1716731927468" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="29342" width="300" height="300"><path d="M931.607098 176.851787H459.277181l-13.308673-44.020995c-5.11872-17.147713-20.858785-28.92077-38.774307-28.92077H92.136966c-22.394401 0-40.565859 18.043489-40.565859 40.437891V879.268183c0 22.394401 18.043489 40.565859 40.437891 40.565858H931.607098c22.394401 0 40.437891-18.171457 40.565859-40.565858V217.289678c0-22.266433-18.171457-40.437891-40.565859-40.437891z" fill="#FFA000" p-id="29343"></path><path d="M867.87903 869.414646H155.865034c-22.394401 0-40.565859-18.043489-40.565859-40.43789V279.226193c0-22.394401 18.043489-40.565859 40.437891-40.565858h712.141964c22.394401 0 40.565859 18.043489 40.565859 40.43789V828.976756c0 22.394401-18.171457 40.565859-40.565859 40.43789z" fill="#FFFFFF" p-id="29344"></path><path d="M931.607098 919.834041H92.136966c-22.394401 0-40.565859-18.043489-40.565859-40.43789V355.367158c0-22.394401 18.043489-40.565859 40.437891-40.565858H931.607098c22.394401 0 40.565859 18.043489 40.565859 40.43789v524.028993c-0.127968 22.394401-18.171457 40.565859-40.565859 40.565858z" fill="#FFCA28" p-id="29345"></path><path d="M379.553112 482.311422H156.76081c-11.133217 0-20.218945-9.085729-20.218945-20.346913 0-11.133217 9.085729-20.218945 20.218945-20.218945h222.92027c11.133217 0 20.218945 9.085729 20.346913 20.218945-0.127968 11.261185-9.213697 20.346913-20.474881 20.346913 0.127968 0 0 0 0 0z m0 137.693577H156.76081c-11.133217 0-20.218945-9.085729-20.218945-20.346914 0-11.133217 9.085729-20.218945 20.218945-20.218945h222.92027c11.133217 0 20.218945 9.085729 20.346913 20.218945-0.127968 11.261185-9.213697 20.346913-20.474881 20.346914 0.127968 0 0 0 0 0z" fill="#FFFFFF" p-id="29346"></path><path d="M844.972757 768.703824l-109.540615-72.301924c-3.327168-2.303424-7.806048-2.303424-11.133217 0l-109.540615 72.429892c-4.734816 3.071232-11.005249 1.791552-14.07648-2.815296-1.151712-1.663584-1.663584-3.583104-1.663585-5.630592V313.393652c0-11.133217 8.701825-20.218945 19.32317-20.218946h222.92027c10.621345 0 19.323169 9.085729 19.323169 20.218946v446.864284c0.127968 5.502624-4.350912 10.109473-9.981505 10.23744-1.91952 0-3.967008-0.63984-5.630592-1.791552z" fill="#F44336" p-id="29347"></path></svg></a><a class="nav-item" title="在线简历" href="/explore/" style="color:#FA6400"><svg t="1716732005879" class="icon" viewBox="0 0 1160 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="31680" width="300" height="300"><path d="M1052.97035029 374.93800508L595.69272266 590.84097031c-14.55525615 8.49056572-32.74932656 8.49056572-47.30458272 0L89.89757422 374.93800508C64.42587617 361.59568701 54.72237237 331.2722375 68.06469043 305.80053945c4.85175234-9.70350381 12.12937998-16.98113232 21.83288379-21.83288466L548.38813994 68.06469043c14.55525615-8.49056572 32.74932656-8.49056572 47.30458272 0l458.49056572 215.90296436c25.47169805 13.34231807 35.17520185 43.66576846 21.83288379 69.13746649-6.06469043 9.70350381-13.34231807 16.98113232-23.04582188 21.8328838z m-817.52021542 120.0808626L547.17520185 641.78436641c14.55525615 8.49056572 32.74932656 8.49056572 47.30458272 0l311.72506699-146.76549873v298.3827498c0 92.18328838-150.40431299 167.38544443-335.98382695 167.38544444S235.45013487 886.79784395 235.45013487 793.40161748V495.01886768z" fill="#2F77F1" p-id="31681"></path><path d="M1056.60916455 424.66846396c13.34231807 0 24.25875996 10.9164419 24.25875996 24.25875997v145.55256064c0 13.34231807-10.9164419 24.25875996-24.25875996 24.25875996s-24.25875996-10.9164419-24.25875996-24.25875996v-145.55256064c0-13.34231807 10.9164419-24.25875996 24.25875996-24.25875997z" fill="#AFFCFE" p-id="31682"></path></svg></a><a class="nav-item" title="社交" href="/friends/" style="color:#F44336"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="m13.629 20.472l-.542.916c-.483.816-1.69.816-2.174 0l-.542-.916c-.42-.71-.63-1.066-.968-1.262c-.338-.197-.763-.204-1.613-.219c-1.256-.021-2.043-.098-2.703-.372a5 5 0 0 1-2.706-2.706C2 14.995 2 13.83 2 11.5v-1c0-3.273 0-4.91.737-6.112a5 5 0 0 1 1.65-1.651C5.59 2 7.228 2 10.5 2h3c3.273 0 4.91 0 6.113.737a5 5 0 0 1 1.65 1.65C22 5.59 22 7.228 22 10.5v1c0 2.33 0 3.495-.38 4.413a5 5 0 0 1-2.707 2.706c-.66.274-1.447.35-2.703.372c-.85.015-1.275.022-1.613.219c-.338.196-.548.551-.968 1.262" opacity=".5"/><path fill="currentColor" d="M10.99 14.308c-1.327-.978-3.49-2.84-3.49-4.593c0-2.677 2.475-3.677 4.5-1.609c2.025-2.068 4.5-1.068 4.5 1.609c0 1.752-2.163 3.615-3.49 4.593c-.454.335-.681.502-1.01.502c-.329 0-.556-.167-1.01-.502"/></svg></a></nav>
</div>
<div class="widgets">
<widget class="widget-wrapper post-list"><div class="widget-header dis-select"><span class="name">专栏：自然语言处理论文阅读</span></div><div class="widget-body"><a class="item" href="/2024/05/06/ji-yu-lian-shi-tui-li-de-wen-dang-ji-shi-jian-lun-yuan-ti-qu/"><span class="title">基于链式推理的文档级事件论元提取</span></a><a class="item" href="/2023/11/06/wang-diao-ni-xiang-wang-diao-de-dong-xi-llms-de-gao-xiao-wang-que/"><span class="title">忘掉你想忘掉的东西：LLMs的高效忘却</span></a><a class="item" href="/2023/11/02/qing-jing-xue-xi-chuang-jian-ren-wu-xiang-liang/"><span class="title">情境学习创建任务向量</span></a><a class="item" href="/2023/11/02/bian-ji-da-xing-yu-yan-mo-xing-wen-ti-fang-fa-he-ji-yu/"><span class="title">编辑大型语言模型：问题、方法和机遇</span></a><a class="item active" href="/2023/11/02/zhi-shi-shen-jing-yuan-zhong-xin-zhi-lu-yu-yan-wu-guan-zhi-shi-shen-jing-yuan-he-jian-bing-zhi-shi-shen-jing-yuan-de-fa-xian/"><span class="title">知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现</span><svg class="active-icon" xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="currentColor" d="M21 11.098v4.993c0 3.096 0 4.645-.734 5.321c-.35.323-.792.526-1.263.58c-.987.113-2.14-.907-4.445-2.946c-1.02-.901-1.529-1.352-2.118-1.47a2.225 2.225 0 0 0-.88 0c-.59.118-1.099.569-2.118 1.47c-2.305 2.039-3.458 3.059-4.445 2.945a2.238 2.238 0 0 1-1.263-.579C3 20.736 3 19.188 3 16.091v-4.994C3 6.81 3 4.666 4.318 3.333C5.636 2 7.758 2 12 2c4.243 0 6.364 0 7.682 1.332C21 4.665 21 6.81 21 11.098" opacity=".5"/><path fill="currentColor" d="M9 5.25a.75.75 0 0 0 0 1.5h6a.75.75 0 0 0 0-1.5z"/></svg></a><a class="item" href="/2023/11/02/wo-men-ke-yi-tong-guo-qing-jing-xue-xi-lai-bian-ji-shi-shi-zhi-shi-ma/"><span class="title">我们可以通过情景学习来编辑事实知识吗？</span></a><a class="item" href="/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/"><span class="title">我们可以编辑多模态大型语言模型吗？</span></a><a class="item" href="/2023/11/02/duo-yu-yan-mo-xing-zhong-shi-shi-zhi-shi-de-kua-yu-yan-yi-zhi-xing/"><span class="title">多语言模型中事实知识的跨语言一致性</span></a></div></widget>

<widget class="widget-wrapper post-list"><div class="widget-header dis-select"><span class="name">最近更新</span></div><div class="widget-body fs14"><a class="item title" href="/2024/11/23/csac/"><span class="title">csac</span></a><a class="item title" href="/2024/04/10/tui-jian-xi-tong/"><span class="title">推荐系统</span></a><a class="item title" href="/2023/11/02/duo-yu-yan-mo-xing-zhong-shi-shi-zhi-shi-de-kua-yu-yan-yi-zhi-xing/"><span class="title">多语言模型中事实知识的跨语言一致性</span></a><a class="item title" href="/2024/05/06/ji-yu-lian-shi-tui-li-de-wen-dang-ji-shi-jian-lun-yuan-ti-qu/"><span class="title">基于链式推理的文档级事件论元提取</span></a><a class="item title" href="/2023/11/02/zhi-shi-shen-jing-yuan-zhong-xin-zhi-lu-yu-yan-wu-guan-zhi-shi-shen-jing-yuan-he-jian-bing-zhi-shi-shen-jing-yuan-de-fa-xian/"><span class="title">知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现</span></a><a class="item title" href="/2023/11/02/wo-men-ke-yi-tong-guo-qing-jing-xue-xi-lai-bian-ji-shi-shi-zhi-shi-ma/"><span class="title">我们可以通过情景学习来编辑事实知识吗？</span></a><a class="item title" href="/2023/11/02/wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma/"><span class="title">我们可以编辑多模态大型语言模型吗？</span></a><a class="item title" href="/2023/11/06/wang-diao-ni-xiang-wang-diao-de-dong-xi-llms-de-gao-xiao-wang-que/"><span class="title">忘掉你想忘掉的东西：LLMs的高效忘却</span></a><a class="item title" href="/2023/11/02/qing-jing-xue-xi-chuang-jian-ren-wu-xiang-liang/"><span class="title">情境学习创建任务向量</span></a><a class="item title" href="/2023/11/06/ji-yu-ti-shi-de-ling-yang-ben-guan-xi-chou-qu-fang-fa-tan-suo/"><span class="title">基于提示的零样本关系抽取方法探索</span></a></div></widget>
</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://github.com/Humble2967738843" target="_blank" rel="external nofollow noopener noreferrer"><svg t="1716701113980" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="4745" width="200" height="200"><path d="M512 512m-469.333333 0a469.333333 469.333333 0 1 0 938.666666 0 469.333333 469.333333 0 1 0-938.666666 0Z" fill="#434A54" p-id="4746"></path><path d="M610.688 808.149333c0-12.074667 0.426667-51.498667 0.426667-100.437333 0-34.133333-11.733333-56.448-24.832-67.84 81.493333-9.045333 167.125333-39.978667 167.125333-180.608 0-39.936-14.208-72.618667-37.674667-98.261333 3.84-9.216 16.341333-46.421333-3.584-96.853334 0 0-30.72-9.813333-100.565333 37.546667a351.658667 351.658667 0 0 0-91.733333-12.373333 350.549333 350.549333 0 0 0-91.605334 12.373333c-69.973333-47.36-100.693333-37.546667-100.693333-37.546667-19.882667 50.432-7.338667 87.637333-3.541333 96.853334a141.653333 141.653333 0 0 0-37.717334 98.261333c0 140.288 85.461333 171.690667 166.784 180.906667-10.453333 9.173333-19.968 25.301333-23.253333 48.981333-20.906667 9.386667-73.856 25.514667-106.496-30.421333 0 0-19.370667-35.157333-56.149333-37.76 0 0-35.712-0.426667-2.474667 22.272 0 0 23.978667 11.264 40.618667 53.546666 0 0 21.504 65.365333 123.349333 43.221334 0.170667 30.592 0.512 59.392 0.512 68.138666a19.968 19.968 0 0 1-2.218667 9.173334 339.925333 339.925333 0 0 0 187.904 3.114666 19.2 19.2 0 0 1-4.181333-12.288z" fill="#FFFFFF" p-id="4747"></path><path d="M180.138667 843.861333A467.882667 467.882667 0 0 0 512 981.333333c259.2 0 469.333333-210.133333 469.333333-469.333333 0-129.621333-52.522667-246.954667-137.472-331.861333L180.138667 843.861333z" fill="#231F20" opacity=".1" p-id="4748"></path></svg></a><a class="social" href="https://" target="_blank" rel="external nofollow noopener noreferrer"><svg t="1716701237965" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="12408" width="200" height="200"><path d="M276.822886 874.188342c-47.901937-0.484303-85.721573-1.942714-96.222136 64.483801-4.204628 59.150968 41.545465 67.092431 120.596869 81.461911 86.981861 4.49631 137.949214 16.482801 205.118694-49.93821l-227.814878-96.106564" fill="" p-id="12409"></path><path d="M270.939709 875.64125c-24.792995 0-64.296684-0.968605-81.098684 43.647779 2.102314 5.332833 12.327704 2.982864 18.794246-1.562977 9.691557-4.589869 5.162226 1.799625-5.552971 10.412507-11.661788 8.12858-37.660036 34.54509 6.461038 61.633019 46.801249 21.645028 193.974228 61.280799 285.678041-16.169105 0 0-180.892553-56.256158-222.493052-97.944713h-1.788618v-0.01651z" fill="#F0971C" p-id="12410"></path><path d="M764.482659 865.94419c47.896434-0.484303 85.710567-1.948218 96.222136 64.4838 4.204628 59.150968-64.731455 68.859035-120.602373 81.461912-59.734332 5.51995-137.949214 16.488305-205.118693-49.93821l227.820381-96.106564" fill="" p-id="12411"></path><path d="M770.360332 867.402601c24.787491 0 64.29118-0.974109 81.087678 43.642275-2.09681 5.332833-12.316698 2.982864-18.777736-1.562976-9.708067-4.589869-5.16773 1.799625 5.547467 10.412507 11.661788 8.123077 37.66554 34.54509-6.461038 61.633019-46.801249 21.645028-193.974228 61.275295-285.683544-16.174609 0 0 180.903559-56.250655 222.504059-97.939209l1.783114-0.011007z" fill="#F0971C" p-id="12412"></path><path d="M825.598354 441.216247c51.259035 54.307941 83.206502 107.641773 108.412255 245.337829 3.417636 83.393619-7.567229 91.153468-31.947466 113.453405-17.638524-2.905816-36.124577-30.059786-54.616134-94.070291-18.491557-63.993994-21.848655-264.720943-21.848655-264.720943z" fill="" p-id="12413"></path><path d="M847.452513 380.128069c0-15.21701-4.716448-29.894683-13.466917-43.741337C832.994977 150.420008 697.560835 0 530.619501 0 363.683671 0 228.249529 150.420008 227.25891 336.386732c-8.744965 13.852157-13.466916 28.52983-13.466917 43.735833 0 22.228392 10.043777 43.334083 28.078549 62.342963 39.349592 136.000997 153.744086 234.385984 288.748959 234.385984 135.015881 0 249.404871-98.384988 288.759967-234.385984 18.034771-19.003377 28.073045-40.109067 28.073045-62.337459zM198.035646 442.184853C146.771107 496.48729 114.834648 549.826626 89.623392 687.517178c-3.423139 83.393619 25.899186 111.031892 31.936459 113.458909 6.042777 2.410506 36.135584-30.06529 54.62714-94.064788 18.486053-63.988491 21.848655-264.726446 21.848655-264.726446z" fill="" p-id="12414"></path><path d="M864.254513 621.585973c0 200.82601-158.394492 363.634261-353.810621 363.634261-195.394116 0-353.805118-162.80825-353.805118-363.634261 0-200.831514 158.405499-363.639764 353.805118-363.639764 195.410626 0 353.810621 162.802747 353.810621 363.639764z" fill="" p-id="12415"></path><path d="M220.50619 696.245633c0 149.952216 134.702185 271.517691 300.86203 271.517691 166.154342 0 300.86203-121.565475 300.86203-271.517691s-134.707688-271.517691-300.86203-271.51769c-166.154342 0.005503-300.86203 121.565475-300.86203 271.51769z" fill="#FFFFFF" p-id="12416"></path><path d="M232.068916 322.672161s-23.950969 19.636272-18.909818 60.361724c-25.211256 30.544089-20.797498 62.54659-9.454909 95.281048 18.909818 62.788741 146.650152 162.665161 309.684043 162.665161 223.544209-15.018886 289.090173-123.392617 338.890797-180.122071 0 0 29.003126-47.280049-18.909818-129.457407l-72.491305 7.275547-528.80899-16.004002z" fill="" p-id="12417"></path><path d="M302.980734 572.85852c-3.147967 33.086678-15.128955 139.996494 3.786366 166.545086 20.170106 26.190869 44.43477 38.909317 96.112067 30.907316 23.323577-7.638774 28.364727-29.817635 26.163352-61.809129l-0.946592-92.364225-125.115193-43.279048z" fill="" p-id="12418"></path><path d="M223.246902 372.126068s83.200998 144.002998 271.033388 151.273041c83.206502 1.458411 167.662284 8.728455 292.458278-87.26804 24.583864-25.453408 29.003126-33.455409 29.003126-33.455409s8.189118 65.457909-112.820509 147.635268c0 0 82.562599 0.731957 141.812629-170.182859 0 0 29.003126 111.274043-72.480298 164.36022-93.288803 56.008504-154.432015 93.090679-309.480417 84.362224-101.874168-25.326829-254.005746-65.457909-264.726446-192.730451 0 0 6.306942-29.817635 14.49606-24.000499 0 0 0.627392 69.095683 52.943088 92.364225-0.011007 0.005503-52.326703-77.807627-42.238899-132.35772z" fill="#E71F19" p-id="12419"></path><path d="M321.059532 589.605486s-36.240149 138.185862 4.413759 161.459908c41.2813 19.273045 66.498059 14.182364 94.229891 4.727454 11.661788-16.004002 1.260288-111.63727 4.721951-121.455406-6.306942-4.006504-24.892057-8.370731-28.051031-8.370731 0 0-2.834271 40.719949-0.313696 65.463413-5.046654-19.273045-8.827517-19.273045-8.827517-71.275045-30.235897-10.550093-66.173356-30.549593-66.173357-30.549593z" fill="#E71F19" p-id="12420"></path><path d="M506.316313 242.426509c0 50.60963-22.487054 91.637771-50.218886 91.637771s-50.218885-41.028141-50.218885-91.637771 22.487054-91.637771 50.218885-91.63777c27.731832-0.005503 50.218885 41.022638 50.218886 91.63777z m147.492178 0c0 50.60963-22.498061 91.637771-50.218885 91.637771-27.731832 0-50.218885-41.028141-50.218886-91.637771s22.487054-91.637771 50.218886-91.63777c27.726328-0.005503 50.218885 41.022638 50.218885 91.63777z" fill="#FFFFFF" p-id="12421"></path><path d="M499.530572 243.395115c0 19.553721-8.469793 35.398123-18.915322 35.398123-10.440025 0-18.915322-15.844402-18.915321-35.398123 0-19.548217 8.469793-35.398123 18.915321-35.398123 10.445528 0.005503 18.915322 15.855409 18.915322 35.398123z m74.791742 10.54459c-1.794121-18.238398 8.662414-44.368729 29.151719-37.638023 14.86479 6.730706 14.028267 32.547341 13.703565 39.096434-0.324703 6.538086-8.97611 11.089431-12.922076 0-1.893183-10.36848-10.093308-41.63352-17.176235-2.371982-2.360976 9.091682-11.969981 8.915572-12.756973 0.913571z" fill="" p-id="12422"></path><path d="M676.779847 358.472035c-37.032644-15.618761-89.452905-25.381863-147.624261-25.381863-56.872544 0-108.236144 9.32833-145.109189 24.347216-30.57711 12.44878-57.648529 34.567103-56.597371 47.02689 3.153471 12.503815 13.13671 20.626891 68.176608 29.118699-19.53721-18.915322-15.21701-14.600625-15.21701-43.5212 0 28.920575 23.48868 43.669792 61.357847 59.42614 24.853533 10.340963 55.386615 16.455284 88.418259 16.455284 34.677172 0 66.602625-6.73621 92.072542-18.023764 35.805377-15.855409 53.108191-28.661913 53.108191-56.6414 0 27.979486 5.773108 19.030894-17.242276 42.310443 53.366853-9.383364 70.17986-19.757348 71.5227-32.525327 0.022014-11.238023-25.55247-31.072419-52.86604-42.591118z" fill="#F0971C" p-id="12423"></path></svg></a><a class="social" href="https://www.kaggle.com/humbleyll" target="_blank" rel="external nofollow noopener noreferrer"><svg t="1716701307894" class="icon" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="13418" width="200" height="200"><path d="M803.1909 1017.952189c-0.927971 3.935877-4.991844 6.015812-11.999625 6.015812H657.27546c-7.967751 0-14.975532-3.487891-20.991344-10.591669l-220.921096-281.111215-61.790069 58.622168v218.073185c0 10.015687-4.991844 15.007531-14.975532 15.007531H234.88866c-10.079685 0-15.103528-4.991844-15.103528-15.007531V15.071529C219.785132 5.11984 224.808975 0 234.88866 0h103.708759c9.983688 0 14.975532 5.11984 14.975532 15.071529v611.948877l264.663729-267.607638c7.03978-7.03978 14.07956-10.495672 21.11934-10.495672h138.203681c6.143808 0 10.079685 2.55992 12.15962 7.67976 1.951939 6.367801 1.439955 10.87966-1.535952 13.43958l-279.67126 270.679542 291.670885 362.964657c4.063873 4.447861 4.991844 8.863723 2.975907 15.263523z" fill="#20BEFF" p-id="13419"></path></svg></a><a class="social" href="https://" target="_blank" rel="external nofollow noopener noreferrer"><svg t="1716701358358" class="icon" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="14411" width="200" height="200"><path d="M1024.16 694.816c0-149.92-143.104-271.392-319.584-271.392-176.576 0-319.68 121.504-319.68 271.392S528 966.208 704.576 966.208c55.456 0 107.648-12.096 153.184-33.248l125.984 54.528-14.592-140.544c34.784-43.392 55.04-95.808 55.04-152.128zM596.832 621.28c-25.152 0-45.472-20.352-45.472-45.472s20.32-45.472 45.472-45.472c25.12 0 45.44 20.384 45.44 45.472s-20.384 45.472-45.44 45.472z m215.392 0c-25.056 0-45.44-20.352-45.44-45.472s20.384-45.472 45.44-45.472c25.184 0 45.536 20.384 45.536 45.472s-20.352 45.472-45.536 45.472zM704.576 387.488c49.376 0 96.416 8.8 139.264 24.64 0.32-5.728 0.992-11.232 0.992-16.992 0-198.08-189.152-358.624-422.432-358.624C189.184 36.512 0.032 197.024 0.032 395.136c0 74.496 26.816 143.776 72.704 201.12L53.472 781.92l166.432-72.096c41.216 19.2 86.784 32.16 134.88 38.784-3.616-17.504-5.824-35.424-5.824-53.792 0.032-169.44 159.552-307.296 355.616-307.296z m-139.808-209.6c33.184 0 60 26.88 60 60 0 33.184-26.816 60.064-60 60.064s-60.032-26.88-60.032-60.064c0-33.152 26.88-60 60.032-60zM280.032 297.952c-33.184 0-60-26.88-60-60.064 0-33.152 26.848-60 60-60 33.184 0 60.032 26.88 60.032 60s-26.88 60.064-60.032 60.064z" fill="#51C332" p-id="14412"></path></svg></a></div></footer>
</div></aside><div class="l_main" id="main">





<div class="article banner top">
  <div class="content">
    <div class="top bread-nav footnote"><div class="left"><div class="flex-row" id="breadcrumb"><a class="cap breadcrumb" href="/">主页</a>
<span class="sep"></span><a class="cap breadcrumb" id="menu" href="/topic">专栏</a><span class="sep"></span><a class="cap breadcrumb" id="proj" href="/2024/05/06/ji-yu-lian-shi-tui-li-de-wen-dang-ji-shi-jian-lun-yuan-ti-qu/">自然语言处理论文阅读</a></div>
<div class="flex-row" id="post-meta"><span class="text created">发布于：<time datetime="2023-11-02T04:21:08.000Z">2023-11-02</time></span><span class="sep updated"></span><span class="text updated">更新于：<time datetime="2024-05-26T09:36:08.654Z">2024-05-26</time></span></div></div></div>
    
    <div class="bottom only-title">
      
      <div class="text-area">
        <h1 class="text title"><span>知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现</span></h1>
        
      </div>
    </div>
    
  </div>
  </div><article class="md-text content"><h1 id="知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现"><a href="#知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现" class="headerlink" title="知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现"></a>知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现</h1><h2 id="Abstruct"><a href="#Abstruct" class="headerlink" title="Abstruct"></a>Abstruct</h2><p>预训练语言模型 (PLM) 包含大量事实知识，但这些知识如何存储在参数中仍不清楚。本文深入研究了理解事实知识如何存储在多语言 PLM 中的复杂任务，并介绍了适应架构的多语言集成梯度方法，与现有方法相比，该方法<span style="background-color: #ff666680">成功地更精确地定位了知识神经元，并且在各种架构和语言中更加通用。</span>此外，我们对知识神经元进行了深入的探索，得到了以下两个重要发现：<span style="background-color: #ff666680">（1）语言无关的知识神经元的发现，它以超越语言的形式存储事实知识。我们设计了跨语言知识编辑实验，证明 PLM 可以基于语言无关的神经元完成这项任务；</span> <span style="background-color: #ff666680">（2）退化知识神经元的发现，这是一种新型神经元，表明不同的知识神经元可以存储相同的事实。</span>其<span style="background-color: #ff666680">功能重叠的特性赋予 PLM 强大的事实知识掌握能力。</span>我们设计了事实检查实验，证明<span style="background-color: #ff666680">退化知识神经元可以帮助 PLM 检测错误事实。</span>实验证实了这些发现，揭示了多语言 PLM 中事实知识存储的机制，并为该领域提供了宝贵的见解。源代码将公开以供进一步研究。</p>
<h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><p>预训练语言模型 (PLM)（Devlin 等人，2018 年；Radford 等人，2019 年；Shliazhko 等人，2022 年；OpenAI 2023；Touvron 等人，2023 年）因其卓越的性能而彻底改变了自然语言处理领域涵盖广泛的任务。这些模型在维基百科等广泛的语料库上进行训练，被广泛认为封装了大量事实知识（Petroni 等人，2019b；Jiang 等人，2020），但知识如何存储在参数中仍不清楚（Kandpal 等人） .2023）。研究知识存储机制将有助于更深入地理解和掌握 PLM 中的知识（Zhen 等人，2022 年；Zhao 等人，2023 年）。在本文中，我们对知识定位任务（Hase et al. 2023; Andreas 2022）进行了深入研究，该任务旨在确定模型参数中特定事实知识的存储位置，其中此类参数被称为知识神经元（Dai 等人，2022）。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/35MAP8RY.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;35MAP8RY&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22KFEQGR8B%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%221%22%2C%22position%22%3A%7B%22pageIndex%22%3A0%2C%22rects%22%3A%5B%5B310.588%2C334.059%2C565.588%2C579.353%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%221%22%7D%7D&quot; width=&quot;425&quot; height=&quot;409&quot; src=&quot;attachments/35MAP8RY.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>最近，一些已建立的方法致力于阐明 PLM 中的知识存储机制。一种策略是基于梯度的方法（Ancona et al. 2019），<span style="background-color: #2ea8e580">它通过使用积分梯度计算每个神经元的归因得分来评估每个神经元的贡献。另一种是因果启发方法，它采用跟踪算法来跟踪模型层之间的因果影响（Cao 等人，2023）。</span>尽管在知识本地化任务中取得了成功，这些方法仍然面临两个主要挑战：<span style="background-color: #2ea8e580">（1）缺乏针对不同 PLM 架构的通用方法：观察到事实知识出现在各种 PLM 架构中，包括自动编码模型（例如， BERT）（Devlin 等人，2018 年）和自回归模型（例如 GPT）（Shliazhko 等人，2022 年）。</span>然而，虽然有些方法适合自编码模型并且在自回归模型中表现不佳（Meng et al. 2022a），但其他方法是专门为自回归模型设计的并且不能很好地适应自编码模型（Li et al. 2022），在跨两种 PLM 架构都表现良好的通用方法中留下了空白。(2)缺乏多层次探索多种语言：实质性知识与语言无关，当前的大型语言模型支持多语言。然而，现有的方法仅关注英语数据集，可能无法提供跨语言知识存储机制的全面见解，限制了得出多语言结论的能力。</p>
<pre><code> 为了更精确地定位知识神经元，我们遵循基于梯度的方法，提出了一种新颖的知识定位方法，称为架构适应多语言集成梯度（AMIG）。&lt;span style=&quot;background-color: #ff666680&quot;&gt;首先，针对不同 PLM 架构中缺乏通用方法，我们设计了一种架构适配技术，使得集成梯度算法（Lundstrom、Huang 和 Razaviyayn 2022）中的基线向量在不同 PLM 架构之间普遍兼容。其次，针对多语言探索的缺乏，我们引入了多语言阈值调整技术，针对不同语言调整综合梯度计算中的阈值。&lt;/span&gt;多语言数据集上的实验结果表明，与之前最先进的模型相比，我们的方法可以更精确地定位知识神经元。此外，我们还对知识神经元进行了深入的探索，得出了以下两个重要发现。
</code></pre><p><strong>      与语言无关的知识神经元</strong>：我们在多语言 PLM 中发现了一种新型神经元，能够跨语言存储事实知识。我们将它们命名为与语言无关的知识神经元，因为它们的存在超越了特定语言的界限。如图1a所示，<span style="background-color: #ff666680">这些神经元是通过将源自不同语言的知识神经元相交而获得的，封装了跨多种语言一致的知识表示。独立于语言的知识神经元可以帮助跨语言的知识编辑任务：对某些知识的单次编辑可以同时影响所有语言的相应知识。</span>例如，如果我们将事实⟨Tanzania, Capital, Dar es Salaam⟩对应的语言无关神经元编辑为⟨Tanzania, Capital, Dodoma⟩，则该事实在所有语言中都会相应更改。我们设计实验来验证与语言无关的知识神经元的作用。与现有的跨语言知识编辑模型相比，我们的方法的编辑性能更为优越。该实验证明了我们的方法在跨语言知识编辑应用中的潜力。</p>
<pre><code>  **退化知识神经元**：我们发现了一个有趣的现象，对应于一种全新类型的神经元。&lt;span style=&quot;background-color: #ff666680&quot;&gt;给定事实及其相应的知识神经元，知识神经元的某些子集表现出独特的属性。即使该子集中的某些元素被抑制，模型仍然可以正确地表达事实；然而，如果子集中的所有元素都被抑制，模型就无法再正确地表达事实。这一现象表明，一些知识神经元存储着相同的事实知识，模型需要激活至少一个神经元才能正确表达事实。&lt;/span&gt;它与生物系统中的“简并”现象非常相似（Tononi, Sporns, and Edelman 1999; Mason 2015），因此我们将此类神经元命名为简并知识神经元。与冗余不同，&lt;span style=&quot;background-color: #ff666680&quot;&gt;简并知识神经元不能简单地删除，因为它们仅部分重叠。一个退化的知识神经元可能存储多条事实知识，删除它对特定知识没有影响，但可能会影响其他知识。&lt;/span&gt;

 图1b说明了简并知识神经元的获取过程。具体来说，&lt;span style=&quot;background-color: #ff666680&quot;&gt;我们首先对知识神经元进行定位，然后对它们进行聚合和过滤以获得简并的知识神经元。&lt;/span&gt;对于查询“坦桑尼亚的首都是”，PLM 必须激活至少一个相应的简并知识神经元来预测正确的事实 Dodoma。直观上，&lt;span style=&quot;background-color: #ff666680&quot;&gt;简并知识神经元的功能重叠特性赋予 PLM 对事实知识的强大理解，确保其对事实的掌握保持稳定且不易出错&lt;/span&gt;。受此启发，我们设计了一个实验，使用简并知识神经元进行事实检查。我们的实验表明，&lt;span style=&quot;background-color: #5fb23680&quot;&gt;简并知识神经元可以帮助 PLM 检测错误事实，从而说明它们的存在增强了 PLM 对事实知识的稳定掌握。&lt;/span&gt;
</code></pre><p>总的来说，主要贡献总结如下：（1）我们提出了一种新颖的知识本地化方法，称为架构适应的多语言集成梯度，它可以有效解决传统方法的两个挑战：缺乏针对不同 PLM 架构的通用方法和缺乏对多种语言的探索，从而实现知识神经元更精确的定位。 （2）我们发现了独立于语言的知识神经元，它们以超越语言障碍的形式存储事实知识。实验结果表明它们有利于跨语言知识编辑任务。 （3）我们发现了简并知识神经元，这是一种具有功能重叠特性的新型神经元，使得模型对事实知识的掌握更加稳健。实验证明它们可以帮助检测不正确的事实。</p>
<h2 id="2-Methodology"><a href="#2-Methodology" class="headerlink" title="2 Methodology"></a>2 Methodology</h2><p>图 2 示意性地展示了我们提出的框架。它由三个主要模块组成，包括知识神经元定位（模块1）、语言无关知识神经元检测（模块2）和简并知识神经元检测（模块3）。我们详细说明了每个模块。</p>
<h3 id="2-1KnowLedge-Neuron-Localization"><a href="#2-1KnowLedge-Neuron-Localization" class="headerlink" title="2.1KnowLedge Neuron Localization"></a>2.1KnowLedge Neuron Localization</h3><p>图 2 的模块 1 展示了知识定位模块，该模块旨在查明 PLM 中知识神经元的确切位置。使用填空完形填空任务（Petroni 等人，2019a），我们评估对 PLM 对特定事实的理解。例如，给定一个事实 ⟨Tanzania, Capital, Dodoma⟩ 以及相应的查询“坦桑尼亚的首都是”，Petroni 等人 (2019a) 描述，如果模型能够预测正确答案，则模型知道一个事实。在本研究中，我们通过引入架构适应多语言集成梯度方法来扩展此分析，以定位专门负责处理事实信息的神经元。</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/MXWX28JU.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;MXWX28JU&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%228TICA8HS%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B49.615%2C585.462%2C563.654%2C764.885%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;857&quot; height=&quot;299&quot; src=&quot;attachments/MXWX28JU.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>从数学上来说，给定一个查询 q，PLM 预测的正确答案的概率可以定义为：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/WXFG2TDF.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;WXFG2TDF&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22HCSPGJUY%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B55.385%2C522%2C295.962%2C544.5%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;401&quot; height=&quot;38&quot; src=&quot;attachments/WXFG2TDF.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 y* 是正确答案，w(l) j 是第 l 层的第 j 个神经元，^ w(l) j 是 w(l) j 分配的值。为了计算每个神经元的归因分数，我们使用积分梯度（Sundararajan、Taly 和 Yan 2017）。考虑一个神经元 w(l) j ，我们可以计算它的归因分数：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/9PTK8AY8.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;9PTK8AY8&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22WI26DVZD%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B50.192%2C407.769%2C302.308%2C456.808%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;420&quot; height=&quot;82&quot; src=&quot;attachments/9PTK8AY8.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 w(l) j 是 w(l) j 的值，w′(l) j 是 w(l) j 的基线向量，并且 ∂ F(w′(l) j +α(w(l)) j −w′(l) j )) ∂ w(l) j 计算梯度。当 α 从 0 变为 1 时，(w′(l) j +α(w(l) j −w′(l) j )) 从 w′(l) j 变为 w(l) j ，因此 Attr (w(l) j )可以通过对梯度进行积分来累积因w(l) j 变化而引起的概率变化。理想的基线向量 w′(l) j 应该缺乏信息（Liu et al. 2022)，当前的方法用零向量对其进行近似。然而，这样的设置没有考虑各种 PLM 架构之间的差异，导致性能不佳。为了缓解这个问题，我们设计了一种架构适应技术来计算各种 PLM 架构的基线向量。</p>
<pre><code> 首先，为了最小化基线向量中的信息内容，我们遵循Enguehard（2023）的方法，将输入查询q分成m个单词，然后将每个单词分别输入到PLM中以计算神经元的激活分数对应每个词qi。随后，我们精心设计了不同 PLM 架构的基线向量。设qi对应的基线句子为q′ i，q′ i包含m个单词，长度与q一致，记为q′ i = (q′ i1 . . . q′ ik . . . q′ im) ， 在哪里：
</code></pre><p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/YTVPUQLA.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;YTVPUQLA&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22BS72ACK6%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B53.654%2C87.577%2C296.538%2C128.538%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;405&quot; height=&quot;68&quot; src=&quot;attachments/YTVPUQLA.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 ⟨mask⟩ 用于屏蔽自动编码模型，⟨eos⟩ 代表自回归模型中的“序列结束”，qk 是查询的第 k 个单词。在此设计中，第 l 层中的第 i 个神经元（用 w(l) j 表示）对应于 qi，其相关基线向量 w’(l) j 对应于 q’ i。然后，我们可以根据方程（2）计算使用 qi 作为输入时每个神经元的归因得分 Attri(w(l) j )。为了计算积分，我们使用黎曼近似：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/J2V52PJA.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;J2V52PJA&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22SMECCLTE%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B317.885%2C419.308%2C562.5%2C460.846%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;408&quot; height=&quot;69&quot; src=&quot;attachments/J2V52PJA.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 N 是近似步数。然后对每个单词 qi 的归因进行求和并标准化，得出查询的最终归因分数：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/TJE9KG2X.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;TJE9KG2X&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22RUKKVDNU%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%223%22%2C%22position%22%3A%7B%22pageIndex%22%3A2%2C%22rects%22%3A%5B%5B320.192%2C349.5%2C562.5%2C388.731%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%223%22%7D%7D&quot; width=&quot;404&quot; height=&quot;65&quot; src=&quot;attachments/TJE9KG2X.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 n 是第 l 层中的神经元数量。最后，<strong>我们可以找到归因分数大于阈值τ的神经元，并将其视为知识神经元</strong>，记为N。</p>
<h3 id="2-2-Language-Indepent-Knowledge-Neuron-Dectection"><a href="#2-2-Language-Indepent-Knowledge-Neuron-Dectection" class="headerlink" title="2.2 Language-Indepent Knowledge Neuron Dectection"></a>2.2 Language-Indepent Knowledge Neuron Dectection</h3><p><strong>解释</strong> 许多 PLM 支持多语言，并且这些模型中的事实知识的很大一部分是与语言无关的（Xu 等人，2023 年；Wang、Lipton 和 Tsvetkov，2020 年）。这种必要性对于探索多语言 PLM 中事实知识的存储机制变得越来越重要。我们将存储多种语言共有的事实知识的神经元定义为与语言无关的知识神经元，记为 L。为了识别这些类型的知识神经元，我们设计了一种检测算法，如下所示。</p>
<p><strong>算法</strong> 如图 2 的模块 2 所示，<span style="background-color: #ff666680">给定 K 种语言中具有相同语义的事实三元组，让相应的查询用 qk 表示，其中 k = 1, 2, …。 。 。 ，K。对于每个查询，我们使用知识神经元定位模块来获取相应的知识神经元，其中神经元 w(l) i 的属性得分记为攻击 (w(l) i )。多语言PLM对不同语言的敏感度不同，导致不同语言查询的归因分数存在显着差异。</span>因此，很难通过设置统一的阈值来获得所有语言的知识神经元。为了解决这个问题，我们设计了一种多语言阈值调整技术。<span style="background-color: #ff666680">我们为不同的语言设置不同的缩放因子τk，并记录查询qk对应的神经元的最大归因得分，然后确定动态阈值：</span></p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/P9A52SBQ.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;P9A52SBQ&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22CMF6JKKR%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B51.923%2C615.462%2C295.962%2C635.077%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;407&quot; height=&quot;33&quot; src=&quot;attachments/P9A52SBQ.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>然后，我们使用阈值过滤来识别第 k 种语言的知识神经元 Nk ，如下所示：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/363QLBKW.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;363QLBKW&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22UBY826H3%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B54.231%2C568.731%2C295.962%2C590.077%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;403&quot; height=&quot;36&quot; src=&quot;attachments/363QLBKW.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>最后，我们计算所有语言的知识神经元的交集：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/IECZZKEA.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;IECZZKEA&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22KA6FW74N%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B55.962%2C509.308%2C294.808%2C545.077%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;398&quot; height=&quot;60&quot; src=&quot;attachments/IECZZKEA.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 L 代表独立于语言的知识神经元，编码在所有考虑的语言中一致的事实知识。通过上述算法，我们最终可以得到它们。</p>
<h3 id="2-3Degenerate-Knowledge-Neuron-Detection"><a href="#2-3Degenerate-Knowledge-Neuron-Detection" class="headerlink" title="2.3Degenerate Knowledge Neuron Detection"></a>2.3Degenerate Knowledge Neuron Detection</h3><p><strong>解释 </strong>通过进行深入分析，我们发现了一个有趣的现象：<span style="background-color: #ff666680">不同的神经元组负责存储相同的事实知识。例如，对于表示为 ⟨h, r, t⟩ 的特定事实，假设我们定位 10 个标记为 N = {1, 2, … 的知识神经元。 。 。 ，10}。如果我们抑制集合 A = {1, 2} 或 B = {3, 4, 5}（N 的两个子集）的神经元，我们观察到预测概率没有显着下降。相反，同时抑制这两组神经元（即 A∪B）会导致预测概率的大幅损失。这表明 A 组和 B 组都包含相同的事实知识，至少其中一个必须是活跃的，模型才能准确理解事实。</span>此外，<span style="background-color: #5fb23680">这两组神经元并不相互冗余。也就是说，除了事实⟨h，r，t⟩之外，A还可以存储事实⟨h1，r1，t1⟩，而B可以存储⟨h2，r2，t2⟩，从而在PLM中发挥附加作用。</span>鉴于这种行为与生物神经网络中的退化现象相似（Tononi、Sporns 和 Edelman 1999；Mason 2015），我们为这些神经元创造了术语“退化知识神经元”。接下来详细介绍这个概念。算法 正式地，令 N = {n1, . 。 。 , nk} 是所有局部知识神经元 1 的集合，我们将退化知识神经元定义为 D = {d1D, . 。 。 , dDm}，其中每个 dD i = {ni1, . 。 。 ,niv}包含v个知识神经元，并且满足以下条件：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/SUFF78G3.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;SUFF78G3&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22BIA5YB3C%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B53.654%2C117%2C297.115%2C163.154%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;406&quot; height=&quot;77&quot; src=&quot;attachments/SUFF78G3.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/GVDGBVQB.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;GVDGBVQB&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22MLUF5LZR%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%224%22%2C%22position%22%3A%7B%22pageIndex%22%3A3%2C%22rects%22%3A%5B%5B313.269%2C513.346%2C565.962%2C745.269%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%224%22%7D%7D&quot; width=&quot;421&quot; height=&quot;386&quot; src=&quot;attachments/GVDGBVQB.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中 Ps(ni) 是并集 Sv j=1 nij 的真子集，即 Ps(ni) ⊊ Sv j=1 nij。 Prob(X)是当神经元集合X被激活时模型的预测概率，Tlow和Thigh是可接受的预测概率差的预定义阈值。式(9)表明，抑制dD i 的任意真子集，即Ps(ni)，不会导致预测概率显着下降；而等式（10)表明，抑制dD i 中的所有神经元将导致预测概率显着下降。这表明这些神经元存储相同的知识。</p>
<pre><code>一般情况下，&lt;span style=&quot;background-color: #ff666680&quot;&gt;考虑到我们有n个知识神经元，我们需要评估所有可能的子集，找到D的复杂度是O(2n)。为了使问题易于处理，我们通过假设每个 dD i 仅包含两个知识神经元来简化问题。这个假设将问题复杂度降低到 O(n2)。&lt;/span&gt;

为了进一步减少计算量，我们设计了两步过滤过程。如图2的算法1和模块3所示，我们&lt;span style=&quot;background-color: #ff666680&quot;&gt;首先抑制每个神经元并记录不会导致预测概率显着下降的神经元，这些神经元被视为潜在的简并知识神经元Pd&lt;/span&gt;。对于Pd中的元素，进行二次过滤：&lt;span style=&quot;background-color: #ff666680&quot;&gt;抑制其中的神经元对，如果该操作导致模型的预测概率显着下降，则将该神经元对记录为退化知识神经元dD i &lt;/span&gt;。最后我们可以将退化的知识神经元返回为D。
</code></pre><h2 id="3-Experiments"><a href="#3-Experiments" class="headerlink" title="3 Experiments"></a>3 Experiments</h2><h3 id="3-1Experimental-Settings"><a href="#3-1Experimental-Settings" class="headerlink" title="3.1Experimental Settings"></a>3.1Experimental Settings</h3><p><strong>模型选择和数据集</strong> 在我们的实验中，我们选择了两种不同的多语言 PLM：m-BERT (Devlin et al. 2018) 和 m-GPT (Shliazhko et al. 2022)。 m-BERT 是一种自动编码模型，针对多种多语言数据集进行了预训练，而 m-GPT 是一种自回归模型，旨在处理 61 种语言的广泛语料库。关于数据集，我们采用 mLAMA (Kassner, Dufter, and Sch utze 2021)，它是原始 LAMA (Petroni et al. 2019a, 2020) 的多语言扩展，用于本地化多语言 PLM 中的知识。</p>
<p><strong>评估指标 </strong>我们对这两种方法应用相同的神经元编辑操作，其中检测到的知识神经元被抑制或增强，然后计算 PLM 对相关和不相关事实的预测概率。为了全面比较不同方法的知识定位精度，我们提出了一种新的评估指标来评估整个数据集知识定位的结果：</p>
<p><img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="../imgs/$%7Bfiilename%7D/KX4LAIXX.png" alt="\&lt;img alt=&quot;&quot; data-attachment-key=&quot;KX4LAIXX&quot; data-annotation=&quot;%7B%22attachmentURI%22%3A%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2FEYHEHWWX%22%2C%22annotationKey%22%3A%22V2QVIJNH%22%2C%22color%22%3A%22%23ffd400%22%2C%22pageLabel%22%3A%225%22%2C%22position%22%3A%7B%22pageIndex%22%3A4%2C%22rects%22%3A%5B%5B54.808%2C575.077%2C295.962%2C609.115%5D%5D%7D%2C%22citationItem%22%3A%7B%22uris%22%3A%5B%22http%3A%2F%2Fzotero.org%2Fusers%2Flocal%2FiUeV0SQs%2Fitems%2F77ALXKTW%22%5D%2C%22locator%22%3A%225%22%7D%7D&quot; width=&quot;402&quot; height=&quot;57&quot; src=&quot;attachments/KX4LAIXX.png&quot; ztype=&quot;zimage&quot;&gt;"></p>
<p>其中SRx是编辑成功率，x代表我们抑制或增强神经元的编辑操作。给定一个查询，它本身被认为是相关事实，并且随机选择不同类型的事实作为其不相关事实。 ΔP robrx 和 ΔP robix 分别表示相关事实和不相关事实在操作 x 下预测概率的平均变化。总体而言，<span style="background-color: #ff666680">我们希望相关事实随着知识神经元的变化而变化，而不相关事实保持不变；因此，成功率越高，定位效果越好</span>2。由于我们分别对神经元进行抑制和增强操作，因此将这两种情况的成功率总结为最终的成功率：SR = SRenhance + SRsuppress。</p>
<h3 id="3-2Localization-of-Knowledge-Neurons"><a href="#3-2Localization-of-Knowledge-Neurons" class="headerlink" title="3.2Localization of Knowledge Neurons"></a>3.2Localization of Knowledge Neurons</h3><p>我们使用模块1在英语和中文数据集上的m-BERT和m-GPT模型上进行实验，并以Dai等人（2022）提出的方法作为基线，我们将其表示为B-KN。我们的研究结果如表 1 和图 3 所示，从中我们得出了一些重要的见解。</p>
<pre><code>  (1) 我们的方法在所有设置下都取得了更好的结果。在表1中，我们使用AMIG来表示我们的方法，表中的结果代表平均成功率SR。在所有设置下，我们的方法都优于 B-KN，特别是对于中国数据集，m-BERT 和 m-GPT 的成功率分别提高了 84.34% 和 44.49%。这表明我们的方法定位的知识神经元更加精确。

&lt;span style=&quot;background-color: #5fb23680&quot;&gt;（2）在m-BERT中，知识神经元主要位于最后层，而在m-GPT中，知识神经元位于前、中、最后层，如图3所示，其中x和y轴代表PLM分别是层数和知识神经元的百分比。这可能是由于自动编码模型（例如 m-BERT）共享编码空间并在最后几层中编码高级特征，而自回归模型（例如 m-GPT）逐渐细化每层的特征来预测下一个单词。&lt;/span&gt;

  (3)汉语和英语的知识神经元分布较为相似，但也存在差异。相似之处可能是由于事实具有相同的含义语言之间存在差异，而差异可能是由于语言之间固有的结构和句法差异或预训练语料库质量的差异造成的。
</code></pre><h2 id="3-3-Language-Independence-Neurons-and-Cross-Lingual-Knowledge-Editing"><a href="#3-3-Language-Independence-Neurons-and-Cross-Lingual-Knowledge-Editing" class="headerlink" title="3.3 Language-Independence Neurons and Cross-Lingual Knowledge Editing"></a>3.3 Language-Independence Neurons and Cross-Lingual Knowledge Editing</h2><p><strong>语言无关神经元</strong>的定位通过我们对模块 2 的实验，我们捕获了图 3 中的结果。结果表明，<span style="background-color: #ff666680">无论是 m-BERT 还是 m-GPT，语言无关的知识神经元主要集中在最后一两个层。</span><span style="background-color: #5fb23680">这可能是因为独立于语言的事实充当高级特征，而 PLM 只能在最后几层成功地对它们进行编码。</span>跨语言知识编辑实验设置我们基于与语言无关的知识神经元设计跨语言编辑实验。与知识本地化实验的设置类似，我们抑制或者增强语言无关的知识神经元并计算编辑成功率SR。为了证明独立于语言的知识神经元的作用，我们设计了两个比较实验。（1）编辑一种语言的知识神经元，观察另一种语言相应事实的变化。 （2）依次编辑两种语言的知识神经元，观察两种语言对应事实的变化。跨语言知识编辑实验结果我们对表2的分析揭示了两个见解：</p>
<p><span style="background-color: #ff666680">（1）独立于语言的知识神经元促进跨语言编辑。与仅编辑中文或英文相比，编辑与语言无关的知识神经元在所有设置下都有更高的成功率；在中国数据集中，m-BERT 和 m-GPT 的成功率分别提高了 213.05% 和 277.36%。这意味着用一种语言编辑事实知识并期望其他语言发生相应变化的挑战；然而，利用独立于语言的知识神经元可以实现这一点。</span></p>
<p><span style="background-color: #ff666680">（2） 单独编辑每种语言并不能保证获得更好的结果。尽管人们可以直观地编辑每种语言以实现跨语言的更改，但我们的实验表明，这种方法不仅依赖更多的计算资源，而且可能表现不佳。与使用语言无关神经元相比，顺序编辑导致 mBERT 和 m-GPT 的成功率分别降低 42.97% 和 58.80%，这可能是由于多次编辑造成的混乱。这强调了语言独立神经元的重要性。</span></p>
<h3 id="3-4Degenerate-Knowledge-Neurons-and-Fact-Checking-Experiment"><a href="#3-4Degenerate-Knowledge-Neurons-and-Fact-Checking-Experiment" class="headerlink" title="3.4Degenerate Knowledge Neurons and Fact-Checking Experiment"></a>3.4Degenerate Knowledge Neurons and Fact-Checking Experiment</h3><p>多语言PLM中简并知识神经元的识别我们使用模块3设置了一个实验来研究简并知识神经元，结果如图4所示。<span style="background-color: #5fb23680">根据我们的观察，m-BERT和m-GPT中的简并知识神经元表现出分布模式类似于知识神经元。</span>这不仅表明了简并性之间存在很强的相关性。单语言 PLM 中简并知识神经元的识别在我们的单语言 PLM 实验中，我们成功识别了简并知识神经元，并证明它们本质上存在于 PLM 中。关于简并知识神经元的一个可能的问题是：<span style="background-color: #5fb23680">PLM 是否以多种语言存储相同的事实，从而利用多个神经元集来获取相同的信息？为了消除这种观念并证明简并知识神经元的存在与 PLM 中多语言的支持无关，</span>我们将探索扩展到单语言 PLM，特别是 BERT 和 GPT-2。这些简并知识神经元的分布如图 5 所示，进一步证实了我们的结论。事实检查实验设置 PLM 可能会隐藏虚假事实（Edwards 2023；Pitt 2022），而当前的解决方案通常依赖外部数据进行事实检查（Vladika 和 Matthes 2023）。考虑到简并知识神经元功能重叠的性质，我们设计了一个事实检查实验，以在不依赖外部数据的情况下基于简并知识神经元检测错误事实。接下来，我们详细介绍我们的实验设置。</p>
<pre><code> 首先，mLAMA 数据集被修改以包含错误的事实属性。&lt;span style=&quot;background-color: #2ea8e580&quot;&gt;对于与某个事实关系名称相关的事实三元组，例如 ⟨Tanzania, Capital, Dodoma⟩ ，我们从相同的关系名称中随机选择一个对象（例如，达累斯萨拉姆）作为错误事实。&lt;/span&gt;然后，为了验证我们的发现的实际意义，&lt;span style=&quot;background-color: #ff666680&quot;&gt;我们将数据集中的每种类型的查询按比例分为两部分。对于每种类型，第一段用于获取简并知识神经元，并识别数量超过 t% 特定阈值的神经元。随后，我们将第二部分中的查询以及相应的正确或错误事实作为输入并计算简并知识神经元的平均激活分数。如果平均激活分数超过预定义的阈值 λ，则该事实被分类为正确；否则，它被归类为 false。&lt;/span&gt;我们使用原始PLM直接评估事实的正确性进行比较分析。这种配置可以防止 PLM 使用查询本身的简并知识神经元进行事实检查，从而使实验更加令人信服。我们在表3中将我们的方法表示为“with DKN”。最后，由于当前的事实核查方法必须依赖于外部数据，因此我们使用PLM直接执行事实核查作为我们方法的基线，表示为“wo表 3 中的“DKN”。我们使用 Precision、Recall 和 F1-score 作为评估指标。
</code></pre><p>事实核查实验结果 表 3 中的结果使我们得出以下结论。</p>
<p>（1）<span style="background-color: #ff666680">退化的知识神经元可以帮助 PLM 检测错误的事实。</span>在各种设置下，我们的方法比基线方法更好，特别是对于中国数据集和自回归模型。例如，在 m-GPT 和中文数据集的背景下，我们的方法的 F1 分数与基线相比增加了 167150%。<span style="background-color: #5fb23680">这一实质性改进表明简并知识神经元的存在增强了 PLM 对事实知识的稳定掌握。</span></p>
<p>（2）<span style="background-color: #5fb23680">使用PLM进行事实检查，他们经常判断一个事实是正确的，从而导致极高的召回率。这与观察结果一致，即如果提出错误的前提，生成语言模型可能会产生不正确的信息</span>（Edwards 2022；Lakshmanan 2022；Metz 2022）。</p>
<p>（3）<span style="background-color: #5fb23680">自回归模型比自编码模型表现出更高的召回率。</span>这可能是由于自回归设计更注重一致性而不是准确性，并且自动编码在评估中可能更加保守（Zhou et al. 2023）。 (4)简并知识神经元的存在与PLM中多语言的支持无关。在单语言 PLM 中，即 BERT 和 GPT-2，事实检查也可以基于简并知识神经元进行。这一结果进一步证明了简并知识神经元的存在及其有用性。</p>
<h2 id="4-Related-Work"><a href="#4-Related-Work" class="headerlink" title="4 Related Work"></a>4 Related Work</h2><p>知识定位现有的方法大致分为两类：（1）基于梯度的方法：Dai et al.（2022）首先引入了知识神经元的概念，并通过评估每个神经元的贡献来定位它们（​​Geva et al. 2021）使用积分梯度计算他们的归因得分。 （2）Causal-inspired方法，由Meng等人（2022a）提出，将知识神经元定义为PLM中对预测某些事实知识具有最强因果效应的神经元激活，该方法启发了知识编辑算法的创建例如 ROME（Meng 等人，2022a）、MEMIT（Meng 等人，2022b）和 MEND（Mitchell 等人，2022）。然而，当前的方法缺乏针对不同 PLM 架构和多种语言探索的通用方法。公理归因方法 Sundararajan、Taly 和 Yan（2017）介绍了公理归因方法，强调敏感性和实现不变性作为归因方法的核心公理，从而产生了积分梯度（IG）。后续研究包括Discretized IG (Sanyal and Ren 2021)，它使用插值策略来提高梯度精度； Sequential IG (Enguehard 2023) 专为单词重要性评估而设计；有效 Shapley 值以及 Shapley IG，由 Liu 等人 (2022) 开发，用于提高效率和效果。我们改进了 IG 的基线向量，以最大限度地减少其信息内容。</p>
<h2 id="5-Conclusion"><a href="#5-Conclusion" class="headerlink" title="5 Conclusion"></a>5 Conclusion</h2><pre><code> 在这项研究中，我们使用适应架构的多语言集成梯度方法探索多语言 PLM 中的事实知识本地化。我们进一步设计了两个模块，导致了语言无关知识神经元和简并知识神经元的两个发现。前者肯定了多语言PLM中的一部分知识以超越语言的形式存在，而后者则引入了一种新型神经元，类似于生物系统中观察到的退化现象，这些神经元可以用来检测不正确的信息。事实。
</code></pre>
<div class="article-footer fs14">
    <section id="license">
      <div class="header"><span>许可协议</span></div>
      <div class="body"><p>本文采用 <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">署名-非商业性使用-相同方式共享 4.0 国际</a> 许可协议，转载请注明出处。</p>
</div>
    </section>
    
    <section id="share">
      <div class="header"><span>分享文章</span></div>
      <div class="body">
        <div class="link"><input class="copy-area" readonly="true" id="copy-link" value="http://humble2967738843.github.io/2023/11/02/zhi-shi-shen-jing-yuan-zhong-xin-zhi-lu-yu-yan-wu-guan-zhi-shi-shen-jing-yuan-he-jian-bing-zhi-shi-shen-jing-yuan-de-fa-xian/" /></div>
        <div class="social-wrap dis-select"><a class="social share-item wechat" onclick="util.toggle(&quot;qrcode-wechat&quot)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/b32ef3da1162a.svg" /></a><a class="social share-item weibo" target="_blank" rel="external nofollow noopener noreferrer" href="https://service.weibo.com/share/share.php?url=http://humble2967738843.github.io/2023/11/02/zhi-shi-shen-jing-yuan-zhong-xin-zhi-lu-yu-yan-wu-guan-zhi-shi-shen-jing-yuan-he-jian-bing-zhi-shi-shen-jing-yuan-de-fa-xian/&title=知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现 - humbleyl&summary=知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现Abstruct预训练语言模型 (PLM) 包含大量事实知识，但这些知识如何存储在参数中仍不清楚。本文深入研究了理解事实知识如何存储在多语言 PLM 中的复杂任务，并介绍了适..."><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/80c07e4dbb303.svg" /></a><a class="social share-item email" href="mailto:?subject=知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现 - humbleyl&amp;body=http://humble2967738843.github.io/2023/11/02/zhi-shi-shen-jing-yuan-zhong-xin-zhi-lu-yu-yan-wu-guan-zhi-shi-shen-jing-yuan-he-jian-bing-zhi-shi-shen-jing-yuan-de-fa-xian/"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/a1b00e20f425d.svg" /></a><a class="social share-item link" onclick="util.copy(&quot;copy-link&quot;, &quot;复制成功&quot;)"><img class="lazy"  src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/8411ed322ced6.svg" /></a></div>
        
        <div class="qrcode" id="qrcode-wechat" style="opacity:0;height:0">
          <img class="lazy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAABGdBTUEAALGPC/xhBQAAADhlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAAqACAAQAAAABAAAAAaADAAQAAAABAAAAAQAAAADa6r/EAAAAC0lEQVQIHWNgAAIAAAUAAY27m/MAAAAASUVORK5CYII=" data-src="https://api.qrserver.com/v1/create-qr-code/?size=256x256&data=http://humble2967738843.github.io/2023/11/02/zhi-shi-shen-jing-yuan-zhong-xin-zhi-lu-yu-yan-wu-guan-zhi-shi-shen-jing-yuan-he-jian-bing-zhi-shi-shen-jing-yuan-de-fa-xian/"/>
        </div>
        
      </div>
    </section>
    </div>
</article>
<div class="related-wrap" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">较新文章</div><a href="/2023/11/02/wo-men-ke-yi-tong-guo-qing-jing-xue-xi-lai-bian-ji-shi-shi-zhi-shi-ma/">我们可以通过情景学习来编辑事实知识吗？</a></div><div class="item" id="next"><div class="note">较早文章</div><a href="/2023/11/02/bian-ji-da-xing-yu-yan-mo-xing-wen-ti-fang-fa-he-ji-yu/">编辑大型语言模型：问题、方法和机遇</a></div></section></div>

<div class="related-wrap" id="related-posts">
    <section class='header'>
      <div class='title cap theme'>您可能感兴趣的文章</div>
    </section>
    <section class='body'>
    <div class="related-posts"><a class="item" href="\2023\11\02\duo-yu-yan-mo-xing-zhong-shi-shi-zhi-shi-de-kua-yu-yan-yi-zhi-xing\" title="多语言模型中事实知识的跨语言一致性"><span class="title">多语言模型中事实知识的跨语言一致性</span></a><a class="item" href="\2023\11\06\wang-diao-ni-xiang-wang-diao-de-dong-xi-llms-de-gao-xiao-wang-que\" title="忘掉你想忘掉的东西：LLMs的高效忘却"><span class="title">忘掉你想忘掉的东西：LLMs的高效忘却</span></a><a class="item" href="\2023\11\02\qing-jing-xue-xi-chuang-jian-ren-wu-xiang-liang\" title="情境学习创建任务向量"><span class="title">情境学习创建任务向量</span></a><a class="item" href="\2023\11\02\wo-men-ke-yi-bian-ji-duo-mo-tai-da-xing-yu-yan-mo-xing-ma\" title="我们可以编辑多模态大型语言模型吗？"><span class="title">我们可以编辑多模态大型语言模型吗？</span></a><a class="item" href="\2023\11\02\wo-men-ke-yi-tong-guo-qing-jing-xue-xi-lai-bian-ji-shi-shi-zhi-shi-ma\" title="我们可以通过情景学习来编辑事实知识吗？"><span class="title">我们可以通过情景学习来编辑事实知识吗？</span></a></div></section></div>


  <div class="related-wrap md-text" id="comments">
    <section class='header cmt-title cap theme'>
      <p>快来参与讨论吧~</p>

    </section>
    <section class='body cmt-body giscus'>
      

<svg class="loading" style="vertical-align:middle;fill:currentColor;overflow:hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg>

<div id="giscus" src="https://giscus.app/client.js" data-repo="Humble2967738843/giscus" data-repo-id="R_kgDOLsS5kA" data-category="Announcements" data-category-id="DIC_kwDOLsS5kM4Cel5C" data-mapping="url" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="preferred_color_scheme" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous"></div>

    </section>
  </div>



<footer class="page-footer footnote"><hr><div class="text"><center>
</br>
</br>
<script type="text/javascript">
function show_runtime() {
    window.setTimeout("show_runtime()", 1000);
    X = new Date("10/20/2023 00:00:00");
    Y = new Date();
    T = (Y.getTime() - X.getTime());
    M = 24 * 60 * 60 * 1000;
    a = T / M;
    A = Math.floor(a);
    b = (a - A) * 24;
    B = Math.floor(b);
    c = (b - B) * 60;
    C = Math.floor((b - B) * 60);
    D = Math.floor((c - C) * 60);
    runtime_span.innerHTML = "⏲️本站已运行 " + A + "天|" + B + "小时|" + C + "分|" + D + "秒⏲️"
}
show_runtime();
</script>
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv">🤩本站总访问量<span id="busuanzi_value_site_pv"></span>次</span><br>
<span id="runtime_span"></span>
</center>
</div></footer>
<div class="main-mask" onclick="sidebar.dismiss()"></div></div><aside class="l_right">
<div class="widgets">



<widget class="widget-wrapper toc" id="data-toc" collapse="false"><div class="widget-header dis-select"><span class="name">本文目录</span><a class="cap-action" onclick="sidebar.toggleTOC()" ><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg></a></div><div class="widget-body"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E7%A5%9E%E7%BB%8F%E5%85%83%E4%B8%AD%E5%BF%83%E4%B9%8B%E6%97%85%EF%BC%9A%E8%AF%AD%E8%A8%80%E6%97%A0%E5%85%B3%E7%9F%A5%E8%AF%86%E7%A5%9E%E7%BB%8F%E5%85%83%E5%92%8C%E7%AE%80%E5%B9%B6%E7%9F%A5%E8%AF%86%E7%A5%9E%E7%BB%8F%E5%85%83%E7%9A%84%E5%8F%91%E7%8E%B0"><span class="toc-text">知识神经元中心之旅：语言无关知识神经元和简并知识神经元的发现</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Abstruct"><span class="toc-text">Abstruct</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-Introduction"><span class="toc-text">1 Introduction</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#2-Methodology"><span class="toc-text">2 Methodology</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1KnowLedge-Neuron-Localization"><span class="toc-text">2.1KnowLedge Neuron Localization</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Language-Indepent-Knowledge-Neuron-Dectection"><span class="toc-text">2.2 Language-Indepent Knowledge Neuron Dectection</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3Degenerate-Knowledge-Neuron-Detection"><span class="toc-text">2.3Degenerate Knowledge Neuron Detection</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-Experiments"><span class="toc-text">3 Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1Experimental-Settings"><span class="toc-text">3.1Experimental Settings</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2Localization-of-Knowledge-Neurons"><span class="toc-text">3.2Localization of Knowledge Neurons</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#3-3-Language-Independence-Neurons-and-Cross-Lingual-Knowledge-Editing"><span class="toc-text">3.3 Language-Independence Neurons and Cross-Lingual Knowledge Editing</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4Degenerate-Knowledge-Neurons-and-Fact-Checking-Experiment"><span class="toc-text">3.4Degenerate Knowledge Neurons and Fact-Checking Experiment</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#4-Related-Work"><span class="toc-text">4 Related Work</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#5-Conclusion"><span class="toc-text">5 Conclusion</span></a></li></ol></li></ol></div><div class="widget-footer">

<a class="top" onclick="util.scrollTop()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 12c0-4.714 0-7.071 1.464-8.536C4.93 2 7.286 2 12 2c4.714 0 7.071 0 8.535 1.464C22 4.93 22 7.286 22 12c0 4.714 0 7.071-1.465 8.535C19.072 22 16.714 22 12 22s-7.071 0-8.536-1.465C2 19.072 2 16.714 2 12Z"/><path stroke-linecap="round" stroke-linejoin="round" d="m9 15.5l3-3l3 3m-6-4l3-3l3 3"/></g></svg><span>回到顶部</span></a></div></widget>
</div></aside><div class='float-panel blur'>
  <button type='button' style='display:none' class='laptop-only rightbar-toggle mobile' onclick='sidebar.rightbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg>
  </button>
  <button type='button' style='display:none' class='mobile-only leftbar-toggle mobile' onclick='sidebar.leftbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 11c0-3.771 0-5.657 1.172-6.828C4.343 3 6.229 3 10 3h4c3.771 0 5.657 0 6.828 1.172C22 5.343 22 7.229 22 11v2c0 3.771 0 5.657-1.172 6.828C19.657 21 17.771 21 14 21h-4c-3.771 0-5.657 0-6.828-1.172C2 18.657 2 16.771 2 13z"/><path id="sep" stroke-linecap="round" d="M5.5 10h6m-5 4h4m4.5 7V3"/></g></svg>
  </button>
</div>
</div><div class="scripts">
<script type="text/javascript">
  const ctx = {
    date_suffix: {
      just: `刚刚`,
      min: `分钟前`,
      hour: `小时前`,
      day: `天前`,
    },
    root : `/`,
  };

  // required plugins (only load if needs)
  if (`local_search`) {
    ctx.search = {};
    ctx.search.service = `local_search`;
    if (ctx.search.service == 'local_search') {
      let service_obj = Object.assign({}, `{"field":"all","path":"/search.json","content":true,"codeblock":true,"sort":"-date"}`);
      ctx.search[ctx.search.service] = service_obj;
    }
  }
  const def = {
    avatar: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/3442075.svg`,
    cover: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/cover/76b86c0226ffd.svg`,
  };
  const deps = {
    jquery: `https://cdn.bootcdn.net/ajax/libs/jquery/3.7.1/jquery.min.js`,
    marked: `https://cdn.bootcdn.net/ajax/libs/marked/4.0.18/marked.min.js`
  }
  

</script>

<script type="text/javascript">
  const utils = {
    // 懒加载 css https://github.com/filamentgroup/loadCSS
    css: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    js: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')){
        src = ctx.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // 默认异步，如果需要同步，第二个参数传入 {} 即可
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    jq: (fn) => {
      if (typeof jQuery === 'undefined') {
        utils.js(deps.jquery).then(fn)
      } else {
        fn()
      }
    },
    
    onLoading: (el) => {
      if (el) {
        $(el).append('<div class="loading-wrap"><svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" stroke-opacity=".3" d="M12 3C16.9706 3 21 7.02944 21 12C21 16.9706 16.9706 21 12 21C7.02944 21 3 16.9706 3 12C3 7.02944 7.02944 3 12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="1.3s" values="60;0"/></path><path stroke-dasharray="15" stroke-dashoffset="15" d="M12 3C16.9706 3 21 7.02944 21 12"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.3s" values="15;0"/><animateTransform attributeName="transform" dur="1.5s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></g></svg></div>');
      }
    },
    onLoadSuccess: (el) => {
      if (el) {
        $(el).find('.loading-wrap').remove();
      }
    },
    onLoadFailure: (el) => {
      if (el) {
        $(el).find('.loading-wrap svg').remove();
        $(el).find('.loading-wrap').append('<svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" d="M12 3L21 20H3L12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.5s" values="60;0"/></path><path stroke-dasharray="6" stroke-dashoffset="6" d="M12 10V14"><animate fill="freeze" attributeName="stroke-dashoffset" begin="0.6s" dur="0.2s" values="6;0"/></path></g><circle cx="12" cy="17" r="1" fill="currentColor" fill-opacity="0"><animate fill="freeze" attributeName="fill-opacity" begin="0.8s" dur="0.4s" values="0;1"/></circle></svg>');
        $(el).find('.loading-wrap').addClass('error');
      }
    },
    request: (el, url, callback, onFailure) => {
      let retryTimes = 3;
      utils.onLoading(el);
      function req() {
        return new Promise((resolve, reject) => {
          let status = 0; // 0 等待 1 完成 2 超时
          let timer = setTimeout(() => {
            if (status === 0) {
              status = 2;
              timer = null;
              reject('请求超时');
              if (retryTimes == 0) {
                onFailure();
              }
            }
          }, 5000);
          fetch(url).then(function(response) {
            if (status !== 2) {
              clearTimeout(timer);
              resolve(response);
              timer = null;
              status = 1;
            }
            if (response.ok) {
              return response.json();
            }
            throw new Error('Network response was not ok.');
          }).then(function(data) {
            retryTimes = 0;
            utils.onLoadSuccess(el);
            callback(data);
          }).catch(function(error) {
            if (retryTimes > 0) {
              retryTimes -= 1;
              setTimeout(() => {
                req();
              }, 5000);
            } else {
              utils.onLoadFailure(el);
              onFailure();
            }
          });
        });
      }
      req();
    },
  };
</script>

<script>
  const sidebar = {
    leftbar: () => {
      if (l_body) {
        l_body.toggleAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    rightbar: () => {
      if (l_body) {
        l_body.toggleAttribute('rightbar');
        l_body.removeAttribute('leftbar');
      }
    },
    dismiss: () => {
      if (l_body) {
        l_body.removeAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    toggleTOC: () => {
      document.querySelector('#data-toc').classList.toggle('collapse');
    }
  }
</script>

<!-- required -->
<script src="/js/main.js?v=1.27.0" async></script>

<!-- optional -->

  <script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const els = document.querySelectorAll("#comments #giscus");
    if (els.length === 0) return;
    els.forEach((el, i) => {
      try {
        el.innerHTML = '';
      } catch (error) {
        console.error(error);
      }
      var script = document.createElement('script');
      script.async = true;
      for (let key of Object.keys(el.attributes)) {
        let attr = el.attributes[key];
        if (['class', 'id'].includes(attr.name) === false) {
          script.setAttribute(attr.name, attr.value);
        }
      }
      el.appendChild(script);
    });
  });
</script>




<script defer>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.services = Object.assign({}, JSON.parse(`{"mdrender":{"js":"/js/services/mdrender.js"},"siteinfo":{"js":"/js/services/siteinfo.js","api":null},"ghinfo":{"js":"/js/services/ghinfo.js"},"sites":{"js":"/js/services/sites.js"},"friends":{"js":"/js/services/friends.js"},"timeline":{"js":"/js/services/timeline.js"},"fcircle":{"js":"/js/services/fcircle.js"},"weibo":{"js":"/js/services/weibo.js"},"memos":{"js":"/js/services/memos.js"}}`));
    for (let id of Object.keys(ctx.services)) {
      const js = ctx.services[id].js;
      if (id == 'siteinfo') {
        ctx.cardlinks = document.querySelectorAll('a.link-card[cardlink]');
        if (ctx.cardlinks?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            setCardLink(ctx.cardlinks);
          });
        }
      } else {
        const els = document.getElementsByClassName(`ds-${id}`);
        if (els?.length > 0) {
          utils.jq(() => {
            if (id == 'timeline' || 'memos' || 'marked') {
              utils.js(deps.marked).then(function () {
                utils.js(js, { defer: true });
              });
            } else {
              utils.js(js, { defer: true });
            }
          });
        }
      }
    }
  });
</script>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.search = {
      path: `/search.json`,
    }
    utils.js('/js/search/local-search.js', { defer: true });
  });
</script><script>
  window.FPConfig = {
    delay: 0,
    ignoreKeywords: [],
    maxRPS: 5,
    hoverDelay: 25
  };
</script>
<script defer src="https://cdn.bootcdn.net/ajax/libs/flying-pages/2.1.2/flying-pages.min.js"></script><script defer src="https://cdn.bootcdn.net/ajax/libs/vanilla-lazyload/17.8.4/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazy",
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    window.lazyLoadInstance?.update();
  });
</script><script>
  ctx.fancybox = {
    selector: `.timenode p>img`,
    css: `https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.22/fancybox/fancybox.min.css`,
    js: `https://cdn.bootcdn.net/ajax/libs/fancyapps-ui/5.0.22/fancybox/fancybox.umd.min.js`
  };
  var selector = '[data-fancybox]:not(.error)';
  if (ctx.fancybox.selector) {
    selector += `, ${ctx.fancybox.selector}`
  }
  var needFancybox = document.querySelectorAll(selector).length !== 0;
  if (!needFancybox) {
    const els = document.getElementsByClassName('ds-memos');
    if (els != undefined && els.length > 0) {
      needFancybox = true;
    }
  }
  if (needFancybox) {
    utils.css(ctx.fancybox.css);
    utils.js(ctx.fancybox.js, { defer: true }).then(function () {
      Fancybox.bind(selector, {
        hideScrollbar: false,
        Thumbs: {
          autoStart: false,
        },
        caption: (fancybox, slide) => {
          return slide.triggerEl.alt || null
        }
      });
    })
  }
</script><script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const swiper_api = document.getElementById('swiper-api');
    if (swiper_api != undefined) {
      utils.css(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.css`);
      utils.js(`https://unpkg.com/swiper@10.3.1/swiper-bundle.min.js`, { defer: true }).then(function () {
        const effect = swiper_api.getAttribute('effect') || '';
        var swiper = new Swiper('.swiper#swiper-api', {
          slidesPerView: 'auto',
          spaceBetween: 8,
          centeredSlides: true,
          effect: effect,
          loop: true,
          pagination: {
            el: '.swiper-pagination',
            clickable: true,
          },
          navigation: {
            nextEl: '.swiper-button-next',
            prevEl: '.swiper-button-prev',
          },
        });
      })
    }
  });
</script><script>
  document.addEventListener('DOMContentLoaded', function () {
    window.codeElements = document.querySelectorAll('.code');
    if (window.codeElements.length > 0) {
      ctx.copycode = {
        default_text: `Copy`,
        success_text: `Copied`,
        toast: `复制成功`,
      };
      utils.js('/js/plugins/copycode.js');
    }
  });
</script>


<!-- inject -->

</div></body></html>
